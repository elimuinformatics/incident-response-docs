{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This documentation covers parts of the Elimu Informatics Incident Response process. It is a cut-down version of our internal documentation, used at Elimu Informatics for any major incidents, and to prepare new employees for on-call responsibilities. It provides information not only on preparing for an incident, but also what to do during and after. It is intended to be used by on-call practitioners and those involved in an operational incident response process (or those wishing to enact a formal incident response process). See the about page for more information on what this documentation is and why it exists. Don't know where to start? If you're new to incident response and don't yet have a formal process in your organization, we recommend looking at our Getting Started page for a quick list of things you can do to begin, and our Training Course page for a more detailed overview of our process. Being On-Call # If you've never been on-call before, you might be wondering what it's all about. These pages describe what the expectations of being on-call are, along with some resources to help you. Being On-Call - A guide to being on-call, both what your responsibilities are, and what they are not. Alerting Principles - The principles we use to determine what things page an engineer, and what time of day they page. Before an Incident # Reading material for things you probably want to know before an incident occurs. You likely don't want to be reading these during an actual incident. What is an Incident? - Before we can talk about incident response, we need to define what an incident actually is. Severity Levels - Information on our severity level classification. What constitutes a SEV-3 vs SEV-1? What response do they get? Different Roles for Incidents - Information on the roles during an incident; Incident Commander, Scribe, etc. Incident Call Etiquette - Our etiquette guidelines for incident calls, before you find yourself in one. Complex Incidents - Our guide for handling larger, more complex incidents. During an Incident # Information and processes during a major incident. During an Incident - Information on what to do during an incident, and how to constructively contribute. Security Incident Response - Security incidents are handled differently to normal operational incidents. After an Incident # Our followup processes, how we make sure we don't repeat mistakes and are always improving. After an Incident - Information on what to do after an incident is resolved. Post-Mortem Process - Information on our post-mortem process; what's involved and how to write or run a post-mortem. Post-Mortem Template - The template we use for writing our post-mortems for major incidents. Effective Post-Mortems - A guide for writing effective post-mortems. Training # So, you want to learn about incident response? You've come to the right place. Training Overview - An overview of our training guides and additional training material from third-parties. Glossary of Incident Response Terms - A collection of terms that you may hear being used, along with their definition. Training Guides # Incident Commander Training - A guide to becoming our next Incident Commander. Deputy Training - How to be a deputy and back up the Incident Commander. Scribe Training - A guide to scribing. Subject Matter Expert Training - A guide on responsibilities and behavior for all participants in a major incident. Customer Liaison Training - A guide on how to act as our public representative during an incident. Internal Liaison Training - A guide on how to liaise with internal teams during an incident. Training Courses # Incident Response Training Course - An introductory course on incident response and the role of the Incident Commander. Additional Resources # Useful material and resources from external parties that are relevant to incident response. Reading - Recommended reading material relevant to incident response. ChatOps - Description of the chat bot commands we've referenced in this documentation. Anti-Patterns - List of things we've tried and then rejected, learn from our mistakes.","title":"Home"},{"location":"#being-on-call","text":"If you've never been on-call before, you might be wondering what it's all about. These pages describe what the expectations of being on-call are, along with some resources to help you. Being On-Call - A guide to being on-call, both what your responsibilities are, and what they are not. Alerting Principles - The principles we use to determine what things page an engineer, and what time of day they page.","title":"Being On-Call"},{"location":"#before-an-incident","text":"Reading material for things you probably want to know before an incident occurs. You likely don't want to be reading these during an actual incident. What is an Incident? - Before we can talk about incident response, we need to define what an incident actually is. Severity Levels - Information on our severity level classification. What constitutes a SEV-3 vs SEV-1? What response do they get? Different Roles for Incidents - Information on the roles during an incident; Incident Commander, Scribe, etc. Incident Call Etiquette - Our etiquette guidelines for incident calls, before you find yourself in one. Complex Incidents - Our guide for handling larger, more complex incidents.","title":"Before an Incident"},{"location":"#during-an-incident","text":"Information and processes during a major incident. During an Incident - Information on what to do during an incident, and how to constructively contribute. Security Incident Response - Security incidents are handled differently to normal operational incidents.","title":"During an Incident"},{"location":"#after-an-incident","text":"Our followup processes, how we make sure we don't repeat mistakes and are always improving. After an Incident - Information on what to do after an incident is resolved. Post-Mortem Process - Information on our post-mortem process; what's involved and how to write or run a post-mortem. Post-Mortem Template - The template we use for writing our post-mortems for major incidents. Effective Post-Mortems - A guide for writing effective post-mortems.","title":"After an Incident"},{"location":"#training","text":"So, you want to learn about incident response? You've come to the right place. Training Overview - An overview of our training guides and additional training material from third-parties. Glossary of Incident Response Terms - A collection of terms that you may hear being used, along with their definition.","title":"Training"},{"location":"#training-guides","text":"Incident Commander Training - A guide to becoming our next Incident Commander. Deputy Training - How to be a deputy and back up the Incident Commander. Scribe Training - A guide to scribing. Subject Matter Expert Training - A guide on responsibilities and behavior for all participants in a major incident. Customer Liaison Training - A guide on how to act as our public representative during an incident. Internal Liaison Training - A guide on how to liaise with internal teams during an incident.","title":"Training Guides"},{"location":"#training-courses","text":"Incident Response Training Course - An introductory course on incident response and the role of the Incident Commander.","title":"Training Courses"},{"location":"#additional-resources","text":"Useful material and resources from external parties that are relevant to incident response. Reading - Recommended reading material relevant to incident response. ChatOps - Description of the chat bot commands we've referenced in this documentation. Anti-Patterns - List of things we've tried and then rejected, learn from our mistakes.","title":"Additional Resources"},{"location":"about/","text":"This site documents parts of the Elimu Informatics Incident Response process. It is a cut-down version of our internal documentation, used at Elimu Informatics for any major incidents, and to prepare new employees for on-call responsibilities. It provides information not only on preparing for an incident, but also what to do during and after. Few companies seem to talk about their internal processes for dealing with major incidents. We would like to change that by opening up our documentation to the community, in the hopes that it proves useful to others who may want to formalize their own processes. Additionally, it provides an opportunity for others to suggest improvements, which ends up helping everyone. What is this? # A collection of pages detailing how to efficiently deal with any major incidents that might arise, along with information on how to go on-call effectively. It provides lessons learned the hard way, along with training material for getting you up to speed quickly. Who is this for? # It is intended for on-call practitioners and those involved in an operational incident response process, or those wishing to enact a formal incident response process. Why do I need it? # Incident response is something you hope to never need, but when you do, you want it to go smoothly and seamlessly. Normally the knowledge of how to handle incidents within your company will be built up over time, getting better with each incident. While tools such as Elimu Informatics's Major Incidents Application can help you recover quickly, the process you follow is just as important. This documentation will allow you to learn from the start something which has taken us years to build up. Giving you a head start on how to deal with major incidents in a way which leads to the fastest possible recovery time. What is covered? # Anything from preparing to go on-call , definitions of severities , incident call etiquette , all the way to how to run a post-mortem , and providing our post-mortem template . We even include our security incident response process . What is missing? # This isn't an exact clone of our internal documentation, but instead has some information removed. Things such as our phone bridge numbers, names of internal tools and systems which are not (yet) open sourced, images of our dashboards, etc. Basically anything that is specific to Elimu Informatics or is too sensitive to share. License # This documentation is provided under the Apache License 2.0. In plain English that means you can use and modify this documentation and use it both commercially and for private use. However, you must include any original copyright notices, and the original LICENSE file.","title":"About"},{"location":"about/#what-is-this","text":"A collection of pages detailing how to efficiently deal with any major incidents that might arise, along with information on how to go on-call effectively. It provides lessons learned the hard way, along with training material for getting you up to speed quickly.","title":"What is this?"},{"location":"about/#who-is-this-for","text":"It is intended for on-call practitioners and those involved in an operational incident response process, or those wishing to enact a formal incident response process.","title":"Who is this for?"},{"location":"about/#why-do-i-need-it","text":"Incident response is something you hope to never need, but when you do, you want it to go smoothly and seamlessly. Normally the knowledge of how to handle incidents within your company will be built up over time, getting better with each incident. While tools such as Elimu Informatics's Major Incidents Application can help you recover quickly, the process you follow is just as important. This documentation will allow you to learn from the start something which has taken us years to build up. Giving you a head start on how to deal with major incidents in a way which leads to the fastest possible recovery time.","title":"Why do I need it?"},{"location":"about/#what-is-covered","text":"Anything from preparing to go on-call , definitions of severities , incident call etiquette , all the way to how to run a post-mortem , and providing our post-mortem template . We even include our security incident response process .","title":"What is covered?"},{"location":"about/#what-is-missing","text":"This isn't an exact clone of our internal documentation, but instead has some information removed. Things such as our phone bridge numbers, names of internal tools and systems which are not (yet) open sourced, images of our dashboards, etc. Basically anything that is specific to Elimu Informatics or is too sensitive to share.","title":"What is missing?"},{"location":"about/#license","text":"This documentation is provided under the Apache License 2.0. In plain English that means you can use and modify this documentation and use it both commercially and for private use. However, you must include any original copyright notices, and the original LICENSE file.","title":"License"},{"location":"getting_started/","text":"Credit: Breakingpic @ Pexels If you don't yet have a process in your own organization, or if you're just starting out, you may find the sheer quantity of information in this documentation overwhelming. It's important to remember that this isn't something you'll be able to implement overnight . It's a process that should be built up over time. While it took us years to get to this point, our hope is that you can make use of this documentation to skip some of the awkward growing pains we went through, and reach a more mature incident response process in the most efficient way possible. To that end, we've put together this \"Getting Started\" guide to help you navigate the most important parts of our process, and provide some guidelines about which bits we think you should start with. If you're just starting out with your own incident response process, this is a great way to know what order we think you should do things in. Define what an \"Incident\" and \"Major Incident\" are for you. # You don't have to use our definitions , they're just a starting point. Feel free to come up with whatever you want. The point is that the definition should be a short, simple statement that makes sure everyone is on the same page. You want to remove any discussion around whether something is an incident or not during your response process. If you have a metric to use (e.g. \"if errors go above 100/minute it's a major incident\"), that's great. If not, don't let that stop you from defining what a Major Incident is. The reason this should be your first step is because you can't respond to an incident until you know what an incident is. If one person considers something an incident but the rest of the organization doesn't, that's going to create ambiguity and confusion during any sort of incident response. Having a clear definition that's disseminated to your entire organization makes sure that everyone has the same understanding and prevents any confusion. What about Severity Levels? You shouldn't need to worry about \"Severity Levels\" at the beginning. Just whether something is an incident or not. You can add severity levels later once you want to flesh out your response process more. Determine how you will mobilize responders. # What is going to trigger your incident response process? Will it be an automated alert tied to a metric? That's a great place to start! Even if it's just a single alert that goes out to a group of responders. Have a way to manually trigger incident response Having a way for humans to manually trigger incident response when they see something wrong will help to improve your response times. It took us a while to do this, but if we could go back in time we'd do this from the start! Make sure to set up a phone bridge and chat room dedicated for incident response. You want to prepare this in advance, and make sure the numbers and connection information are written down and shared with anyone who may need to respond. You don't want to be setting up the call and chat room while trying to respond to an incident. You should make the call and room names static or as easily discoverable as possible. You also want to set expectations for your responders . Make sure they know that they need to join the call and chat room if they get paged and that they shouldn't just jump into solving the problem. Finally, you want to make sure your alerts are actionable . There's nothing worse than waking everyone up for something you cannot control. Make sure anything that is going to trigger your incident response and page people is something that requires immediate human action to resolve. Define the incident response roles. # You only care about the Incident Commander role to begin with. If you have enough people you can also have a Scribe . But to start with, just have an Incident Commander and your responders. The Incident Commander shouldn't be taking any remediation actions at all, they should just be leading the response and making the decisions. You don't need to follow the entire training guide to begin with, just the basics of asking questions and assigning tasks are enough to get you started. Create a post-mortem template. # You can use our template to get started, or come up with your own version. Just make sure that you have a structured template so that it makes it easier to compare incidents to each other. It can be as simple as three headings to begin with: \"What happened?\", \"Why did it happen?\", \"How are we going to make sure it doesn't happen again?\". Adding more detailed fields and information can come later. Name Doesn't Matter You don't need to call them \"post-mortem\"'s. After-action review, learning review, retrospective, etc. are all valid names. The point is that you review what happened and learn from it, the name you give to the process doesn't really matter. Practice # Run a fake incident, mobilize your responders, and have someone act as the Incident Commander. Get used to the switch from normal day-to-day operations, and the emergency operations of an incident. Switching to having an Incident Commander running the show can be jarring at first, so it helps to practice it in a low-risk situation to begin with. Playing a game of \" Keep Talking and Nobody Explodes \" is a light-hearted way of practicing the skills required for incident response. You can also run your own version of Failure Friday , where you manually inject some failure into your system and treat it as a major incident. Use it for a real incident. # Once you've got the basics in place, start using the process for a real incident. The more you use it, the more natural it will become. As you use it more and more, you can add more process into it and tweak it for your needs. Things may not go smoothly the first time, but don't give up! What comes next? # You can now start expanding your process and adding some more things. Here are our recommendations for the next things you should incorporate: Add a scribe, if you haven't already. # Keeping an accurate timeline of events becomes really important when you want to go back and review your incidents. Scribe should be the next role you start using. Expand your IC rotation. # You don't want to just have a single IC, you want as many as you can get. Start training up more people and create an on-call rotation for it. At first, you will probably use weekly rotations. We recommend trying to get to a daily rotation as quickly as you can. Add in a deputy as a role. # Once you have a few more IC's, start adding a deputy to your response. Having a Deputy will give you the ability to quickly handover in longer incidents, and also gives the IC some backup for shorter incidents. Define severity levels. # Once you have the process working well, you can start to add more granularity to your response and incident definitions. Perhaps you don't want to do a \"full\" response for certain incidents. Define some severity levels to document the level of response you want. Start adding other roles. # As your process becomes more established, you want to start adding other roles. We recommend a Customer Liaison as the next one you include. Practice, practice, practice. # It cannot be overstated how much it helps to practice your incident response. If you trigger incident response and realize it's not really an incident, treat it as one anyway. You've already mobilized your responders, so it's free practice. Define a process for larger incidents. # We call these Complex Incidents . You won't use this often, but you'll want the phone bridge numbers and chat rooms prepared ahead of time. You'll also want to make sure your responders are aware of the process.","title":"Getting Started"},{"location":"getting_started/#define-what-an-incident-and-major-incident-are-for-you","text":"You don't have to use our definitions , they're just a starting point. Feel free to come up with whatever you want. The point is that the definition should be a short, simple statement that makes sure everyone is on the same page. You want to remove any discussion around whether something is an incident or not during your response process. If you have a metric to use (e.g. \"if errors go above 100/minute it's a major incident\"), that's great. If not, don't let that stop you from defining what a Major Incident is. The reason this should be your first step is because you can't respond to an incident until you know what an incident is. If one person considers something an incident but the rest of the organization doesn't, that's going to create ambiguity and confusion during any sort of incident response. Having a clear definition that's disseminated to your entire organization makes sure that everyone has the same understanding and prevents any confusion. What about Severity Levels? You shouldn't need to worry about \"Severity Levels\" at the beginning. Just whether something is an incident or not. You can add severity levels later once you want to flesh out your response process more.","title":"Define what an \"Incident\" and \"Major Incident\" are for you."},{"location":"getting_started/#determine-how-you-will-mobilize-responders","text":"What is going to trigger your incident response process? Will it be an automated alert tied to a metric? That's a great place to start! Even if it's just a single alert that goes out to a group of responders. Have a way to manually trigger incident response Having a way for humans to manually trigger incident response when they see something wrong will help to improve your response times. It took us a while to do this, but if we could go back in time we'd do this from the start! Make sure to set up a phone bridge and chat room dedicated for incident response. You want to prepare this in advance, and make sure the numbers and connection information are written down and shared with anyone who may need to respond. You don't want to be setting up the call and chat room while trying to respond to an incident. You should make the call and room names static or as easily discoverable as possible. You also want to set expectations for your responders . Make sure they know that they need to join the call and chat room if they get paged and that they shouldn't just jump into solving the problem. Finally, you want to make sure your alerts are actionable . There's nothing worse than waking everyone up for something you cannot control. Make sure anything that is going to trigger your incident response and page people is something that requires immediate human action to resolve.","title":"Determine how you will mobilize responders."},{"location":"getting_started/#define-the-incident-response-roles","text":"You only care about the Incident Commander role to begin with. If you have enough people you can also have a Scribe . But to start with, just have an Incident Commander and your responders. The Incident Commander shouldn't be taking any remediation actions at all, they should just be leading the response and making the decisions. You don't need to follow the entire training guide to begin with, just the basics of asking questions and assigning tasks are enough to get you started.","title":"Define the incident response roles."},{"location":"getting_started/#create-a-post-mortem-template","text":"You can use our template to get started, or come up with your own version. Just make sure that you have a structured template so that it makes it easier to compare incidents to each other. It can be as simple as three headings to begin with: \"What happened?\", \"Why did it happen?\", \"How are we going to make sure it doesn't happen again?\". Adding more detailed fields and information can come later. Name Doesn't Matter You don't need to call them \"post-mortem\"'s. After-action review, learning review, retrospective, etc. are all valid names. The point is that you review what happened and learn from it, the name you give to the process doesn't really matter.","title":"Create a post-mortem template."},{"location":"getting_started/#practice","text":"Run a fake incident, mobilize your responders, and have someone act as the Incident Commander. Get used to the switch from normal day-to-day operations, and the emergency operations of an incident. Switching to having an Incident Commander running the show can be jarring at first, so it helps to practice it in a low-risk situation to begin with. Playing a game of \" Keep Talking and Nobody Explodes \" is a light-hearted way of practicing the skills required for incident response. You can also run your own version of Failure Friday , where you manually inject some failure into your system and treat it as a major incident.","title":"Practice"},{"location":"getting_started/#use-it-for-a-real-incident","text":"Once you've got the basics in place, start using the process for a real incident. The more you use it, the more natural it will become. As you use it more and more, you can add more process into it and tweak it for your needs. Things may not go smoothly the first time, but don't give up!","title":"Use it for a real incident."},{"location":"getting_started/#what-comes-next","text":"You can now start expanding your process and adding some more things. Here are our recommendations for the next things you should incorporate:","title":"What comes next?"},{"location":"getting_started/#add-a-scribe-if-you-havent-already","text":"Keeping an accurate timeline of events becomes really important when you want to go back and review your incidents. Scribe should be the next role you start using.","title":"Add a scribe, if you haven't already."},{"location":"getting_started/#expand-your-ic-rotation","text":"You don't want to just have a single IC, you want as many as you can get. Start training up more people and create an on-call rotation for it. At first, you will probably use weekly rotations. We recommend trying to get to a daily rotation as quickly as you can.","title":"Expand your IC rotation."},{"location":"getting_started/#add-in-a-deputy-as-a-role","text":"Once you have a few more IC's, start adding a deputy to your response. Having a Deputy will give you the ability to quickly handover in longer incidents, and also gives the IC some backup for shorter incidents.","title":"Add in a deputy as a role."},{"location":"getting_started/#define-severity-levels","text":"Once you have the process working well, you can start to add more granularity to your response and incident definitions. Perhaps you don't want to do a \"full\" response for certain incidents. Define some severity levels to document the level of response you want.","title":"Define severity levels."},{"location":"getting_started/#start-adding-other-roles","text":"As your process becomes more established, you want to start adding other roles. We recommend a Customer Liaison as the next one you include.","title":"Start adding other roles."},{"location":"getting_started/#practice-practice-practice","text":"It cannot be overstated how much it helps to practice your incident response. If you trigger incident response and realize it's not really an incident, treat it as one anyway. You've already mobilized your responders, so it's free practice.","title":"Practice, practice, practice."},{"location":"getting_started/#define-a-process-for-larger-incidents","text":"We call these Complex Incidents . You won't use this often, but you'll want the phone bridge numbers and chat rooms prepared ahead of time. You'll also want to make sure your responders are aware of the process.","title":"Define a process for larger incidents."},{"location":"after/after_an_incident/","text":"Information on what to do after a major incident. Our followup and after action review procedures. Followup Actions for Response Roles # In addition to any direct followup items generated from an incident, each of our response roles will have a few standard followup tasks. These are generally lightweight actions that ensure we organize information and followup with customers appropriately. Steps for Incident Commander # Update the incident in PagerDuty. Group any related incidents under the primary incident. Set the final severity of the incident. Resolve the incident. Create the post-mortem, and assign an owner to the post-mortem for the incident. Send out an internal email to the relevant stakeholders explaining that we had an incident, provide a link to the post-mortem. Occasionally check on the progress of the post-mortem to ensure that it is completed within the desired time frame. Steps for Deputy # There are no additional steps after an incident is resolved. However the IC may ask for your help with their steps. Steps for Scribe # Review the chat communications and extract any relevant items from key events. Collect all TODO items and add them to the post-mortem. Steps for Subject Matter Experts # Add any notes you think are relevant to the post-mortem. Steps for Customer Liaison # Reply to any customer enquiries we received about the incident. Follow the post-mortem progress, and update our status page with the external message once it is available. Steps for Internal Liaison # There are no additional steps after an incident is resolved. However the IC may ask for your help with answering questions from internal stakeholders. Reviewing the Incident # It's important that we review the incident in detail to see exactly what went wrong, why it went wrong, and what we can do to make sure it doesn't happen again. These take many names; after-action reviews, incident review, followup review, etc. We use the term post-mortem. You can read all about our post-mortem process , which goes over this in more detail. Reviewing the Process # As well as reviewing the incident, it's important to review our process. Did we handle the incident well, or are there things we could have done better? This review isn't very formal yet, and typically involves a few of the incident commanders getting together to discuss how we might have done things differently, or if there are any tweaks we can make to our incident response process. If you're interested in joining these meetings, just let one of the incident commanders know and we'll be sure to invite you.","title":"After an Incident"},{"location":"after/after_an_incident/#followup-actions-for-response-roles","text":"In addition to any direct followup items generated from an incident, each of our response roles will have a few standard followup tasks. These are generally lightweight actions that ensure we organize information and followup with customers appropriately.","title":"Followup Actions for Response Roles"},{"location":"after/after_an_incident/#steps-for-incident-commander","text":"Update the incident in PagerDuty. Group any related incidents under the primary incident. Set the final severity of the incident. Resolve the incident. Create the post-mortem, and assign an owner to the post-mortem for the incident. Send out an internal email to the relevant stakeholders explaining that we had an incident, provide a link to the post-mortem. Occasionally check on the progress of the post-mortem to ensure that it is completed within the desired time frame.","title":"Steps for Incident Commander"},{"location":"after/after_an_incident/#steps-for-deputy","text":"There are no additional steps after an incident is resolved. However the IC may ask for your help with their steps.","title":"Steps for Deputy"},{"location":"after/after_an_incident/#steps-for-scribe","text":"Review the chat communications and extract any relevant items from key events. Collect all TODO items and add them to the post-mortem.","title":"Steps for Scribe"},{"location":"after/after_an_incident/#steps-for-subject-matter-experts","text":"Add any notes you think are relevant to the post-mortem.","title":"Steps for Subject Matter Experts"},{"location":"after/after_an_incident/#steps-for-customer-liaison","text":"Reply to any customer enquiries we received about the incident. Follow the post-mortem progress, and update our status page with the external message once it is available.","title":"Steps for Customer Liaison"},{"location":"after/after_an_incident/#steps-for-internal-liaison","text":"There are no additional steps after an incident is resolved. However the IC may ask for your help with answering questions from internal stakeholders.","title":"Steps for Internal Liaison"},{"location":"after/after_an_incident/#reviewing-the-incident","text":"It's important that we review the incident in detail to see exactly what went wrong, why it went wrong, and what we can do to make sure it doesn't happen again. These take many names; after-action reviews, incident review, followup review, etc. We use the term post-mortem. You can read all about our post-mortem process , which goes over this in more detail.","title":"Reviewing the Incident"},{"location":"after/after_an_incident/#reviewing-the-process","text":"As well as reviewing the incident, it's important to review our process. Did we handle the incident well, or are there things we could have done better? This review isn't very formal yet, and typically involves a few of the incident commanders getting together to discuss how we might have done things differently, or if there are any tweaks we can make to our incident response process. If you're interested in joining these meetings, just let one of the incident commanders know and we'll be sure to invite you.","title":"Reviewing the Process"},{"location":"after/effective_post_mortems/","text":"Writing an effective post-mortem allows us to learn quickly from our mistakes and improve our systems and processes for everyone. We want to be sure we're writing detailed and accurate post-mortems in order to get the most benefit out of them. This guide lists some of the things we can do to make sure our post-mortems are effective. Dos # Make sure the timeline is an accurate representation of events. Describe any technical lingo/acronyms you use that newcomers may not understand. Discuss how the incident fits into our understanding of the health and resiliency of the services affected . Don'ts # Don't use the word \"outage\" unless it really was an outage. We want to be sure we accurately reflect the impact of an incident, and outage is usually too broad a term to use. It can lead customers to think we were fully unavailable when that likely was nowhere near the case. Don't change details or events to make things \"look better\". We need to be honest in our post-mortems, even to ourselves, otherwise they lose their effectiveness. Don't name and shame someone. We keep our post-mortems blameless. If someone deployed a change that broke things, it's not their fault, it's our fault for having a system that allowed them to deploy a breaking change, etc. Suggestions # Avoid the concept of \"human error\". This is related to the point above about \"naming and shaming\", but there's a subtle difference - very rarely is the mistake \"rooted\" in a human performing an action, there are often several contributing factors (the script the human ran didn't have rate limiting, the documentation was out of date, etc...) that can and should be addressed. Avoid \"alternate reality\" discussion in the timeline or description sections. \"Service X started seeing elevated traffic early this morning, and stopped responding to requests. If service X had rate limited the requests from the customer, it would not have failed.\" & \"Service X began slowly responding to requests this evening, there was insufficient monitoring to detect the elevated CPU usage.\" as two examples, blends describing the actual problem with a hypothetical fix - keep the improvements separate from the description, so that each can be appropriately discussed. These videos go into more detail on the above points: \" Three analytical traps in accident investigation \" \" Two views on Human Error \" Reviewing # We have a Slack room dedicated to reviewing post-mortems before the scheduled meeting. Here are some of the things we're looking for: Does it provide enough detail? Rather than just pointing out what went wrong, does it drill down to the underlying causes of the issue? Does it separate \u201cWhat Happened?\u201d from \u201cHow to Fix it\u201d? Do the proposed action items make sense? Are they well-scoped enough? Is the post-mortem well written and understandable? Does the external message resonate well with customers or is it likely to cause outrage? Reviewing a post-mortem isn't about nit-picking typos (although we should make sure our external message isn't littered with spelling errors). It's about providing constructive feedback on valuable changes to a post-mortem so that we get the most benefit from them. Examples # Here are some examples of post-mortems from other companies as a reference, Stripe LastPass AWS Twilio Heroku Netflix GOV.UK Rail Accident Investigation A List of Post-mortems! Useful Resources # Advanced PostMortem Fu and Human Error 101 (Velocity 2011) Blame. Language. Sharing.","title":"Effective Post-Mortems"},{"location":"after/effective_post_mortems/#dos","text":"Make sure the timeline is an accurate representation of events. Describe any technical lingo/acronyms you use that newcomers may not understand. Discuss how the incident fits into our understanding of the health and resiliency of the services affected .","title":"Dos"},{"location":"after/effective_post_mortems/#donts","text":"Don't use the word \"outage\" unless it really was an outage. We want to be sure we accurately reflect the impact of an incident, and outage is usually too broad a term to use. It can lead customers to think we were fully unavailable when that likely was nowhere near the case. Don't change details or events to make things \"look better\". We need to be honest in our post-mortems, even to ourselves, otherwise they lose their effectiveness. Don't name and shame someone. We keep our post-mortems blameless. If someone deployed a change that broke things, it's not their fault, it's our fault for having a system that allowed them to deploy a breaking change, etc.","title":"Don'ts"},{"location":"after/effective_post_mortems/#suggestions","text":"Avoid the concept of \"human error\". This is related to the point above about \"naming and shaming\", but there's a subtle difference - very rarely is the mistake \"rooted\" in a human performing an action, there are often several contributing factors (the script the human ran didn't have rate limiting, the documentation was out of date, etc...) that can and should be addressed. Avoid \"alternate reality\" discussion in the timeline or description sections. \"Service X started seeing elevated traffic early this morning, and stopped responding to requests. If service X had rate limited the requests from the customer, it would not have failed.\" & \"Service X began slowly responding to requests this evening, there was insufficient monitoring to detect the elevated CPU usage.\" as two examples, blends describing the actual problem with a hypothetical fix - keep the improvements separate from the description, so that each can be appropriately discussed. These videos go into more detail on the above points: \" Three analytical traps in accident investigation \" \" Two views on Human Error \"","title":"Suggestions"},{"location":"after/effective_post_mortems/#reviewing","text":"We have a Slack room dedicated to reviewing post-mortems before the scheduled meeting. Here are some of the things we're looking for: Does it provide enough detail? Rather than just pointing out what went wrong, does it drill down to the underlying causes of the issue? Does it separate \u201cWhat Happened?\u201d from \u201cHow to Fix it\u201d? Do the proposed action items make sense? Are they well-scoped enough? Is the post-mortem well written and understandable? Does the external message resonate well with customers or is it likely to cause outrage? Reviewing a post-mortem isn't about nit-picking typos (although we should make sure our external message isn't littered with spelling errors). It's about providing constructive feedback on valuable changes to a post-mortem so that we get the most benefit from them.","title":"Reviewing"},{"location":"after/effective_post_mortems/#examples","text":"Here are some examples of post-mortems from other companies as a reference, Stripe LastPass AWS Twilio Heroku Netflix GOV.UK Rail Accident Investigation A List of Post-mortems!","title":"Examples"},{"location":"after/effective_post_mortems/#useful-resources","text":"Advanced PostMortem Fu and Human Error 101 (Velocity 2011) Blame. Language. Sharing.","title":"Useful Resources"},{"location":"after/post_mortem_process/","text":"For every major incident (SEV-2/1), we need to follow up with a post-mortem. A blame-free, detailed description, of exactly what went wrong in order to cause the incident, along with a list of steps to take in order to prevent a similar incident from occurring again in the future. The incident response process itself should also be included. Don't Neglect the Post-Mortem Don't make the mistake of neglecting a post-mortem after an incident. Without a post-mortem you fail to recognize what you're doing right, where you could improve, and most importantly, how to avoid making the same exact mistakes next time around. A well-designed, blameless post-mortem allows teams to continuously learn, and serves as a way to iteratively improve your infrastructure and incident response process. Owner Designation # The first step is that a post-mortem owner will be designated. This is done by the IC either at the end of a major incident call, or very shortly after. You will be notified directly by the IC if you are the owner for the post-mortem. The owner is responsible for populating the post-mortem, looking up logs, managing the followup investigation, and keeping all interested parties in the loop. Please use Slack for coordinating followup. A detailed list of the steps is available below, Owner Responsibilities # As owner of a post-mortem, you are responsible for the following, Scheduling the post-mortem meeting (on the shared calendar) and inviting the relevant people (this should be scheduled within 3 calendar days for a SEV-1 and 5 business days for a SEV-2). Investigating the incident, pulling in whomever you need from other teams to assist in the investigation. Updating the page with all of the necessary content. Creating follow-up JIRA tickets ( You are only responsible for creating the tickets, not following them up to resolution ). Reviewing the post-mortem content with appropriate parties before the meeting. Running through the topics at the post-mortem meeting (the IC will \"run\" the meeting and keep the discussion on track, but you will likely be doing most of the talking). Communicating the results of the post-mortem internally. Status Descriptions # Our post-mortems have a \"Status\" field which indicates where in our process the post-mortem currently is. Here's a description of the values and how we use them. Status Description Draft Indicates that the content of the post-mortem is still being worked on. In Review The content of the post-mortem has been completed, and is ready to be reviewed during the post-mortem meeting. Reviewed The meeting is over and the content has been reviewed and agreed upon. If there is an \"External Message\", the Customer Support team will take the message and update our status page as appropriate. Closed No further actions are needed on the post-mortem (outstanding issues are tracked in JIRA). If no \"External Message\", you can skip straight to this once the meeting is over. If there's an \"External Message\", then the Support team will update it to this status once the message is posted. Post-Mortem # Once you've been designated as the owner of a post-mortem, you should start creating one and updating it with all the relevant information. (If not already done by the IC) Create a new post-mortem for the incident. Schedule a post-mortem meeting for within 3 calendar days for a SEV-1 and 5 business days for a SEV-2. You should schedule this before filling in the post-mortem, just so it's on the calendar. Create the meeting on the \"Incident Post-Mortem Meetings\" shared calendar. Begin populating the page with all of the information you have. The timeline should be the main focus to begin with. The timeline should include important changes in status/impact, and also key actions taken by responders. Go through the history in Slack to identify the responders, and add them to the page. Identify the Incident Commander and Scribe in this list. Populate the post-mortem with more detailed information. For each item in the timeline, identify a metric, or some third-party page where the data came from. This could be a link to a Datadog graph, a SumoLogic search, a Tweet, etc. Anything which shows the data point you're trying to illustrate in the timeline. Add a link to the incident call recording. Perform an analysis of the incident. Capture all available data regarding the incident. What caused it, how many customers were affected, etc. Any commands or queries you use to look up data should be posted in the page so others can see how the data was gathered. Capture the impact to customers (generally in terms of event submission, delayed processing, and slow notification delivery) Identify the underlying cause of the incident (What happened, and why did it happen). Write the external message that will be sent to customers. This will be reviewed during the post-mortem meeting before it is sent out. Avoid using the word \"outage\" unless it really was a full outage, use the word \"incident\" or \"service degradation\" instead. Customers generally see \"outage\" and assume the worst. Look at other examples of previous post-mortems to see the kind of thing you should send. Post a link to the post-mortem into Slack to be reviewed for style and content by internal parties, you should try to do this about 24 hours before the meeting is scheduled. Experienced post-mortem writers will give you feedback on the level of detail and content of the post-mortem. This avoids wasted time during the meeting. Attend the post-mortem meeting (see below section for more information). Create any followup action JIRA tickets (or note down topics for discussion if we need to decide on a direction to go before creating tickets), Go through the history in Slack to identify any TODO items. Label all tickets with their severity level and date tags. Any actions which can reduce re-occurrence of the incident. (There may be some trade-off here, and that's fine. Sometimes the ROI isn't worth the effort that would go into it). Identify any actions which can make our incident response process better. Be careful with creating too many tickets. Generally we only want to create things that are P0/P1's. Things that absolutely should be dealt with. Communicate internally so we can learn from the incident. Send out an internal email to the relevant stakeholders describing the results and key learnings. Include a link to the post-mortem. Post-Mortem Meeting # These meetings should generally last 15-30 minutes, and are intended to be a wrap up of the post-mortem process. We should discuss what happened, what we could've done better, and any followup actions we need to take. The goal is to suss out any disagreement on the facts, analysis, or recommended actions, and to get some wider awareness of the problems that are causing reliability issues for us. You should invite the following people to the post-mortem meeting, Always The incident commander. The incident commander shadowee (if there was one). Service owners involved in the incident. Key engineer(s)/responders involved in the incident. Engineering manager for impacted systems. Product manager for impacted systems. Optional Customer liaison. (Only SEV-1 incidents) The incident commander will run the meeting, keeping the discussion focused and on track. However the post-mortem owner will likely be doing most of the talking as they walk through the post-mortem report. A general agenda for the meeting would be something like, Recap the timeline, to make sure everyone agrees and is on the same page. Recap important points, and any unusual items. Discuss how the problem could've been caught. Did it show up in canary ? Could it have been caught in tests, or loadtest environment? Discuss customer impact. Any comments from customers, etc. Review action items that have been created, discuss if appropriate, or if more are needed, etc. Examples # Here are some examples of post-mortems from other companies as a reference, Stripe LastPass AWS Twilio Heroku Netflix GOV.UK Rail Accident Investigation A List of Post-mortems! Useful Resources # Advanced PostMortem Fu and Human Error 101 (Velocity 2011) Blame. Language. Sharing.","title":"Post-Mortem Process"},{"location":"after/post_mortem_process/#owner-designation","text":"The first step is that a post-mortem owner will be designated. This is done by the IC either at the end of a major incident call, or very shortly after. You will be notified directly by the IC if you are the owner for the post-mortem. The owner is responsible for populating the post-mortem, looking up logs, managing the followup investigation, and keeping all interested parties in the loop. Please use Slack for coordinating followup. A detailed list of the steps is available below,","title":"Owner Designation"},{"location":"after/post_mortem_process/#owner-responsibilities","text":"As owner of a post-mortem, you are responsible for the following, Scheduling the post-mortem meeting (on the shared calendar) and inviting the relevant people (this should be scheduled within 3 calendar days for a SEV-1 and 5 business days for a SEV-2). Investigating the incident, pulling in whomever you need from other teams to assist in the investigation. Updating the page with all of the necessary content. Creating follow-up JIRA tickets ( You are only responsible for creating the tickets, not following them up to resolution ). Reviewing the post-mortem content with appropriate parties before the meeting. Running through the topics at the post-mortem meeting (the IC will \"run\" the meeting and keep the discussion on track, but you will likely be doing most of the talking). Communicating the results of the post-mortem internally.","title":"Owner Responsibilities"},{"location":"after/post_mortem_process/#status-descriptions","text":"Our post-mortems have a \"Status\" field which indicates where in our process the post-mortem currently is. Here's a description of the values and how we use them. Status Description Draft Indicates that the content of the post-mortem is still being worked on. In Review The content of the post-mortem has been completed, and is ready to be reviewed during the post-mortem meeting. Reviewed The meeting is over and the content has been reviewed and agreed upon. If there is an \"External Message\", the Customer Support team will take the message and update our status page as appropriate. Closed No further actions are needed on the post-mortem (outstanding issues are tracked in JIRA). If no \"External Message\", you can skip straight to this once the meeting is over. If there's an \"External Message\", then the Support team will update it to this status once the message is posted.","title":"Status Descriptions"},{"location":"after/post_mortem_process/#post-mortem","text":"Once you've been designated as the owner of a post-mortem, you should start creating one and updating it with all the relevant information. (If not already done by the IC) Create a new post-mortem for the incident. Schedule a post-mortem meeting for within 3 calendar days for a SEV-1 and 5 business days for a SEV-2. You should schedule this before filling in the post-mortem, just so it's on the calendar. Create the meeting on the \"Incident Post-Mortem Meetings\" shared calendar. Begin populating the page with all of the information you have. The timeline should be the main focus to begin with. The timeline should include important changes in status/impact, and also key actions taken by responders. Go through the history in Slack to identify the responders, and add them to the page. Identify the Incident Commander and Scribe in this list. Populate the post-mortem with more detailed information. For each item in the timeline, identify a metric, or some third-party page where the data came from. This could be a link to a Datadog graph, a SumoLogic search, a Tweet, etc. Anything which shows the data point you're trying to illustrate in the timeline. Add a link to the incident call recording. Perform an analysis of the incident. Capture all available data regarding the incident. What caused it, how many customers were affected, etc. Any commands or queries you use to look up data should be posted in the page so others can see how the data was gathered. Capture the impact to customers (generally in terms of event submission, delayed processing, and slow notification delivery) Identify the underlying cause of the incident (What happened, and why did it happen). Write the external message that will be sent to customers. This will be reviewed during the post-mortem meeting before it is sent out. Avoid using the word \"outage\" unless it really was a full outage, use the word \"incident\" or \"service degradation\" instead. Customers generally see \"outage\" and assume the worst. Look at other examples of previous post-mortems to see the kind of thing you should send. Post a link to the post-mortem into Slack to be reviewed for style and content by internal parties, you should try to do this about 24 hours before the meeting is scheduled. Experienced post-mortem writers will give you feedback on the level of detail and content of the post-mortem. This avoids wasted time during the meeting. Attend the post-mortem meeting (see below section for more information). Create any followup action JIRA tickets (or note down topics for discussion if we need to decide on a direction to go before creating tickets), Go through the history in Slack to identify any TODO items. Label all tickets with their severity level and date tags. Any actions which can reduce re-occurrence of the incident. (There may be some trade-off here, and that's fine. Sometimes the ROI isn't worth the effort that would go into it). Identify any actions which can make our incident response process better. Be careful with creating too many tickets. Generally we only want to create things that are P0/P1's. Things that absolutely should be dealt with. Communicate internally so we can learn from the incident. Send out an internal email to the relevant stakeholders describing the results and key learnings. Include a link to the post-mortem.","title":"Post-Mortem"},{"location":"after/post_mortem_process/#post-mortem-meeting","text":"These meetings should generally last 15-30 minutes, and are intended to be a wrap up of the post-mortem process. We should discuss what happened, what we could've done better, and any followup actions we need to take. The goal is to suss out any disagreement on the facts, analysis, or recommended actions, and to get some wider awareness of the problems that are causing reliability issues for us. You should invite the following people to the post-mortem meeting, Always The incident commander. The incident commander shadowee (if there was one). Service owners involved in the incident. Key engineer(s)/responders involved in the incident. Engineering manager for impacted systems. Product manager for impacted systems. Optional Customer liaison. (Only SEV-1 incidents) The incident commander will run the meeting, keeping the discussion focused and on track. However the post-mortem owner will likely be doing most of the talking as they walk through the post-mortem report. A general agenda for the meeting would be something like, Recap the timeline, to make sure everyone agrees and is on the same page. Recap important points, and any unusual items. Discuss how the problem could've been caught. Did it show up in canary ? Could it have been caught in tests, or loadtest environment? Discuss customer impact. Any comments from customers, etc. Review action items that have been created, discuss if appropriate, or if more are needed, etc.","title":"Post-Mortem Meeting"},{"location":"after/post_mortem_process/#examples","text":"Here are some examples of post-mortems from other companies as a reference, Stripe LastPass AWS Twilio Heroku Netflix GOV.UK Rail Accident Investigation A List of Post-mortems!","title":"Examples"},{"location":"after/post_mortem_process/#useful-resources","text":"Advanced PostMortem Fu and Human Error 101 (Velocity 2011) Blame. Language. Sharing.","title":"Useful Resources"},{"location":"after/post_mortem_template/","text":"This is a standard template we use for post-mortems at PagerDuty. Each section describes the type of information you will want to put in that section. Guidelines This page is intended to be reviewed during a post-mortem meeting that should be scheduled within 5 business days of any event. Your first step should be to schedule the post-mortem meeting in the shared calendar for within 5 business days after the incident. Don't wait until you've filled in the info to schedule the meeting, however make sure the page is completed by the meeting. ** Post-Mortem Owner:** Your name goes here. ** Meeting Scheduled For:** Schedule the meeting on the \"Incident Post-Mortem Meetings\" shared calendar, for within 5 business days after the incident. Put the date/time here. ** Call Recording:** Link to the incident call recording. Overview # Include a short sentence or two summarizing the contributing factors, timeline summary, and the impact. E.g. \"On the morning of August 99th, we suffered a 1 minute SEV-1 due to a runaway process on our primary database machine. This slowness caused roughly 0.024% of alerts that had begun during this time to be delivered out of SLA.\" What Happened # Include a short description of what happened. Contributing Factors # Include a description of any conditions that contributed to the issue. If there were any actions taken that exacerbated the issue, also include them here with the intention of learning from any mistakes made during the resolution process. Resolution # Include a description what solved the problem. If there was a temporary fix in place, describe that along with the long-term solution. Impact # Be very specific here, include exact numbers. Time in SEV-1 ?mins Time in SEV-2 ?mins Notifications Delivered out of SLA ??% (?? of ??) Events Dropped / Not Accepted ??% (?? of ??) Should usually be 0, but always check Accounts Affected ?? Users Affected ?? Support Requests Raised ?? Include any relevant links to tickets Responders # Who was the IC? Who was the scribe? Who else was involved? Timeline # Some important times to include: (1) time the contributing factor began, (2) time of the page, (3) time that the status page was updated (i.e. when the incident became public), (4) time of any significant actions, (5) time the SEV-2/1 ended, (6) links to tools/logs that show how the timestamp was arrived at. Time (UTC) Event Data Link How'd We Do? # What Went Well? # List anything you did well and want to call out. It's OK to not list anything. What Didn't Go So Well? # List anything you think we didn't do very well. The intent is that we should follow up on all points here to improve our processes. Action Items # Each action item should be in the form of a JIRA ticket, and each ticket should have the same set of two tags: \u201csev1_YYYYMMDD\u201d (such as sev1_20150911) and simply \u201csev1\u201d. Include action items such as: (1) any fixes required to prevent the contributing factor in the future, (2) any preparedness tasks that could help mitigate the problem if it came up again, (3) remaining post-mortem steps, such as the internal email, as well as the status-page public post, (4) any improvements to our incident response process. Messaging # Internal Email # This is a follow-up for employees. It should be sent out right after the post-mortem meeting is over. It only needs a short paragraph summarizing the incident and a link to this wiki page. Briefly summarize what happened and where the post-mortem page (this page) can be found. External Message # This is what will be included on the status.pagerduty.com website regarding this incident. What are we telling customers, including an apology? (The apology should be genuine, not rote.) Summary What Happened? What Are We Doing About This?","title":"Post-Mortem Template"},{"location":"after/post_mortem_template/#overview","text":"Include a short sentence or two summarizing the contributing factors, timeline summary, and the impact. E.g. \"On the morning of August 99th, we suffered a 1 minute SEV-1 due to a runaway process on our primary database machine. This slowness caused roughly 0.024% of alerts that had begun during this time to be delivered out of SLA.\"","title":"Overview"},{"location":"after/post_mortem_template/#what-happened","text":"Include a short description of what happened.","title":"What Happened"},{"location":"after/post_mortem_template/#contributing-factors","text":"Include a description of any conditions that contributed to the issue. If there were any actions taken that exacerbated the issue, also include them here with the intention of learning from any mistakes made during the resolution process.","title":"Contributing Factors"},{"location":"after/post_mortem_template/#resolution","text":"Include a description what solved the problem. If there was a temporary fix in place, describe that along with the long-term solution.","title":"Resolution"},{"location":"after/post_mortem_template/#impact","text":"Be very specific here, include exact numbers. Time in SEV-1 ?mins Time in SEV-2 ?mins Notifications Delivered out of SLA ??% (?? of ??) Events Dropped / Not Accepted ??% (?? of ??) Should usually be 0, but always check Accounts Affected ?? Users Affected ?? Support Requests Raised ?? Include any relevant links to tickets","title":"Impact"},{"location":"after/post_mortem_template/#responders","text":"Who was the IC? Who was the scribe? Who else was involved?","title":"Responders"},{"location":"after/post_mortem_template/#timeline","text":"Some important times to include: (1) time the contributing factor began, (2) time of the page, (3) time that the status page was updated (i.e. when the incident became public), (4) time of any significant actions, (5) time the SEV-2/1 ended, (6) links to tools/logs that show how the timestamp was arrived at. Time (UTC) Event Data Link","title":"Timeline"},{"location":"after/post_mortem_template/#howd-we-do","text":"","title":"How'd We Do?"},{"location":"after/post_mortem_template/#what-went-well","text":"List anything you did well and want to call out. It's OK to not list anything.","title":"What Went Well?"},{"location":"after/post_mortem_template/#what-didnt-go-so-well","text":"List anything you think we didn't do very well. The intent is that we should follow up on all points here to improve our processes.","title":"What Didn't Go So Well?"},{"location":"after/post_mortem_template/#action-items","text":"Each action item should be in the form of a JIRA ticket, and each ticket should have the same set of two tags: \u201csev1_YYYYMMDD\u201d (such as sev1_20150911) and simply \u201csev1\u201d. Include action items such as: (1) any fixes required to prevent the contributing factor in the future, (2) any preparedness tasks that could help mitigate the problem if it came up again, (3) remaining post-mortem steps, such as the internal email, as well as the status-page public post, (4) any improvements to our incident response process.","title":"Action Items"},{"location":"after/post_mortem_template/#messaging","text":"","title":"Messaging"},{"location":"after/post_mortem_template/#internal-email","text":"This is a follow-up for employees. It should be sent out right after the post-mortem meeting is over. It only needs a short paragraph summarizing the incident and a link to this wiki page. Briefly summarize what happened and where the post-mortem page (this page) can be found.","title":"Internal Email"},{"location":"after/post_mortem_template/#external-message","text":"This is what will be included on the status.pagerduty.com website regarding this incident. What are we telling customers, including an apology? (The apology should be genuine, not rote.) Summary What Happened? What Are We Doing About This?","title":"External Message"},{"location":"before/call_etiquette/","text":"Credit: Official White House Photo by Pete Souza You've just joined an incident call, and you've never been on one before. You have no idea what's going on, or what you're supposed to be doing. This page will help you through your first time on an incident call, and will provide a reference for future calls you may be a part of. First Steps # If you intend on participating on the incident call you should join both the call, and Slack. Make sure you are in a quiet environment in order to participate on the call. Background noise should be kept to a minimum. Keep your microphone muted until you have something to say. Identify yourself when you join the call; State your name and the system you are the expert for. Speak up and speak clearly. Be direct and factual. Keep conversations/discussions short and to the point. Bring any concerns to the Incident Commander (IC) on the call. Respect time constraints given by the Incident Commander. Lingo # Use clear terminology, and avoid using acronyms or abbreviations during a call. Clear and accurate communication is more important than quick communication. Standard radio voice procedure does not need to be followed on calls. However, you should familiarize yourself with the terms, as you may hear them on a call (or need to use them yourself). The ones in more active use on major incident calls are, Ack/Rog - \"I have received and understood\" Say Again - \"Repeat your last message\" Standby - \"Please wait a moment for the next response\" Wilco - \"Will comply\" Do not invent new abbreviations, and always favor being explicit of implicit. It is better to make things clearer than to try and save time by abbreviating, only to have a misunderstanding because others didn't know the abbreviation. The Commander # The Incident Commander (IC) is the leader of the incident response process, and is responsible for bringing the incident to resolution. They will announce themselves at the start of the call, and will generally be doing most of the talking. Follow all instructions from the incident commander, without exception. Do not perform any actions unless the incident commander has told you to do so. The commander will typically poll for any strong objections before performing a large action. This is your time to raise any objections if you have them. Once the commander has made a decision, that decision is final and should be followed, even if you disagreed during the poll. Answer any questions the commander asks you in a clear and concise way. Answering that you \"don't know\" something is perfectly acceptable. Do not try to guess. The commander may ask you to investigate something and get back to them in X minutes. Make sure you are ready with an answer within that time. Answering that you need more time is perfectly acceptable, but you need to give the commander an estimate of how much time. Problems? # There's no incident commander on the call! I don't know what to do! # Ask on the call if an IC is present. If you have no response, type !ic page in Slack. This will page the primary and backup IC to the call. I can join the call or Slack, but not both, what should I do? # You're welcome to join only one of the channels, however you should not actively participate in the incident response if so, as it causes disjointed communication. Liaise with someone who is both in Slack and on the call to provide any input you may have so that they can raise it.","title":"Call Etiquette"},{"location":"before/call_etiquette/#first-steps","text":"If you intend on participating on the incident call you should join both the call, and Slack. Make sure you are in a quiet environment in order to participate on the call. Background noise should be kept to a minimum. Keep your microphone muted until you have something to say. Identify yourself when you join the call; State your name and the system you are the expert for. Speak up and speak clearly. Be direct and factual. Keep conversations/discussions short and to the point. Bring any concerns to the Incident Commander (IC) on the call. Respect time constraints given by the Incident Commander.","title":"First Steps"},{"location":"before/call_etiquette/#lingo","text":"Use clear terminology, and avoid using acronyms or abbreviations during a call. Clear and accurate communication is more important than quick communication. Standard radio voice procedure does not need to be followed on calls. However, you should familiarize yourself with the terms, as you may hear them on a call (or need to use them yourself). The ones in more active use on major incident calls are, Ack/Rog - \"I have received and understood\" Say Again - \"Repeat your last message\" Standby - \"Please wait a moment for the next response\" Wilco - \"Will comply\" Do not invent new abbreviations, and always favor being explicit of implicit. It is better to make things clearer than to try and save time by abbreviating, only to have a misunderstanding because others didn't know the abbreviation.","title":"Lingo"},{"location":"before/call_etiquette/#the-commander","text":"The Incident Commander (IC) is the leader of the incident response process, and is responsible for bringing the incident to resolution. They will announce themselves at the start of the call, and will generally be doing most of the talking. Follow all instructions from the incident commander, without exception. Do not perform any actions unless the incident commander has told you to do so. The commander will typically poll for any strong objections before performing a large action. This is your time to raise any objections if you have them. Once the commander has made a decision, that decision is final and should be followed, even if you disagreed during the poll. Answer any questions the commander asks you in a clear and concise way. Answering that you \"don't know\" something is perfectly acceptable. Do not try to guess. The commander may ask you to investigate something and get back to them in X minutes. Make sure you are ready with an answer within that time. Answering that you need more time is perfectly acceptable, but you need to give the commander an estimate of how much time.","title":"The Commander"},{"location":"before/call_etiquette/#problems","text":"","title":"Problems?"},{"location":"before/call_etiquette/#theres-no-incident-commander-on-the-call-i-dont-know-what-to-do","text":"Ask on the call if an IC is present. If you have no response, type !ic page in Slack. This will page the primary and backup IC to the call.","title":"There's no incident commander on the call! I don't know what to do!"},{"location":"before/call_etiquette/#i-can-join-the-call-or-slack-but-not-both-what-should-i-do","text":"You're welcome to join only one of the channels, however you should not actively participate in the incident response if so, as it causes disjointed communication. Liaise with someone who is both in Slack and on the call to provide any input you may have so that they can raise it.","title":"I can join the call or Slack, but not both, what should I do?"},{"location":"before/complex_incidents/","text":"There will come a time when you will be involved in an incident (or multiple concurrent incidents) which ends up spanning a large number of resources. In these cases it's important for everyone to maintain an effective span of control . This page describes how we manage such incidents. Identifying Complex Incidents # Perhaps multiple issues are happening at once, or an existing incident escalated and had a knock-on effect on other services. It's important to identify these types of incidents as early as possible to prevent confusion and burn-out. Here are a few key things the Incident Commanders will be watching out for to help to identify a complex incident, Are multiple teams involved? Are most of them actively investigating multiple issues? Multiple symptoms are present, and do not appear to have any obvious correlation. A group of SME's are all working on the same analysis. The incident calls \"feels crowded\". This is a rather fuzzy metric, but most people can generally get a feeling when there's too many responders on the call. Sub-Teams # When it is identified that we have a complex incident, the incident commander will spin off sub-teams to work on each individual problem. We have three pre-defined sub-teams, which the Incident Commander may assign you to; Alpha, Bravo, and Charlie. Each team has it's own Slack room and conference call bridge already set up and ready for use. Team Names We chose the phonetic alphabet for our teams. We did not use colors as \"Red Team\" and \"Blue Team\" have other definitions within security incident response, and we wanted to prevent confusion. Alpha Team #team-alpha +1.555.123.4567 Bravo Team #team-bravo +1.555.123.4568 Charlie Team #team-charlie +1.555.123.4569 All three teams do not necessarily need to be active at the same time, an incident may call for only one, or even more than three if necessary. Team leaders will be picked and then assigned to a specific team designation by the Incident Commander. Role Structure # How do sub-teams fit into the role structure for our incident response? The team leaders essentially replace the SME's in the normal role structure, and then the SME's will report to their team leader. This ensures that the Incident Commander and Team Leaders can maintain an effective span of control. Spinning Off Sub-Teams # The IC will assign a leader to each team who will report to them directly. All other team members will report to their team leader. When assigning a team leader, the IC will also designate which team name they fall under (Alpha, Bravo, or Charlie). Team leaders do not have to be trained as incident commanders, however some leadership experience would be prudent. Each team will be given a specific task to complete, and will be time-boxed as they normally would for an individual responder. How teams are split is at the discretion of the incident commander. Potential structures include, Cross-functional groups to address one entire problem domain per group. Groups of Subject Matter Experts to focus on one specific element of a larger problem. Teams based on normal day-to-day roles. Switching Sub-Teams # If you feel like you would be better suited on another sub-team, you should bring this up with your current team leader . Do not bring it up to the Incident Commander, or with the leader of the team you wish to be on. An incident is currently in progress, and the defined escalation path should still be followed.","title":"Complex Incidents"},{"location":"before/complex_incidents/#identifying-complex-incidents","text":"Perhaps multiple issues are happening at once, or an existing incident escalated and had a knock-on effect on other services. It's important to identify these types of incidents as early as possible to prevent confusion and burn-out. Here are a few key things the Incident Commanders will be watching out for to help to identify a complex incident, Are multiple teams involved? Are most of them actively investigating multiple issues? Multiple symptoms are present, and do not appear to have any obvious correlation. A group of SME's are all working on the same analysis. The incident calls \"feels crowded\". This is a rather fuzzy metric, but most people can generally get a feeling when there's too many responders on the call.","title":"Identifying Complex Incidents"},{"location":"before/complex_incidents/#sub-teams","text":"When it is identified that we have a complex incident, the incident commander will spin off sub-teams to work on each individual problem. We have three pre-defined sub-teams, which the Incident Commander may assign you to; Alpha, Bravo, and Charlie. Each team has it's own Slack room and conference call bridge already set up and ready for use. Team Names We chose the phonetic alphabet for our teams. We did not use colors as \"Red Team\" and \"Blue Team\" have other definitions within security incident response, and we wanted to prevent confusion. Alpha Team #team-alpha +1.555.123.4567 Bravo Team #team-bravo +1.555.123.4568 Charlie Team #team-charlie +1.555.123.4569 All three teams do not necessarily need to be active at the same time, an incident may call for only one, or even more than three if necessary. Team leaders will be picked and then assigned to a specific team designation by the Incident Commander.","title":"Sub-Teams"},{"location":"before/complex_incidents/#role-structure","text":"How do sub-teams fit into the role structure for our incident response? The team leaders essentially replace the SME's in the normal role structure, and then the SME's will report to their team leader. This ensures that the Incident Commander and Team Leaders can maintain an effective span of control.","title":"Role Structure"},{"location":"before/complex_incidents/#spinning-off-sub-teams","text":"The IC will assign a leader to each team who will report to them directly. All other team members will report to their team leader. When assigning a team leader, the IC will also designate which team name they fall under (Alpha, Bravo, or Charlie). Team leaders do not have to be trained as incident commanders, however some leadership experience would be prudent. Each team will be given a specific task to complete, and will be time-boxed as they normally would for an individual responder. How teams are split is at the discretion of the incident commander. Potential structures include, Cross-functional groups to address one entire problem domain per group. Groups of Subject Matter Experts to focus on one specific element of a larger problem. Teams based on normal day-to-day roles.","title":"Spinning Off Sub-Teams"},{"location":"before/complex_incidents/#switching-sub-teams","text":"If you feel like you would be better suited on another sub-team, you should bring this up with your current team leader . Do not bring it up to the Incident Commander, or with the leader of the team you wish to be on. An incident is currently in progress, and the defined escalation path should still be followed.","title":"Switching Sub-Teams"},{"location":"before/different_roles/","text":"There are several main roles for our incident response teams at PagerDuty. Certain roles only have one person per incident (e.g. IC), whereas other roles can have multiple people (e.g. Subject Matter Expert, SME). It's all about coming together as a team, working the problem, and getting a solution quickly. Here is a rough outline of our role hierarchy, with each role discussed in more detail on the rest of this page. During larger complex incidents, the role structure may be adjusted to account for the creation of sub-teams. Read about how we handle complex incidents for more information. Flexible Structure It is not intended that every role be filled by a different person for every incident. For example, if the incident is small enough in scope, the Deputy might also take on the responsibilities of the Scribe and Internal Liaison for that specific incident. The structure should be flexible and scale based on the size and scope of the incident. Incident Commander (IC) # What is it? # An Incident Commander acts as the single source of truth of what is currently happening and what is going to happen during an major incident. They come in all shapes, sizes, and colors. Why have one? # As any software system grows in size and complexity, things break and cause incidents. The Incident Commander is needed to help drive major incidents to resolution. What are the responsibilities? # Help prepare for major incidents, Setup communications channels for major incidents. Funnel people to these communications channels when there is a major incident. Train team members on how to communicate during major incidents and train other Incident Commanders. Drive major incidents to resolution, Get everyone on the same communication channel. Collect information from team members for their services/area of ownership status. Collect proposed repair actions, then recommend repair actions to be taken. Delegate all repair actions, the Incident Commander is NOT a resolver. Be the single authority on system status Post Mortem, Creating the initial template right after the incident so people can put in their thoughts while fresh. Assigning the post-mortem after the event is over, this can be done after the call. Work with Team Leads/Managers on scheduling preventive actions. Who are they? # Anyone on the Incident Commander on-call schedule. Trainees are typically on the Incident Commander Shadow schedule. How can I become one? # Take a look at our Incident Commander training guide . Deputy # What is it? # A Deputy is a direct support role for the Incident Commander. This is not a shadow where the person just observes, the Deputy is expected to perform important tasks during an incident. Why have one? # It's important for the IC to focus on the problem at hand, rather than worrying about documenting the steps or monitoring timers. The deputy helps to support the IC and keep them focussed on the incident. What are the responsibilities? # The Deputy is expected to: Bring up issues to the Incident Commander that may otherwise not be addressed (keeping an eye on timers that have been started, circling back around to missed items from a roll call, etc). Be a \"hot standby\" Incident Commander, should the primary need to either transition to a SME, or otherwise have to step away from the IC role. Manage the incident call, and be prepared to remove people from the call if instructed by the Incident Commander. Who are they? # Any Incident Commander can act as a deputy. Deputies need to be trained as an Incident Commander as they may be required to take over command. How can I become one? # Take a look at our Deputy training guide . Deputies also need to be trained as an Incident Commander . Scribe # What is it? # A Scribe documents the timeline of an incident as it progresses, and makes sure all important decisions and data are captured for later review. Why have one? # The incident commander will need to focus on the problem at hand, and the subject matter experts will need to focus on resolving the incident. It is important to capture a timeline of events as they happen so that they can be reviewed during the post-mortem to determine how well we performed, and so we can accurately determine any additional impact that we might not have noticed at the time. What are the responsibilities? # The Scribe is expected to: Ensure the incident call is being recorded. Note in Slack important data, events, and actions, as they happen. Specifically: Key actions as they are taken (Example: \"prod-server-387723 is being restarted to attempt to remove the stuck lock\") Status reports when one is provided by the IC (Example: \"We are in SEV-1, service A is currently not processing events due to a stuck lock, X is restarting the app stack, next checkin in 3 minutes\") Any key callouts either during the call or at the ending review (Example: \"Note: (Bob B) We should have a better way to determine stuck locks.\") Who are they? # Anyone can act as a scribe during an incident, and are chosen by the Incident Commander at the start of the call. Typically the Deputy will act as the Scribe, but that doesn't necessarily need to happen, and for larger incidents may not be possible. How can I become one? # Follow our Scribe training guide , and then notify the Incident Commanders that you would like to be considered for scribing for the next incident. Subject Matter Expert # What is it? # A Subject Matter Expert (SME), sometimes called a \"Resolver\", is a domain expert or designated owner of a component or service that is part of the PagerDuty software stack. Why have one? # The IC and deputy are not all-knowing super beings. When there is a problem with a service, an expert in that service is needed to be able to quickly help identify and fix issues. What are the responsibilities? # Being able to diagnose common problems with the service. Being able to rapidly fix issues found during an incident. Concise communication skills, specifically for CAN reports: Condition: What is the current state of the service? Is it healthy or not? Actions: What actions need to be taken if the service is not in a healthy state? Needs: What support does the resolver need to perform an action? Who are they? # Anyone who is considered a \"domain expert\" can act as a resolver for an incident. Typically the service's primary on-call will act as the SME for that service. How can I become one? # Take a look at our Subject Matter Expert training guide . You should also discuss with your team and service owner to determine what the requirements are for your particular service. Customer Liaison # What is it? # A person responsible for interacting with customers, either directly, or via our public communication channels. Typically a member of the Customer Support team. Why have one? # All of the other roles will be actively working on identifying the cause and resolving the issue, we need a role which is focused purely on the customer interaction side of things so that it can be done properly, with the due care and attention it needs. What are the responsibilities? # Post any publicly facing messages regarding the incident (Twitter, StatusPage, etc). Notify the IC of any customers reporting that they are affected by the incident. Provide customers with the external message from the post-mortem once it is completed. Who are they? # Any member of the Support Team can act as a customer liaison. How can I become one? # Follow our Customer Liaison training guide , and discuss with the Support Team about becoming our next Customer Liaison. Internal Liaison # What is it? # A person responsible for interacting with internal stakeholders. Whether it's notifying an internal team of the incident, or mobilizing additional responders within the organization. Why have one? # For larger incidents, we may have multiple teams across the organization involved. Having a dedicated liaison to mobilize those teams and bring them up to speed free's up the rest of the responders to handle the incident. What are the responsibilities? # Page SME's or other on-call engineers as instructed by the Incident Commander. Notify other teams within the organization (e.g. Finance, Legal, Marketing), as instructed by the Incident Commander. Liaise with stakeholders and provide status updates as necessary. Interact with internal stakeholders to answer their questions, to keep the primary call distraction free. Who are they? # Anyone designated by the Incident Commander during incident response. How can I become one? # Follow our Internal Liaison training .","title":"Different Roles"},{"location":"before/different_roles/#incident-commander-ic","text":"","title":"Incident Commander (IC)"},{"location":"before/different_roles/#what-is-it","text":"An Incident Commander acts as the single source of truth of what is currently happening and what is going to happen during an major incident. They come in all shapes, sizes, and colors.","title":"What is it?"},{"location":"before/different_roles/#why-have-one","text":"As any software system grows in size and complexity, things break and cause incidents. The Incident Commander is needed to help drive major incidents to resolution.","title":"Why have one?"},{"location":"before/different_roles/#what-are-the-responsibilities","text":"Help prepare for major incidents, Setup communications channels for major incidents. Funnel people to these communications channels when there is a major incident. Train team members on how to communicate during major incidents and train other Incident Commanders. Drive major incidents to resolution, Get everyone on the same communication channel. Collect information from team members for their services/area of ownership status. Collect proposed repair actions, then recommend repair actions to be taken. Delegate all repair actions, the Incident Commander is NOT a resolver. Be the single authority on system status Post Mortem, Creating the initial template right after the incident so people can put in their thoughts while fresh. Assigning the post-mortem after the event is over, this can be done after the call. Work with Team Leads/Managers on scheduling preventive actions.","title":"What are the responsibilities?"},{"location":"before/different_roles/#who-are-they","text":"Anyone on the Incident Commander on-call schedule. Trainees are typically on the Incident Commander Shadow schedule.","title":"Who are they?"},{"location":"before/different_roles/#how-can-i-become-one","text":"Take a look at our Incident Commander training guide .","title":"How can I become one?"},{"location":"before/different_roles/#deputy","text":"","title":"Deputy"},{"location":"before/different_roles/#what-is-it_1","text":"A Deputy is a direct support role for the Incident Commander. This is not a shadow where the person just observes, the Deputy is expected to perform important tasks during an incident.","title":"What is it?"},{"location":"before/different_roles/#why-have-one_1","text":"It's important for the IC to focus on the problem at hand, rather than worrying about documenting the steps or monitoring timers. The deputy helps to support the IC and keep them focussed on the incident.","title":"Why have one?"},{"location":"before/different_roles/#what-are-the-responsibilities_1","text":"The Deputy is expected to: Bring up issues to the Incident Commander that may otherwise not be addressed (keeping an eye on timers that have been started, circling back around to missed items from a roll call, etc). Be a \"hot standby\" Incident Commander, should the primary need to either transition to a SME, or otherwise have to step away from the IC role. Manage the incident call, and be prepared to remove people from the call if instructed by the Incident Commander.","title":"What are the responsibilities?"},{"location":"before/different_roles/#who-are-they_1","text":"Any Incident Commander can act as a deputy. Deputies need to be trained as an Incident Commander as they may be required to take over command.","title":"Who are they?"},{"location":"before/different_roles/#how-can-i-become-one_1","text":"Take a look at our Deputy training guide . Deputies also need to be trained as an Incident Commander .","title":"How can I become one?"},{"location":"before/different_roles/#scribe","text":"","title":"Scribe"},{"location":"before/different_roles/#what-is-it_2","text":"A Scribe documents the timeline of an incident as it progresses, and makes sure all important decisions and data are captured for later review.","title":"What is it?"},{"location":"before/different_roles/#why-have-one_2","text":"The incident commander will need to focus on the problem at hand, and the subject matter experts will need to focus on resolving the incident. It is important to capture a timeline of events as they happen so that they can be reviewed during the post-mortem to determine how well we performed, and so we can accurately determine any additional impact that we might not have noticed at the time.","title":"Why have one?"},{"location":"before/different_roles/#what-are-the-responsibilities_2","text":"The Scribe is expected to: Ensure the incident call is being recorded. Note in Slack important data, events, and actions, as they happen. Specifically: Key actions as they are taken (Example: \"prod-server-387723 is being restarted to attempt to remove the stuck lock\") Status reports when one is provided by the IC (Example: \"We are in SEV-1, service A is currently not processing events due to a stuck lock, X is restarting the app stack, next checkin in 3 minutes\") Any key callouts either during the call or at the ending review (Example: \"Note: (Bob B) We should have a better way to determine stuck locks.\")","title":"What are the responsibilities?"},{"location":"before/different_roles/#who-are-they_2","text":"Anyone can act as a scribe during an incident, and are chosen by the Incident Commander at the start of the call. Typically the Deputy will act as the Scribe, but that doesn't necessarily need to happen, and for larger incidents may not be possible.","title":"Who are they?"},{"location":"before/different_roles/#how-can-i-become-one_2","text":"Follow our Scribe training guide , and then notify the Incident Commanders that you would like to be considered for scribing for the next incident.","title":"How can I become one?"},{"location":"before/different_roles/#subject-matter-expert","text":"","title":"Subject Matter Expert"},{"location":"before/different_roles/#what-is-it_3","text":"A Subject Matter Expert (SME), sometimes called a \"Resolver\", is a domain expert or designated owner of a component or service that is part of the PagerDuty software stack.","title":"What is it?"},{"location":"before/different_roles/#why-have-one_3","text":"The IC and deputy are not all-knowing super beings. When there is a problem with a service, an expert in that service is needed to be able to quickly help identify and fix issues.","title":"Why have one?"},{"location":"before/different_roles/#what-are-the-responsibilities_3","text":"Being able to diagnose common problems with the service. Being able to rapidly fix issues found during an incident. Concise communication skills, specifically for CAN reports: Condition: What is the current state of the service? Is it healthy or not? Actions: What actions need to be taken if the service is not in a healthy state? Needs: What support does the resolver need to perform an action?","title":"What are the responsibilities?"},{"location":"before/different_roles/#who-are-they_3","text":"Anyone who is considered a \"domain expert\" can act as a resolver for an incident. Typically the service's primary on-call will act as the SME for that service.","title":"Who are they?"},{"location":"before/different_roles/#how-can-i-become-one_3","text":"Take a look at our Subject Matter Expert training guide . You should also discuss with your team and service owner to determine what the requirements are for your particular service.","title":"How can I become one?"},{"location":"before/different_roles/#customer-liaison","text":"","title":"Customer Liaison"},{"location":"before/different_roles/#what-is-it_4","text":"A person responsible for interacting with customers, either directly, or via our public communication channels. Typically a member of the Customer Support team.","title":"What is it?"},{"location":"before/different_roles/#why-have-one_4","text":"All of the other roles will be actively working on identifying the cause and resolving the issue, we need a role which is focused purely on the customer interaction side of things so that it can be done properly, with the due care and attention it needs.","title":"Why have one?"},{"location":"before/different_roles/#what-are-the-responsibilities_4","text":"Post any publicly facing messages regarding the incident (Twitter, StatusPage, etc). Notify the IC of any customers reporting that they are affected by the incident. Provide customers with the external message from the post-mortem once it is completed.","title":"What are the responsibilities?"},{"location":"before/different_roles/#who-are-they_4","text":"Any member of the Support Team can act as a customer liaison.","title":"Who are they?"},{"location":"before/different_roles/#how-can-i-become-one_4","text":"Follow our Customer Liaison training guide , and discuss with the Support Team about becoming our next Customer Liaison.","title":"How can I become one?"},{"location":"before/different_roles/#internal-liaison","text":"","title":"Internal Liaison"},{"location":"before/different_roles/#what-is-it_5","text":"A person responsible for interacting with internal stakeholders. Whether it's notifying an internal team of the incident, or mobilizing additional responders within the organization.","title":"What is it?"},{"location":"before/different_roles/#why-have-one_5","text":"For larger incidents, we may have multiple teams across the organization involved. Having a dedicated liaison to mobilize those teams and bring them up to speed free's up the rest of the responders to handle the incident.","title":"Why have one?"},{"location":"before/different_roles/#what-are-the-responsibilities_5","text":"Page SME's or other on-call engineers as instructed by the Incident Commander. Notify other teams within the organization (e.g. Finance, Legal, Marketing), as instructed by the Incident Commander. Liaise with stakeholders and provide status updates as necessary. Interact with internal stakeholders to answer their questions, to keep the primary call distraction free.","title":"What are the responsibilities?"},{"location":"before/different_roles/#who-are-they_5","text":"Anyone designated by the Incident Commander during incident response.","title":"Who are they?"},{"location":"before/different_roles/#how-can-i-become-one_5","text":"Follow our Internal Liaison training .","title":"How can I become one?"},{"location":"before/severity_levels/","text":"The first step in any incident response process is to determine what actually constitutes an incident . Incidents can then be classified by severity, usually done by using \"SEV\" definitions, with lower numbered severities being more urgent. Operational issues can be classified at one of these severity levels, and in general you are able to take more risky moves to resolve a higher severity issue. Anything above a SEV-3 is automatically considered a \"major incident\" and gets a more intensive response than a normal incident. Always Assume The Worst If you are unsure which level an incident is (e.g. not sure if SEV-2 or SEV-1), treat it as the higher one . During an incident is not the time to discuss or litigate severities, just assume the highest and review during a post-mortem. Can a SEV-3 be a major incident? All SEV-2's are major incidents, but not all major incidents need to be SEV-2's. If you require co-ordinated response, even for lower severity issues, then trigger our incident response process. The IC can make a determination on whether full incident response is necessary. Severity Description Typical Response SEV-1 Critical issue that warrants public notification and liaison with executive teams. The system is in a critical state and is actively impacting a large number of customers. Functionality has been severely impaired for a long time, breaking SLA. Customer-data-exposing security vulnerability has come to our attention. Major incident response. Page an IC in Slack !ic page . See During an Incident . Notify internal stakeholders. Public notification. SEV-2 Critical system issue actively impacting many customers' ability to use the product. Notification pipeline is severely impaired. Incident response functionality (ack, resolve, etc) is severely impaired. Web app is unavailable or experiencing severe performance degradation for most/all users. Monitoring of PagerDuty systems for major incident conditions is impaired. Any other event to which a PagerDuty employee deems necessary of incident response. Major incident response. Page an IC in Slack !ic page . See During an Incident . Anything above this line is considered a \"Major Incident\". Our incident response process should be triggered for any major incidents. SEV-3 Stability or minor customer-impacting issues that require immediate attention from service owners. Partial loss of functionality, not affecting majority of customers. Something that has the likelihood of becoming a SEV-2 if nothing is done. No redundancy in a service (failure of 1 more node will cause outage). High-Urgency page to service team. Work on issue as your top priority. Liaise with engineers of affected systems to identify cause. If related to recent deployment, rollback. Monitor status and notice if/when it escalates. Mention on Slack if you think it has the potential to escalate. Trigger incident response if necessary ( !ic page ). SEV-4 Minor issues requiring action, but not affecting customer ability to use the product. Performance issues (delays, etc). Individual host failure (i.e. one node out of a cluster). Delayed job failure (not impacting event & notification pipeline). Cron failure (not impacting event & notification pipeline). Low-Urgency page to service team. Work on the issue as your first priority (above \"normal\" tasks). Monitor status and notice if/when it escalates. SEV-5 Cosmetic issues or bugs, not affecting customer ability to use the product. Bugs not impacting the immediate ability to use the system. JIRA ticket. Create a JIRA ticket and assign to owner of affected system. Be Specific These severity descriptions have been changed from the PagerDuty internal definitions to be more generic. For your own documentation, you are encouraged to make your definitions very specific, usually referring to a % of users/accounts affected. You will usually want your severity definitions to be metric driven.","title":"Severity Levels"},{"location":"before/what_is_an_incident/","text":"Before we can define our incident response process, we should first define what an incident (and a major incident) is. What is an incident? # Any unplanned disruption or degradation of service that is actively affecting customers ability to use PagerDuty. What is a major incident? # Any incident that requires a co-ordinated response between multiple teams. What is Incident Response? # An organized approach to addressing and managing an incident. The goal isn't just to solve the incident, but to handle the situation in a way that limits damage and reduces recovery time and costs. What triggers our incident response process? # Our incident response process should be initiated for any major incident. It provides a framework for effectively responding and reaching a fast resolution time. Our incident response process can be triggered one of two ways, either via automated monitoring and alerting, or manually via human action. Automated Monitoring # Throughout our system, we monitor various metrics to determine if our system is in a state which would require a co-ordinated human response in order to resolve. To determine which metrics we monitor, and what to monitor them for, we ask ourselves these questions. If the answer to any is \"No\", then we should trigger our incident response process. Can customers perform all incident response functions provided by PagerDuty, across all platforms? e.g. Can customers acknowledge, reassign, and resolve incidents via every supported method? Are customers receiving notifications within SLA? Human Escalation # Automatic monitoring is only part of the process. We may have parts of our functionality which lack the necessary monitoring. It's important to still be able to trigger a coordinated incident response in those cases. For example, if our Support team start to receive requests that indicate a system issue, they need to have the power to trigger our response. Any PagerDuty employee has the ability to trigger our incident response process at any time. We trigger on any unplanned disruption or degradation of service to which any PagerDuty employee deems necessary of requiring co-ordinated incident response. Is a response required? If you are unsure of whether response is required, trigger our incident response process. All you need to do to start the process is page an IC in Slack with !ic page . Incident Severity # Our severity definitions determine how severe we think an incident is, based on some pre-defined guidelines. The intent is to guide responders on the type of response they can provide. For example, the higher the severity, the riskier the decisions you can take to return the system to normal. Severities are useful to quickly determine whether something requires a more complex response, or whether it requires a co-ordinated response at all. However, they are not a black and white definition of what constitutes a major incident. If something is not covered by our severity definitions, but you think it requires incident response, then it requires incident response. We only need to know one thing: \"Is this a major incident?\". The severity level can be determined later, and isn't a requirement of triggering our response process. Mentality Shift # One of the more important concepts of our incident response process is the mentality shift that needs to be made during an incident. We typically call this the \"Peacetime vs Wartime\" mentality shift. The idea is that the decision making process changes when you are in an incident situation, and you are able to take riskier actions than you would normally consider during day-to-day operations. It can be hard for responders to grasp this concept, and your incident response process can be held up by responders who stick to the peacetime way of thinking, not wanting to proceed with a potentially risky action. You can read more about peacetime vs wartime in the Responder Training Guide . Normal vs Emergency Some people don't like the \"Peacetime vs Wartime\" analogy, in which case you can use any other terms you feel appropriate. \"Normal vs Emergency\" is a common choice, but you could equally use \"OK vs Not OK\". It's not terribly important what name you give it, the important part is to make the mentality shift.","title":"What is an Incident?"},{"location":"before/what_is_an_incident/#what-is-an-incident","text":"Any unplanned disruption or degradation of service that is actively affecting customers ability to use PagerDuty.","title":"What is an incident?"},{"location":"before/what_is_an_incident/#what-is-a-major-incident","text":"Any incident that requires a co-ordinated response between multiple teams.","title":"What is a major incident?"},{"location":"before/what_is_an_incident/#what-is-incident-response","text":"An organized approach to addressing and managing an incident. The goal isn't just to solve the incident, but to handle the situation in a way that limits damage and reduces recovery time and costs.","title":"What is Incident Response?"},{"location":"before/what_is_an_incident/#what-triggers-our-incident-response-process","text":"Our incident response process should be initiated for any major incident. It provides a framework for effectively responding and reaching a fast resolution time. Our incident response process can be triggered one of two ways, either via automated monitoring and alerting, or manually via human action.","title":"What triggers our incident response process?"},{"location":"before/what_is_an_incident/#automated-monitoring","text":"Throughout our system, we monitor various metrics to determine if our system is in a state which would require a co-ordinated human response in order to resolve. To determine which metrics we monitor, and what to monitor them for, we ask ourselves these questions. If the answer to any is \"No\", then we should trigger our incident response process. Can customers perform all incident response functions provided by PagerDuty, across all platforms? e.g. Can customers acknowledge, reassign, and resolve incidents via every supported method? Are customers receiving notifications within SLA?","title":"Automated Monitoring"},{"location":"before/what_is_an_incident/#human-escalation","text":"Automatic monitoring is only part of the process. We may have parts of our functionality which lack the necessary monitoring. It's important to still be able to trigger a coordinated incident response in those cases. For example, if our Support team start to receive requests that indicate a system issue, they need to have the power to trigger our response. Any PagerDuty employee has the ability to trigger our incident response process at any time. We trigger on any unplanned disruption or degradation of service to which any PagerDuty employee deems necessary of requiring co-ordinated incident response. Is a response required? If you are unsure of whether response is required, trigger our incident response process. All you need to do to start the process is page an IC in Slack with !ic page .","title":"Human Escalation"},{"location":"before/what_is_an_incident/#incident-severity","text":"Our severity definitions determine how severe we think an incident is, based on some pre-defined guidelines. The intent is to guide responders on the type of response they can provide. For example, the higher the severity, the riskier the decisions you can take to return the system to normal. Severities are useful to quickly determine whether something requires a more complex response, or whether it requires a co-ordinated response at all. However, they are not a black and white definition of what constitutes a major incident. If something is not covered by our severity definitions, but you think it requires incident response, then it requires incident response. We only need to know one thing: \"Is this a major incident?\". The severity level can be determined later, and isn't a requirement of triggering our response process.","title":"Incident Severity"},{"location":"before/what_is_an_incident/#mentality-shift","text":"One of the more important concepts of our incident response process is the mentality shift that needs to be made during an incident. We typically call this the \"Peacetime vs Wartime\" mentality shift. The idea is that the decision making process changes when you are in an incident situation, and you are able to take riskier actions than you would normally consider during day-to-day operations. It can be hard for responders to grasp this concept, and your incident response process can be held up by responders who stick to the peacetime way of thinking, not wanting to proceed with a potentially risky action. You can read more about peacetime vs wartime in the Responder Training Guide . Normal vs Emergency Some people don't like the \"Peacetime vs Wartime\" analogy, in which case you can use any other terms you feel appropriate. \"Normal vs Emergency\" is a common choice, but you could equally use \"OK vs Not OK\". It's not terribly important what name you give it, the important part is to make the mentality shift.","title":"Mentality Shift"},{"location":"during/during_an_incident/","text":"Information on what to do during a major incident. See our severity level descriptions for what constitutes a major incident. Documentation For your own internal documentation, you should make sure that this page has all of the necessary information prominently displayed. Such as: phone bridge numbers, Slack rooms, important chat commands, etc. Here is an example, #incident-chat https://a-voip-provider.com/incident-call +1 555 BIG FIRE (+1 555 244 3473) / PIN: 123456 Need an IC? Do !ic page in Slack For executive summary updates only, join #executive-summary-updates . Security Incident? If this is a security incident, you should follow the Security Incident Response process. Don't Panic! # Join the incident call and chat (see links above). Anyone is free to join the call or chat to observe and follow along with the incident. If you wish to participate however, you should join both. If you can't join the call for some reason, you should have a dedicated proxy for the call. Disjointed discussions in the chat room are ultimately distracting. Follow along with the call/chat, add any comments you feel are appropriate, but keep the discussion relevant to the problem at hand. If you are not an SME, try to filter any discussion through the primary SME for your service. Too many people discussing at once get become overwhelming, so we should try to maintain a hierarchical structure to the call if possible. Follow instructions from the Incident Commander. Is there no IC on the call? Manually page them via Slack, with !ic page in Slack. This will page the primary and backup IC's at the same time. Never hesitate to page the IC. It's much better to have them and not need them than the other way around. Steps for Incident Commander # Resolve the incident as quickly and as safely as possible, use the Deputy to assist you. Delegate any tasks to relevant experts at your discretion. Announce on the call and in Slack that you are the incident commander, who you have designated as deputy (usually the backup IC), and scribe. Identify if there is an obvious cause to the incident (recent deployment, spike in traffic, etc.), delegate investigation to relevant experts, Use the service experts on the call to assist in the analysis. They should be able to quickly provide confirmation of the cause, but not always. It's the call of the IC on how to proceed in cases where the cause is not positively known. Confer with service owners and use their knowledge to help you. Identify investigation & repair actions (roll back, rate-limit services, etc) and delegate actions to relevant service experts. Typically something like this (obviously not an exhaustive list), Bad Deployment: Roll it back. Web Application Stuck/Crashed: Do a rolling restart. Event Flood: Validate automatic throttling is sufficient, adjust manually if not. Data Center Outage: Validate automation has removed bad data center. Force it to do so if not. Degraded Service Behavior without load: Gather forensic data (heap dumps, etc), and consider doing a rolling restart. Listen for prompts from your Deputy regarding severity escalations, decide whether we need to announce publicly, and instruct customer liaison accordingly. Announcing publicly is at your discretion as IC. If you are unsure, then announce publicly (\"If in doubt, tweet it out\"). Keep track of your span of control . If the response starts to become larger, or the incident increases in complexity, consider splitting off sub-teams in order to get a more effective response. Once incident has recovered or is actively recovering, you can announce that the incident is over and that the call is ending. This usually indicates there's no more productive work to be done for the incident right now. Move the remaining, non-time-critical discussion to Slack. Follow up to ensure the customer liaison wraps up the incident publicly. Identify any post-incident clean-up work. You may need to perform debriefing/analysis of the underlying contributing factor. Once the call is over, you can start to follow the steps from After an Incident . Steps for Deputy # You are there to support the IC in whatever they need. Monitor the status of the incident, and notify the IC if/when the incident escalates in severity level. Follow instructions from the Incident Commander. Once the call is over, you can start to follow the steps from After an Incident . Steps for Scribe # You are there to document the key information from the incident in Slack. Update the Slack room with who the IC is, who the Deputy is, and that you're the scribe (if not already done). e.g. \"IC: Bob Boberson, Deputy: Deputy Deputyson, Scribe: Writer McWriterson\" Start our status monitoring bot so that all responders can see the current state without needing to ask. OfficerURL can help you to monitor the status on Slack, !status - Will tell you the current status. !status stalk - Will continually monitor the status and report it to the room every 30s. You should add notes to Slack when significant actions are taken, or findings are determined. You don't need to wait for the IC to direct this - use your own judgment. You should also add TODO notes to the Slack room that indicate follow-ups slated for later. Follow instructions from the Incident Commander. Once the call is over, you can start to follow the steps from After an Incident . Steps for Subject Matter Experts # You are there to support the incident commander in identifying the cause of the incident, suggesting and evaluation repair actions, and following through on the repair actions. Investigate the incident by analyzing any graphs or logs at your disposal. Announce all findings to the incident commander. If you are unsure of the cause, that's fine, state that you are investigating and provide regular updates to the IC. Announce all suggestions for resolution to the incident commander, it is their decision on how to proceed, do not follow any actions unless told to do so! Follow instructions from the incident commander. Once the call is over, you can start to follow the steps from After an Incident . Steps for Customer Liaison # Be on stand-by to post public facing messages regarding the incident. You will typically be required to update the status page and to send Tweets from our various accounts at certain times during the call. Follow instructions from the Incident Commander. Once the call is over, you can start to follow the steps from After an Incident . Steps for Internal Liaison # You are there to provide updates to internal stakeholders, and to mobilize additional internal responders as necessary. Be prepared to page other people as directed by the Incident Commander. Notify internal stakeholders as necessary, adding subscribers to the PagerDuty incident. We have pre-defined teams called \"SEV-1 Stakeholders\" and \"SEV-2 Stakeholders\" which can be used. Provide regular status updates in Slack (roughly every 30mins) to the executive team, giving an executive summary of the current status. Keep it short and to the point, and use @here . Follow instructions from the Incident Commander. Once the call is over, you can start to follow the steps from After an Incident .","title":"During an Incident"},{"location":"during/during_an_incident/#dont-panic","text":"Join the incident call and chat (see links above). Anyone is free to join the call or chat to observe and follow along with the incident. If you wish to participate however, you should join both. If you can't join the call for some reason, you should have a dedicated proxy for the call. Disjointed discussions in the chat room are ultimately distracting. Follow along with the call/chat, add any comments you feel are appropriate, but keep the discussion relevant to the problem at hand. If you are not an SME, try to filter any discussion through the primary SME for your service. Too many people discussing at once get become overwhelming, so we should try to maintain a hierarchical structure to the call if possible. Follow instructions from the Incident Commander. Is there no IC on the call? Manually page them via Slack, with !ic page in Slack. This will page the primary and backup IC's at the same time. Never hesitate to page the IC. It's much better to have them and not need them than the other way around.","title":"Don't Panic!"},{"location":"during/during_an_incident/#steps-for-incident-commander","text":"Resolve the incident as quickly and as safely as possible, use the Deputy to assist you. Delegate any tasks to relevant experts at your discretion. Announce on the call and in Slack that you are the incident commander, who you have designated as deputy (usually the backup IC), and scribe. Identify if there is an obvious cause to the incident (recent deployment, spike in traffic, etc.), delegate investigation to relevant experts, Use the service experts on the call to assist in the analysis. They should be able to quickly provide confirmation of the cause, but not always. It's the call of the IC on how to proceed in cases where the cause is not positively known. Confer with service owners and use their knowledge to help you. Identify investigation & repair actions (roll back, rate-limit services, etc) and delegate actions to relevant service experts. Typically something like this (obviously not an exhaustive list), Bad Deployment: Roll it back. Web Application Stuck/Crashed: Do a rolling restart. Event Flood: Validate automatic throttling is sufficient, adjust manually if not. Data Center Outage: Validate automation has removed bad data center. Force it to do so if not. Degraded Service Behavior without load: Gather forensic data (heap dumps, etc), and consider doing a rolling restart. Listen for prompts from your Deputy regarding severity escalations, decide whether we need to announce publicly, and instruct customer liaison accordingly. Announcing publicly is at your discretion as IC. If you are unsure, then announce publicly (\"If in doubt, tweet it out\"). Keep track of your span of control . If the response starts to become larger, or the incident increases in complexity, consider splitting off sub-teams in order to get a more effective response. Once incident has recovered or is actively recovering, you can announce that the incident is over and that the call is ending. This usually indicates there's no more productive work to be done for the incident right now. Move the remaining, non-time-critical discussion to Slack. Follow up to ensure the customer liaison wraps up the incident publicly. Identify any post-incident clean-up work. You may need to perform debriefing/analysis of the underlying contributing factor. Once the call is over, you can start to follow the steps from After an Incident .","title":"Steps for Incident Commander"},{"location":"during/during_an_incident/#steps-for-deputy","text":"You are there to support the IC in whatever they need. Monitor the status of the incident, and notify the IC if/when the incident escalates in severity level. Follow instructions from the Incident Commander. Once the call is over, you can start to follow the steps from After an Incident .","title":"Steps for Deputy"},{"location":"during/during_an_incident/#steps-for-scribe","text":"You are there to document the key information from the incident in Slack. Update the Slack room with who the IC is, who the Deputy is, and that you're the scribe (if not already done). e.g. \"IC: Bob Boberson, Deputy: Deputy Deputyson, Scribe: Writer McWriterson\" Start our status monitoring bot so that all responders can see the current state without needing to ask. OfficerURL can help you to monitor the status on Slack, !status - Will tell you the current status. !status stalk - Will continually monitor the status and report it to the room every 30s. You should add notes to Slack when significant actions are taken, or findings are determined. You don't need to wait for the IC to direct this - use your own judgment. You should also add TODO notes to the Slack room that indicate follow-ups slated for later. Follow instructions from the Incident Commander. Once the call is over, you can start to follow the steps from After an Incident .","title":"Steps for Scribe"},{"location":"during/during_an_incident/#steps-for-subject-matter-experts","text":"You are there to support the incident commander in identifying the cause of the incident, suggesting and evaluation repair actions, and following through on the repair actions. Investigate the incident by analyzing any graphs or logs at your disposal. Announce all findings to the incident commander. If you are unsure of the cause, that's fine, state that you are investigating and provide regular updates to the IC. Announce all suggestions for resolution to the incident commander, it is their decision on how to proceed, do not follow any actions unless told to do so! Follow instructions from the incident commander. Once the call is over, you can start to follow the steps from After an Incident .","title":"Steps for Subject Matter Experts"},{"location":"during/during_an_incident/#steps-for-customer-liaison","text":"Be on stand-by to post public facing messages regarding the incident. You will typically be required to update the status page and to send Tweets from our various accounts at certain times during the call. Follow instructions from the Incident Commander. Once the call is over, you can start to follow the steps from After an Incident .","title":"Steps for Customer Liaison"},{"location":"during/during_an_incident/#steps-for-internal-liaison","text":"You are there to provide updates to internal stakeholders, and to mobilize additional internal responders as necessary. Be prepared to page other people as directed by the Incident Commander. Notify internal stakeholders as necessary, adding subscribers to the PagerDuty incident. We have pre-defined teams called \"SEV-1 Stakeholders\" and \"SEV-2 Stakeholders\" which can be used. Provide regular status updates in Slack (roughly every 30mins) to the executive team, giving an executive summary of the current status. Keep it short and to the point, and use @here . Follow instructions from the Incident Commander. Once the call is over, you can start to follow the steps from After an Incident .","title":"Steps for Internal Liaison"},{"location":"during/security_incident_response/","text":"Last Reviewed: YYYY-MM-DD Last Tested: YYYY-MM-DD Incident Commander Required As with all major incidents at PagerDuty, security ones will also involve an Incident Commander, who will delegate the tasks to relevant resolvers. Tasks may be performed in parallel as assigned by the IC. Page one at the earliest possible opportunity !ic page . Not Sure it's a Security Incident? Trigger the process anyway. It's better to be safe than sorry. The Incident Commander will make a determination on if response is needed. Checklist # Details for each of these items are available in the next section. Stop the attack in progress. Cut off the attack vector. Assemble the response team. Isolate affected instances. Identify timeline of attack. Identify compromised data. Assess risk to other systems. Assess risk of re-attack. Apply additional mitigations, additions to monitoring, etc. Forensic analysis of compromised systems. Internal communication. Involve law enforcement. Reach out to external parties that may have been used as vector for attack. External communication. Attack Mitigation # Stop the attack as quickly as you can, via any means necessary. Shut down servers, network isolate them, turn off a data center if you have to. Some common things to try, Shutdown the instance from the provider console (do not delete or terminate if you can help it, as we'll need to do forensics). If you happen to be logged into the box you can try to, Re-instate our default iptables rules to restrict traffic. kill -9 any active session you think is an attacker. Change root password, and update /etc/shadow to lock out all other users. sudo shutdown now Cut Off Attack Vector # Identify the likely attack vectors and path/fix them so they cannot be re-exploited immediately after stopping the attack. If you suspect a third-party provider is compromised, delete all accounts except your own (and those of others who are physically present) and immediately rotate your password and MFA tokens. If you suspect a service application was an attack vector, disable any relevant code paths, or shut down the service entirely. Assemble Response Team # Identify the key responders for the security incident, and keep them all in the loop. Set up a secure method of communicating all information associated with the incident. Details on the incident (or even the fact that an incident has occurred) should be kept private to the responders until you are confident the attack is not being triggered internally. Page an Incident Commander if not already done so. They will also appoint the usual incident command roles. The incident command team will be responsible for keeping documentation of actions taken, and for notifying internal stakeholders as appropriate. Give the project an innocuous codename that can be used for chats/documents so if anyone overhears they don't realize it's a security incident. (e.g. sapphire-unicorn). Start the voice call if not already in progress. Setup chat room using the codename of the incident. Invite all responders to the voice call and chat room. The security team should always be included . A representative for any affected services should be included. Executive stakeholders and legal counsel should be invited at earliest possible opportunity, but prioritize operational responders first. Do not communicate with anyone not on the response team about the incident until forensics has been performed. The attack could be happening internally. Prefix all emails, and chat topics with \"Attorney Work Project\". Isolate Affected Instances # Any instances which were affected by the attack should be immediately isolated from any other instances. As soon as possible, an image of the system should be taken and put into a read-only cold storage for later forensic analysis. Blacklist the IP addresses for any affected instances from all other hosts. Turn off and shutdown the instances immediately if you didn't do that to stop the attack. Take a disk image for any disks attached to the instances, and ship them to an off-site cold storage location. You should make sure these images are read-only and cannot be tampered with. Identify Timeline of Attack # Work with all tools at your disposal to identify the timeline of the attack, along with exactly what the attacker did. Any reconnaissance the attacker performed on the system before the attack started. When the attacker gained access to the system. What actions the attacker performed on the system, and when. Identify how long the attacker had access to the system before they were detected, and before they were kicked out. Identify any queries the attacker ran on databases. Try to identify if the attacker still has access to the system via another back door. Monitor logs for unusual activity, etc. Compromised Data # Using forensic analysis of log files, time-series graphs, and any other information/tools at your disposal, attempt to identify what information was compromised (if any), Identify any data that was compromised during the attack. Was any data exfiltrated from a database? What keys were on the system that are now considering compromised? Was the attacker able to identify other components of the system (map out the network, etc). Find exactly what customer data has been compromised, if any. Assess Risk # Based on the data that was compromised, assess the risk to other systems. Does the attacker have enough information to find another way in? Were any passwords or keys stored on the host? If so, they should be considered compromised, regardless of how they were stored. Any user accounts that were used in the initial attack should rotate all of their keys and passwords on every other system they have an account. Apply Additional Mitigations # Start applying mitigations to other parts of your system. Rotate any compromised data. Identify any new alerting which is needed to notify of a similar breach. Block any IP addresses associated with the attack. Identify any keys/credentials that are compromised and revoke their access immediately. Forensic Analysis # Once you are confident the systems are secured, and enough monitoring is in place to detect another attack, you can move onto the forensic analysis stage. Take any read-only images you created, any access logs you have, and comb through them for more information about the attack. Identify exactly what happened, how it happened, and how to prevent it in future. Keep track of all IP addresses involved in the attack. Monitor logs for any attempt to regain access to the system by the attacker. Internal Communication # Delegate to: VP or Director of Engineering Communicate internally only once you are confident (via forensic analysis) that the attack was not sourced internally. Don't go into too much detail. Overview the timeline. Discuss mitigation steps taken. Follow up with more information once it is known. Liaise With Law Enforcement / External Actors # Delegate to: VP or Director of Engineering Work with law enforcement to identify the source of the attack, letting any system owners know that systems under their control may be compromised, etc. Contact local law enforcement. Contact FBI. Contact operators for any systems used in the attack, their systems may also have been compromised. Contact security companies to help in assessing risk and any PR next steps. Contact cyber insurance provider. External Communication # Delegate to: Marketing Team Once you have validated all of the information you have is accurate, have a timeline of events, and know exactly what information was compromised, how it was compromised, and sure that it won't happen again. Only then should you prepare and release a public statement to customers informing them of the compromised information and any steps they need to take. Include the date in the title of any announcement, so that it's never confused for a potential new breach. Don't say \"We take security very seriously\". It makes everyone cringe when they read it. Be honest, accept responsibility, and present the facts, along with exactly how we plan to prevent such things in future. Be as detailed as possible with the timeline. Be as detailed as possible in what information was compromised, and how it affects customers. If we were storing something we shouldn't have been, be honest about it. It'll come out later and it'll be much worse. Don't name and shame any external parties that might have caused the compromise. It's bad form. (Unless they've already publicly disclosed, in which case we can link to their disclosure). Release the external communication as soon as possible, preferably within a few days of the compromise. The longer we wait, the worse it will be. If possible, get in touch with customers' internal security teams before the general public notice is sent. Communicating During an Incident # Prefer voice call and Slack over any other methods. Avoid email, but if you absolutely need to for some reason, Subject of emails should be \"Attorney Work Project\" and nothing else. If email chain has ANY contacts not with the @pagerduty.com domain , make sure your emails are encrypted. Do not use SMS to communicate about the incident. The only exception is to tell someone to move to a more secure channel. e.g. \"Please join Slack ASAP\". Do not disseminate anything about the incident to those outside the response team until you have approval to do so. Additional Reading # Computer Security Incident Handling Guide (NIST) Incident Handler's Handbook (SANS) Responding to IT Security Incidents (Microsoft) Defining Incident Management Processes for CSIRTs: A Work in Progress (CMU) Creating and Managing Computer Security Incident Handling Teams (CSIRTS) (CERT)","title":"Security Incident"},{"location":"during/security_incident_response/#checklist","text":"Details for each of these items are available in the next section. Stop the attack in progress. Cut off the attack vector. Assemble the response team. Isolate affected instances. Identify timeline of attack. Identify compromised data. Assess risk to other systems. Assess risk of re-attack. Apply additional mitigations, additions to monitoring, etc. Forensic analysis of compromised systems. Internal communication. Involve law enforcement. Reach out to external parties that may have been used as vector for attack. External communication.","title":"Checklist"},{"location":"during/security_incident_response/#attack-mitigation","text":"Stop the attack as quickly as you can, via any means necessary. Shut down servers, network isolate them, turn off a data center if you have to. Some common things to try, Shutdown the instance from the provider console (do not delete or terminate if you can help it, as we'll need to do forensics). If you happen to be logged into the box you can try to, Re-instate our default iptables rules to restrict traffic. kill -9 any active session you think is an attacker. Change root password, and update /etc/shadow to lock out all other users. sudo shutdown now","title":"Attack Mitigation"},{"location":"during/security_incident_response/#cut-off-attack-vector","text":"Identify the likely attack vectors and path/fix them so they cannot be re-exploited immediately after stopping the attack. If you suspect a third-party provider is compromised, delete all accounts except your own (and those of others who are physically present) and immediately rotate your password and MFA tokens. If you suspect a service application was an attack vector, disable any relevant code paths, or shut down the service entirely.","title":"Cut Off Attack Vector"},{"location":"during/security_incident_response/#assemble-response-team","text":"Identify the key responders for the security incident, and keep them all in the loop. Set up a secure method of communicating all information associated with the incident. Details on the incident (or even the fact that an incident has occurred) should be kept private to the responders until you are confident the attack is not being triggered internally. Page an Incident Commander if not already done so. They will also appoint the usual incident command roles. The incident command team will be responsible for keeping documentation of actions taken, and for notifying internal stakeholders as appropriate. Give the project an innocuous codename that can be used for chats/documents so if anyone overhears they don't realize it's a security incident. (e.g. sapphire-unicorn). Start the voice call if not already in progress. Setup chat room using the codename of the incident. Invite all responders to the voice call and chat room. The security team should always be included . A representative for any affected services should be included. Executive stakeholders and legal counsel should be invited at earliest possible opportunity, but prioritize operational responders first. Do not communicate with anyone not on the response team about the incident until forensics has been performed. The attack could be happening internally. Prefix all emails, and chat topics with \"Attorney Work Project\".","title":"Assemble Response Team"},{"location":"during/security_incident_response/#isolate-affected-instances","text":"Any instances which were affected by the attack should be immediately isolated from any other instances. As soon as possible, an image of the system should be taken and put into a read-only cold storage for later forensic analysis. Blacklist the IP addresses for any affected instances from all other hosts. Turn off and shutdown the instances immediately if you didn't do that to stop the attack. Take a disk image for any disks attached to the instances, and ship them to an off-site cold storage location. You should make sure these images are read-only and cannot be tampered with.","title":"Isolate Affected Instances"},{"location":"during/security_incident_response/#identify-timeline-of-attack","text":"Work with all tools at your disposal to identify the timeline of the attack, along with exactly what the attacker did. Any reconnaissance the attacker performed on the system before the attack started. When the attacker gained access to the system. What actions the attacker performed on the system, and when. Identify how long the attacker had access to the system before they were detected, and before they were kicked out. Identify any queries the attacker ran on databases. Try to identify if the attacker still has access to the system via another back door. Monitor logs for unusual activity, etc.","title":"Identify Timeline of Attack"},{"location":"during/security_incident_response/#compromised-data","text":"Using forensic analysis of log files, time-series graphs, and any other information/tools at your disposal, attempt to identify what information was compromised (if any), Identify any data that was compromised during the attack. Was any data exfiltrated from a database? What keys were on the system that are now considering compromised? Was the attacker able to identify other components of the system (map out the network, etc). Find exactly what customer data has been compromised, if any.","title":"Compromised Data"},{"location":"during/security_incident_response/#assess-risk","text":"Based on the data that was compromised, assess the risk to other systems. Does the attacker have enough information to find another way in? Were any passwords or keys stored on the host? If so, they should be considered compromised, regardless of how they were stored. Any user accounts that were used in the initial attack should rotate all of their keys and passwords on every other system they have an account.","title":"Assess Risk"},{"location":"during/security_incident_response/#apply-additional-mitigations","text":"Start applying mitigations to other parts of your system. Rotate any compromised data. Identify any new alerting which is needed to notify of a similar breach. Block any IP addresses associated with the attack. Identify any keys/credentials that are compromised and revoke their access immediately.","title":"Apply Additional Mitigations"},{"location":"during/security_incident_response/#forensic-analysis","text":"Once you are confident the systems are secured, and enough monitoring is in place to detect another attack, you can move onto the forensic analysis stage. Take any read-only images you created, any access logs you have, and comb through them for more information about the attack. Identify exactly what happened, how it happened, and how to prevent it in future. Keep track of all IP addresses involved in the attack. Monitor logs for any attempt to regain access to the system by the attacker.","title":"Forensic Analysis"},{"location":"during/security_incident_response/#internal-communication","text":"Delegate to: VP or Director of Engineering Communicate internally only once you are confident (via forensic analysis) that the attack was not sourced internally. Don't go into too much detail. Overview the timeline. Discuss mitigation steps taken. Follow up with more information once it is known.","title":"Internal Communication"},{"location":"during/security_incident_response/#liaise-with-law-enforcement-external-actors","text":"Delegate to: VP or Director of Engineering Work with law enforcement to identify the source of the attack, letting any system owners know that systems under their control may be compromised, etc. Contact local law enforcement. Contact FBI. Contact operators for any systems used in the attack, their systems may also have been compromised. Contact security companies to help in assessing risk and any PR next steps. Contact cyber insurance provider.","title":"Liaise With Law Enforcement / External Actors"},{"location":"during/security_incident_response/#external-communication","text":"Delegate to: Marketing Team Once you have validated all of the information you have is accurate, have a timeline of events, and know exactly what information was compromised, how it was compromised, and sure that it won't happen again. Only then should you prepare and release a public statement to customers informing them of the compromised information and any steps they need to take. Include the date in the title of any announcement, so that it's never confused for a potential new breach. Don't say \"We take security very seriously\". It makes everyone cringe when they read it. Be honest, accept responsibility, and present the facts, along with exactly how we plan to prevent such things in future. Be as detailed as possible with the timeline. Be as detailed as possible in what information was compromised, and how it affects customers. If we were storing something we shouldn't have been, be honest about it. It'll come out later and it'll be much worse. Don't name and shame any external parties that might have caused the compromise. It's bad form. (Unless they've already publicly disclosed, in which case we can link to their disclosure). Release the external communication as soon as possible, preferably within a few days of the compromise. The longer we wait, the worse it will be. If possible, get in touch with customers' internal security teams before the general public notice is sent.","title":"External Communication"},{"location":"during/security_incident_response/#communicating-during-an-incident","text":"Prefer voice call and Slack over any other methods. Avoid email, but if you absolutely need to for some reason, Subject of emails should be \"Attorney Work Project\" and nothing else. If email chain has ANY contacts not with the @pagerduty.com domain , make sure your emails are encrypted. Do not use SMS to communicate about the incident. The only exception is to tell someone to move to a more secure channel. e.g. \"Please join Slack ASAP\". Do not disseminate anything about the incident to those outside the response team until you have approval to do so.","title":"Communicating During an Incident"},{"location":"during/security_incident_response/#additional-reading","text":"Computer Security Incident Handling Guide (NIST) Incident Handler's Handbook (SANS) Responding to IT Security Incidents (Microsoft) Defining Incident Management Processes for CSIRTs: A Work in Progress (CMU) Creating and Managing Computer Security Incident Handling Teams (CSIRTS) (CERT)","title":"Additional Reading"},{"location":"oncall/alerting_principles/","text":"We manage how we get alerted based on a simple principle, an alert is something which requires a human to perform an action . Anything else is a notification, which is something that we cannot control, and for which we cannot make any action to affect it. Notifications are useful, but they shouldn't be waking people up under any circumstance. Alert Priority # High Priority Alerts Anything that wakes up a human in the middle of the night should be immediately human actionable . If it is none of those things, then we need to adjust the alert to not page at those times. Priority Alerts Response High High-Priority PagerDuty Alert 24/7/365. Requires immediate human action . Medium High-Priority PagerDuty Alert during business hours only . Requires human action within 24 hours. Low Low-Priority PagerDuty Alert 24/7/365. Requires human action at some point. Notification Suppressed PagerDuty Event. No response required. Informational only. If you're setting up a new alert/notification, consider the chart above for how you want to alert people. Be mindful of not creating new high-priority alerts if they don't require an immediate response, for example. Priority Examples # \"Production service is failing for 75% of requests, automation is unable to resolve.\"_ # This would be a High priority page, requiring immediate human action to resolve. \"Production server disk space is filling, expected to be full in 48 hours. Log rotation is insufficient to resolve.\" # This would be a Medium priority page, requiring human action soon, but not immediately. \"An SSL certificate is due to expire in one week.\" # This would be a Low priority page, requiring human action some time soon. \"A deployment was successful.\" # This would be a Notification , and should be sent as a suppressed event. It provides useful context should an incident occur, but does not require notifying a human. Alert Content # We should ensure that alerts contain enough useful context to quickly identify the issue and any potential remediation steps. Alerts with generic titles or descriptions are not useful and can cause confusion. We have a set of guidelines for the content of alerts, which all our alerts should follow, Make the title/summary descriptive and concise. # \u2718 ALERT: Something went wrong. \u2713 Disk is 80% full on prod-web-loadbalancer-af5462ce . Make sure to include the metric which triggered the alert somewhere in the body. # \u2718 Diskspace on a disk is filling. \u2713 avg(last_1h):max:system.disk.in_use{env:prod-web-loadbalancer} by {host} > 0.8 The body should also include a description of what the actual problem is, and why it's an issue. # \u2718 Disk is full. \u2713 The disk on this host is at 80% capacity. If it becomes too full it could cause system instability as new files will not be able to be created and current files will not be written to. Provide clear steps to resolve the problem, or link to a run book. Alerts with neither of these things are useless. # \u2718 Fix it by deleting stuff. \u2713 Follow the run book here for identifying and resolving disk space issues: https://example.com/runbook/disk. Additionally, you should investigate whether log rotation thresholds are sufficient to prevent this happening again, the following run book has the necessary steps: https://example.com/runbook/log-rotate Testing Your Alerts # Testing is Critical An alert you haven't tested is equivalent to not having an alert at all. You cannot be sure it will alert you when the time comes. Testing that your alerting actually works is critical to proper service health, and should be included in any release planning / deployment efforts. Make sure to test all alerts that are added. This is usually covered as part of Failure Friday for any new service, however you should manually test them if you need it more quickly. Some things to test, Test that the threshold is set appropriately. We don't want noisy alerts. Test that you get alerted for the \"No Data\" condition if applicable. Generally, receiving no data is the same as breaking your threshold. Test that the alert resolves automatically when the metric returns to normal.","title":"Alerting Principles"},{"location":"oncall/alerting_principles/#alert-priority","text":"High Priority Alerts Anything that wakes up a human in the middle of the night should be immediately human actionable . If it is none of those things, then we need to adjust the alert to not page at those times. Priority Alerts Response High High-Priority PagerDuty Alert 24/7/365. Requires immediate human action . Medium High-Priority PagerDuty Alert during business hours only . Requires human action within 24 hours. Low Low-Priority PagerDuty Alert 24/7/365. Requires human action at some point. Notification Suppressed PagerDuty Event. No response required. Informational only. If you're setting up a new alert/notification, consider the chart above for how you want to alert people. Be mindful of not creating new high-priority alerts if they don't require an immediate response, for example.","title":"Alert Priority"},{"location":"oncall/alerting_principles/#priority-examples","text":"","title":"Priority Examples"},{"location":"oncall/alerting_principles/#production-service-is-failing-for-75-of-requests-automation-is-unable-to-resolve_","text":"This would be a High priority page, requiring immediate human action to resolve.","title":"\"Production service is failing for 75% of requests, automation is unable to resolve.\"_"},{"location":"oncall/alerting_principles/#production-server-disk-space-is-filling-expected-to-be-full-in-48-hours-log-rotation-is-insufficient-to-resolve","text":"This would be a Medium priority page, requiring human action soon, but not immediately.","title":"\"Production server disk space is filling, expected to be full in 48 hours. Log rotation is insufficient to resolve.\""},{"location":"oncall/alerting_principles/#an-ssl-certificate-is-due-to-expire-in-one-week","text":"This would be a Low priority page, requiring human action some time soon.","title":"\"An SSL certificate is due to expire in one week.\""},{"location":"oncall/alerting_principles/#a-deployment-was-successful","text":"This would be a Notification , and should be sent as a suppressed event. It provides useful context should an incident occur, but does not require notifying a human.","title":"\"A deployment was successful.\""},{"location":"oncall/alerting_principles/#alert-content","text":"We should ensure that alerts contain enough useful context to quickly identify the issue and any potential remediation steps. Alerts with generic titles or descriptions are not useful and can cause confusion. We have a set of guidelines for the content of alerts, which all our alerts should follow,","title":"Alert Content"},{"location":"oncall/alerting_principles/#make-the-titlesummary-descriptive-and-concise","text":"\u2718 ALERT: Something went wrong. \u2713 Disk is 80% full on prod-web-loadbalancer-af5462ce .","title":"Make the title/summary descriptive and concise."},{"location":"oncall/alerting_principles/#make-sure-to-include-the-metric-which-triggered-the-alert-somewhere-in-the-body","text":"\u2718 Diskspace on a disk is filling. \u2713 avg(last_1h):max:system.disk.in_use{env:prod-web-loadbalancer} by {host} > 0.8","title":"Make sure to include the metric which triggered the alert somewhere in the body."},{"location":"oncall/alerting_principles/#the-body-should-also-include-a-description-of-what-the-actual-problem-is-and-why-its-an-issue","text":"\u2718 Disk is full. \u2713 The disk on this host is at 80% capacity. If it becomes too full it could cause system instability as new files will not be able to be created and current files will not be written to.","title":"The body should also include a description of what the actual problem is, and why it's an issue."},{"location":"oncall/alerting_principles/#provide-clear-steps-to-resolve-the-problem-or-link-to-a-run-book-alerts-with-neither-of-these-things-are-useless","text":"\u2718 Fix it by deleting stuff. \u2713 Follow the run book here for identifying and resolving disk space issues: https://example.com/runbook/disk. Additionally, you should investigate whether log rotation thresholds are sufficient to prevent this happening again, the following run book has the necessary steps: https://example.com/runbook/log-rotate","title":"Provide clear steps to resolve the problem, or link to a run book. Alerts with neither of these things are useless."},{"location":"oncall/alerting_principles/#testing-your-alerts","text":"Testing is Critical An alert you haven't tested is equivalent to not having an alert at all. You cannot be sure it will alert you when the time comes. Testing that your alerting actually works is critical to proper service health, and should be included in any release planning / deployment efforts. Make sure to test all alerts that are added. This is usually covered as part of Failure Friday for any new service, however you should manually test them if you need it more quickly. Some things to test, Test that the threshold is set appropriately. We don't want noisy alerts. Test that you get alerted for the \"No Data\" condition if applicable. Generally, receiving no data is the same as breaking your threshold. Test that the alert resolves automatically when the metric returns to normal.","title":"Testing Your Alerts"},{"location":"oncall/being_oncall/","text":"A summary of expectations and helpful information for being on-call. What is On-Call? # Being on-call means that you are able to be contacted at any time in order to investigate and fix issues that may arise for the system you are responsible for. For example, if you are on-call for your service at PagerDuty, should any alarms be triggered for that service, you will receive a \"page\" (an alert on your mobile device, email, phone call, or SMS, etc.) giving you details on what has broken and how to fix it. You will be expected to take whatever actions are necessary in order to resolve the issue and return your service to a normal state. On-call responsibilities extend beyond normal office hours, and if you are on-call you are expected to be able to respond to issues, even at 2am. This sounds horrible (and it can be), but this is what our customers go through, and is the problem that the PagerDuty product itself is trying to fix! Responsibilities # Prepare Have your laptop and Internet with you (office, home, a MiFi dongle, a phone with a tethering plan, etc). Have a way to charge your MiFi. Team alert escalation happens within 5 minutes, set/stagger your notification timeouts (push, SMS, phone...) accordingly. Make sure PagerDuty texts and calls can bypass your \"Do Not Disturb\" settings . Be prepared (environment is set up, a current working copy of the necessary repos is local and functioning, you have configured and tested environments on workstations, your credentials for third-party services are current and so on...) Read our Incident Response documentation (that's this!) to understand how we handle serious incidents, what the different roles and methods of communication are, etc. Be aware of your upcoming on-call time (primary, backup) and arrange swaps around travel, vacations, appointments etc. Triage Acknowledge and act on alerts whenever you can (see the first \"Not responsibilities\" point below) Determine the urgency of the problem: Is it something that should be worked on right now or escalated into a major incident? (\"production server on fire\" situations. Security alerts) - do so. Is it some tactical work that doesn't have to happen during the night? (for example, disk utilization high watermark, but there's plenty of space left and the trend is not indicating impending doom) - snooze the alert until a more suitable time (working hours, the next morning...) and get back to fixing it then. Check Slack for current activity. Often (but not always) actions that could potentially cause alerts will be announced there. Does the alert and your initial investigation indicate a general problem or an issue with a specific service that the relevant team should look into? If it does not look like a problem you are the expert for, then escalate to another team. Fix You are empowered to dive into any problem and act to fix it. Involve other team members as necessary: do not hesitate to escalate if you cannot figure out the cause within a reasonable timeframe or if the service / alert is something you have not tackled before. If the issue is not very time sensitive and you have other priority work, create a JIRA ticket to keep a track of it (with an appropriate severity). Improve If a particular issue keeps happening; if an issue alerts often but turns out to be a preventable non-issue \u2013 perhaps improving this should be a longer-term task. Disks that fill up, logs that should be rotated, noisy alerts... If information is difficult / impossible to find, write it down. Constantly refactor and improve our knowledge base and documentation. Add redundant links and pointers if your mental model of the wiki / codebase does not match the way it is currently organized. Support When your on-call \"shift\" ends, let the next on-call know about issues that have not been resolved yet and other experiences of note. If you are making a change that impacts the schedule (adding / removing yourself, for example), let others know since many of us make arrangements around the on-call schedule well in advance. Support each other: when doing activities that might generate plenty of pages, it is courteous to \"take the page\" away from the on-call by notifying them and scheduling an override for the duration. Not Responsibilities # No expectation to be the first to acknowledge all of the alerts during the on-call period. Commute (and other necessary distractions) are facts of life, and sometimes it is not possible to receive or act on an alert before it escalates. That's what we have the backup on-call and schedule for. No expectation to fix all issues by yourself. No one knows everything. Your whole team is here to help. There is no shame, and much to be learned, by escalating issues you are not certain about. Our motto is \"Never hesitate to escalate\". Service owners will always know more about how their stuff works. Especially if our and their documentation is lacking, double-checking with the relevant team avoids mistakes. Measure twice, cut once \u2013 and it's often best to let the subject matter expert do the cutting. Recommendations # If your team is starting its own on-call rotation, here are some scheduling recommendations from the Operations team. Always have a backup schedule. Yes, this means two people being on-call at the same time, however it takes a lot of the stress off of the primary if they know they have a specific backup they can contact, rather than trying to chose a random member of the team. A backup shift should generally come directly after a primary shift. It gives chance for the previous primary to pass on additional context which may have come up during their shift. It also helps to prevent people from sitting on issues with the intent of letting the next shift fix it. The third-level of your escalation (after backup schedule) should probably be your entire team. This should hopefully never happen (it's happened once in the history of the Operations team), but when it does, it's useful to be able to just get the next available person. Team managers can (and should) be part of your normal rotation. It gives a better insight into what has been going on. New members of the team should shadow your on-call rotation during the first few weeks. They should get all alerts, and should follow along with what you are doing. (All new employees shadow the Operations team for one week of on-call, but it's useful to have new team members shadow your team rotations also. Just not at the same time). We recommend you set your escalation timeout to 5 minutes. This should be plenty of time for someone to acknowledge the incident if they're able to. If they're not able to within 5 minutes, then they're probably not in a good position to respond to the incident anyway. When going off-call, you should provide a quick summary to the next on-call about any issues that may come up during their shift. A service has been flapping, an issue is likely to re-occur, etc. If you want to be formal, this can be a written report via email, but generally a verbal summary is sufficient. Notification Method Recommendations # You are free to set up your notification rules as you see fit, to match how you would like to best respond to incidents. If you're not sure how to configure them, the Operations team has some recommendations, Use Push Notification and Email as your first method of notification. Most of us have phones with us at all times, so this is a prudent first method and is usually sufficient. Use Phone and/or SMS notification each minute after, until the escalation time. If Push didn't work, then it's likely you need something stronger, like a phone call. Keep calling every minute until it's too late. If you don't pick up by the 3rd time, then it's unlikely you are able to respond, and the incident will get escalated away from you. Etiquette # If the current on-call comes into the office at 12pm looking tired, it's not because they're lazy. They probably got paged in the night. Cut them some slack and be nice. Don't acknowledge an incident out from under someone else. If you didn't get paged for the incident, then you shouldn't be acknowledging it. Add a comment with your notes instead. If you are testing something, or performing an action that you know will cause a page, it's customary to \"take the pager\" for the time during which you will be testing. Notify the person on-call that you are taking the pager for the next hour while you test. \"Never hesitate to escalate\" - Never feel ashamed to rope in someone else if you're not sure how to resolve an issue. Likewise, never look down on someone else if they ask you for help. Always consider covering an hour or so of someone else's on-call time if they request it and you are able. We all have lives which might get in the way of on-call time, and one day it might be you who needs to swap their on-call time in order to have a night out with your friend from out of town. If an issue comes up during your on-call shift for which you got paged, you are responsible for resolving it. Even if it takes 3 hours and there's only 1 hour left of your shift. You can hand over to the next on-call if they agree, but you should never assume that's possible.","title":"Being On-Call"},{"location":"oncall/being_oncall/#what-is-on-call","text":"Being on-call means that you are able to be contacted at any time in order to investigate and fix issues that may arise for the system you are responsible for. For example, if you are on-call for your service at PagerDuty, should any alarms be triggered for that service, you will receive a \"page\" (an alert on your mobile device, email, phone call, or SMS, etc.) giving you details on what has broken and how to fix it. You will be expected to take whatever actions are necessary in order to resolve the issue and return your service to a normal state. On-call responsibilities extend beyond normal office hours, and if you are on-call you are expected to be able to respond to issues, even at 2am. This sounds horrible (and it can be), but this is what our customers go through, and is the problem that the PagerDuty product itself is trying to fix!","title":"What is On-Call?"},{"location":"oncall/being_oncall/#responsibilities","text":"Prepare Have your laptop and Internet with you (office, home, a MiFi dongle, a phone with a tethering plan, etc). Have a way to charge your MiFi. Team alert escalation happens within 5 minutes, set/stagger your notification timeouts (push, SMS, phone...) accordingly. Make sure PagerDuty texts and calls can bypass your \"Do Not Disturb\" settings . Be prepared (environment is set up, a current working copy of the necessary repos is local and functioning, you have configured and tested environments on workstations, your credentials for third-party services are current and so on...) Read our Incident Response documentation (that's this!) to understand how we handle serious incidents, what the different roles and methods of communication are, etc. Be aware of your upcoming on-call time (primary, backup) and arrange swaps around travel, vacations, appointments etc. Triage Acknowledge and act on alerts whenever you can (see the first \"Not responsibilities\" point below) Determine the urgency of the problem: Is it something that should be worked on right now or escalated into a major incident? (\"production server on fire\" situations. Security alerts) - do so. Is it some tactical work that doesn't have to happen during the night? (for example, disk utilization high watermark, but there's plenty of space left and the trend is not indicating impending doom) - snooze the alert until a more suitable time (working hours, the next morning...) and get back to fixing it then. Check Slack for current activity. Often (but not always) actions that could potentially cause alerts will be announced there. Does the alert and your initial investigation indicate a general problem or an issue with a specific service that the relevant team should look into? If it does not look like a problem you are the expert for, then escalate to another team. Fix You are empowered to dive into any problem and act to fix it. Involve other team members as necessary: do not hesitate to escalate if you cannot figure out the cause within a reasonable timeframe or if the service / alert is something you have not tackled before. If the issue is not very time sensitive and you have other priority work, create a JIRA ticket to keep a track of it (with an appropriate severity). Improve If a particular issue keeps happening; if an issue alerts often but turns out to be a preventable non-issue \u2013 perhaps improving this should be a longer-term task. Disks that fill up, logs that should be rotated, noisy alerts... If information is difficult / impossible to find, write it down. Constantly refactor and improve our knowledge base and documentation. Add redundant links and pointers if your mental model of the wiki / codebase does not match the way it is currently organized. Support When your on-call \"shift\" ends, let the next on-call know about issues that have not been resolved yet and other experiences of note. If you are making a change that impacts the schedule (adding / removing yourself, for example), let others know since many of us make arrangements around the on-call schedule well in advance. Support each other: when doing activities that might generate plenty of pages, it is courteous to \"take the page\" away from the on-call by notifying them and scheduling an override for the duration.","title":"Responsibilities"},{"location":"oncall/being_oncall/#not-responsibilities","text":"No expectation to be the first to acknowledge all of the alerts during the on-call period. Commute (and other necessary distractions) are facts of life, and sometimes it is not possible to receive or act on an alert before it escalates. That's what we have the backup on-call and schedule for. No expectation to fix all issues by yourself. No one knows everything. Your whole team is here to help. There is no shame, and much to be learned, by escalating issues you are not certain about. Our motto is \"Never hesitate to escalate\". Service owners will always know more about how their stuff works. Especially if our and their documentation is lacking, double-checking with the relevant team avoids mistakes. Measure twice, cut once \u2013 and it's often best to let the subject matter expert do the cutting.","title":"Not Responsibilities"},{"location":"oncall/being_oncall/#recommendations","text":"If your team is starting its own on-call rotation, here are some scheduling recommendations from the Operations team. Always have a backup schedule. Yes, this means two people being on-call at the same time, however it takes a lot of the stress off of the primary if they know they have a specific backup they can contact, rather than trying to chose a random member of the team. A backup shift should generally come directly after a primary shift. It gives chance for the previous primary to pass on additional context which may have come up during their shift. It also helps to prevent people from sitting on issues with the intent of letting the next shift fix it. The third-level of your escalation (after backup schedule) should probably be your entire team. This should hopefully never happen (it's happened once in the history of the Operations team), but when it does, it's useful to be able to just get the next available person. Team managers can (and should) be part of your normal rotation. It gives a better insight into what has been going on. New members of the team should shadow your on-call rotation during the first few weeks. They should get all alerts, and should follow along with what you are doing. (All new employees shadow the Operations team for one week of on-call, but it's useful to have new team members shadow your team rotations also. Just not at the same time). We recommend you set your escalation timeout to 5 minutes. This should be plenty of time for someone to acknowledge the incident if they're able to. If they're not able to within 5 minutes, then they're probably not in a good position to respond to the incident anyway. When going off-call, you should provide a quick summary to the next on-call about any issues that may come up during their shift. A service has been flapping, an issue is likely to re-occur, etc. If you want to be formal, this can be a written report via email, but generally a verbal summary is sufficient.","title":"Recommendations"},{"location":"oncall/being_oncall/#notification-method-recommendations","text":"You are free to set up your notification rules as you see fit, to match how you would like to best respond to incidents. If you're not sure how to configure them, the Operations team has some recommendations, Use Push Notification and Email as your first method of notification. Most of us have phones with us at all times, so this is a prudent first method and is usually sufficient. Use Phone and/or SMS notification each minute after, until the escalation time. If Push didn't work, then it's likely you need something stronger, like a phone call. Keep calling every minute until it's too late. If you don't pick up by the 3rd time, then it's unlikely you are able to respond, and the incident will get escalated away from you.","title":"Notification Method Recommendations"},{"location":"oncall/being_oncall/#etiquette","text":"If the current on-call comes into the office at 12pm looking tired, it's not because they're lazy. They probably got paged in the night. Cut them some slack and be nice. Don't acknowledge an incident out from under someone else. If you didn't get paged for the incident, then you shouldn't be acknowledging it. Add a comment with your notes instead. If you are testing something, or performing an action that you know will cause a page, it's customary to \"take the pager\" for the time during which you will be testing. Notify the person on-call that you are taking the pager for the next hour while you test. \"Never hesitate to escalate\" - Never feel ashamed to rope in someone else if you're not sure how to resolve an issue. Likewise, never look down on someone else if they ask you for help. Always consider covering an hour or so of someone else's on-call time if they request it and you are able. We all have lives which might get in the way of on-call time, and one day it might be you who needs to swap their on-call time in order to have a night out with your friend from out of town. If an issue comes up during your on-call shift for which you got paged, you are responsible for resolving it. Even if it takes 3 hours and there's only 1 hour left of your shift. You can hand over to the next on-call if they agree, but you should never assume that's possible.","title":"Etiquette"},{"location":"oncall/whos_oncall/","text":"Organizational structures vary, but these are general guidelines about the way different functions in a business relate to incident response. Generally speaking, every department should have a primary point of contact, on-call rotation, and clear escalation path. Organizations should always strive to minimize dependencies and empower response teams as much as possible, but in novel situations you do not know who you will need to help out. A clear system for recruiting responders from all parts of the business ensures that when the unexpected happens, responders don\u2019t waste time on manual processes or ambiguous points of contact. Engineering # Engineers are typically the primary responders and subject matter experts during incident response. Which engineering teams are involved in which responses varies with a company\u2019s operational model. In some, a designated \u201cOperations\u201d or \u201cSite Reliability Engineering\u201d team may have initial responsibility for triage and assessment of new issues. At PagerDuty, the on-call engineer for an affected service has initial triage and assessment responsibility. Customer Support / Customer Success # Support is the voice of the customer during incident response. A member of the Customer Support team is the default Customer Liaison within the response team, updating customers and stakeholders about incident status through Twitter, an internal Slack channel, and other channels as needed. They may also serve as an internal liaison to keep stakeholders within the company up to date. Marketing # Marketing or Public Relations is the primary response team for any Public Relations Incident. Additionally, Marketing or Public Relations should be engaged in any incident of scope or severity such that the company\u2019s brand or image is at risk, or where public updates need to be sent through customer communication channels such as mass email or the company blog. Product Management and Design # Product Managers and Designers are often on the hook for helping response teams make decisions when product functionality is impacted across multiple services or products. For example, if the response team has to decide which service to bring back up first, a Product Manager can help decide which one is more customer impacting. Product will also be involved in the post-mortem process , for both scheduling follow-up action against other work as well as advising on any required product changes due to the issue. Executive Team # Clear processes for updating the executive team during major incident response helps ensure organization leadership has the context and information they need, and prevents executive swoops . Additionally, while the Incident Commander has final authority during response, occasionally a major incident may require action at the highest levels of a company. For example, a senior executive may want to reach out to an impacted customer or partner to manage their relationship and help assure them the issue is getting the attention it needs. Sales # Sales are generally stakeholders during incident response. Salespeople should be notified when there is any impact to the product that may affect demos or customer conversations, and account owners should understand any potential impact to their accounts. Human Resources (HR) # HR is generally involved in any response to incidents affecting the safety or health of employees. Additionally, during a security incident, the security team may need to coordinate with HR both for management of an internal attacker as well as protection of any affected employees. Finance # Finance is most often a stakeholder during incident response, and should be kept up to date of any impacts to the platform that may affect billing, accounting, or end-of-month/quarter activities. However, finance should also have a clear on-call rotation and escalation path, as there may be components of incident response that require third-party account management or related actions. Consider Your Entire Organization There may be other parts of your organization that need to be part of incident response, either as responders or stakeholders. It is important to identify the different areas of your business and think through situations in which they may need to be involved, as well as ensure that anyone on-call has proper incident response training and understands their responsibilities.","title":"Who's On-Call?"},{"location":"oncall/whos_oncall/#engineering","text":"Engineers are typically the primary responders and subject matter experts during incident response. Which engineering teams are involved in which responses varies with a company\u2019s operational model. In some, a designated \u201cOperations\u201d or \u201cSite Reliability Engineering\u201d team may have initial responsibility for triage and assessment of new issues. At PagerDuty, the on-call engineer for an affected service has initial triage and assessment responsibility.","title":"Engineering"},{"location":"oncall/whos_oncall/#customer-support-customer-success","text":"Support is the voice of the customer during incident response. A member of the Customer Support team is the default Customer Liaison within the response team, updating customers and stakeholders about incident status through Twitter, an internal Slack channel, and other channels as needed. They may also serve as an internal liaison to keep stakeholders within the company up to date.","title":"Customer Support / Customer Success"},{"location":"oncall/whos_oncall/#marketing","text":"Marketing or Public Relations is the primary response team for any Public Relations Incident. Additionally, Marketing or Public Relations should be engaged in any incident of scope or severity such that the company\u2019s brand or image is at risk, or where public updates need to be sent through customer communication channels such as mass email or the company blog.","title":"Marketing"},{"location":"oncall/whos_oncall/#product-management-and-design","text":"Product Managers and Designers are often on the hook for helping response teams make decisions when product functionality is impacted across multiple services or products. For example, if the response team has to decide which service to bring back up first, a Product Manager can help decide which one is more customer impacting. Product will also be involved in the post-mortem process , for both scheduling follow-up action against other work as well as advising on any required product changes due to the issue.","title":"Product Management and Design"},{"location":"oncall/whos_oncall/#executive-team","text":"Clear processes for updating the executive team during major incident response helps ensure organization leadership has the context and information they need, and prevents executive swoops . Additionally, while the Incident Commander has final authority during response, occasionally a major incident may require action at the highest levels of a company. For example, a senior executive may want to reach out to an impacted customer or partner to manage their relationship and help assure them the issue is getting the attention it needs.","title":"Executive Team"},{"location":"oncall/whos_oncall/#sales","text":"Sales are generally stakeholders during incident response. Salespeople should be notified when there is any impact to the product that may affect demos or customer conversations, and account owners should understand any potential impact to their accounts.","title":"Sales"},{"location":"oncall/whos_oncall/#human-resources-hr","text":"HR is generally involved in any response to incidents affecting the safety or health of employees. Additionally, during a security incident, the security team may need to coordinate with HR both for management of an internal attacker as well as protection of any affected employees.","title":"Human Resources (HR)"},{"location":"oncall/whos_oncall/#finance","text":"Finance is most often a stakeholder during incident response, and should be kept up to date of any impacts to the platform that may affect billing, accounting, or end-of-month/quarter activities. However, finance should also have a clear on-call rotation and escalation path, as there may be components of incident response that require third-party account management or related actions. Consider Your Entire Organization There may be other parts of your organization that need to be part of incident response, either as responders or stakeholders. It is important to identify the different areas of your business and think through situations in which they may need to be involved, as well as ensure that anyone on-call has proper incident response training and understands their responsibilities.","title":"Finance"},{"location":"resources/anti_patterns/","text":"Process Anti-Patterns These are some processes that we've found do not work well. We've either tried them in the past and regretted it, or spent considerable time thinking about them and ultimately rejected them. We want to document them to make sure we don't repeat mistakes or wonder why a decision was made later down the line. The list is in no particular order. Getting everyone on the call. # Believe it or not, we used to page every single engineer at PagerDuty when we had a SEV-2 incident. If a SEV-2 happened at 3am, then we'd be paging the entire engineering department at 3am. There a few reasons we went down this path, When the company was smaller, there were only a few engineers. So this process worked well, since you really did need every engineer on the call. Rather than triaging an incident and then having to page in additional people, if everyone is on the call to begin with then the thinking was that we'd get a faster response. As we grew our engineer department, this did not scale well at all, and problems quickly became apparent, Most of the people on the call had nothing to do. They'd been woken up for no reason. Paging people has a cost impact. Both in employee health, and in finance. Waking up your entire engineering department at 3am means nothing productive is going to be done the next day, across the entire department. People who weren't on-call would still get paged. It's important to maintain an effective span of control on any incident response. If you have more than 7 or 8 people directly reporting to the Incident Commander things can quickly get overwhelming. We now will only page the engineers who are on-call for a specific service, rather than the entire team. If more responders are required, then they will be mobilized by the Internal Liaison to join the response. 9 times out of 10 we don't need additional responders, so the rest of the engineering department can get some rest without interference. This results in a happier engineering department and a more streamlined response process. Forcing everyone to stay on the call. # Our original thinking was that if you've been mobilized onto a response call, then we would need you to stick around until the end, since if you were needed at one point, chances are you'll be needed again before the incident is resolved. Unfortunately, we've since found that that's never really the case. Typically someone will be mobilized to investigate a specific system, or perform a specific action, after which time there would be nothing further required from them. We'd have a call full of people who weren't doing anything and could've gone back to sleep. This can also encourage 'hero' mentalities where individuals feel pressured that they have to stay on a call. Now we ask people to leave the call if they're no longer needed. Once the Incident Commander has ascertained which systems are impacted, they let representatives for the other systems leave the call so they can get some rest. You can always mobilize them again if they're really needed. Most of the time they won't be needed again, so we're optimizing for the 99% case. Too frequent status updates. # Executives need to know what's going on and want to be provided status updates every 5 minutes to keep them in the loop. The problem with this is that you'll spend the entire time providing status updates rather than resolving the incident. We've found that providing status updates every 20-30 minutes during a major incident is a typical cadence that works. This ensures we're not just providing updates for the sake of providing updates, but that they're more likely to have some actual useful information in them. That's not to say you can't provide updates more frequently if there really is new information to share, but it shouldn't be a requirement. We want to spend as much time fixing the incident as we can, but we also want to make sure we keep stakeholders in the loop. It's a delicate balance that's easy to get wrong. Assuming silence means no progress. # It can be common to assume silence on an incident call means that nothing is being done, however this is rarely the case. When joining a call, be aware that a chatterless session can be acceptable and reasonable. Silence usually means everyone is working on fixing the problem rather than talking and providing updates. We're not playing a game of \"Keep talking and nobody gets fired\". The Incident Commander is the one who should be doing most of the talking on a call. They will typically fill silence with a status update if appropriate, but others within the organization need to be trained to know that silence on a call isn't a bad thing, and doesn't mean that progress has stalled. Making sure staff are aware of this ahead of time will prevent awkward conversations during an incident call, which would be ultimately distracting from resolving the incident. Litigating severities during the incident call. # The start of a lot of incident calls in PagerDuty's past consisted of a discussion around whether we were really in a SEV-2 situation, or whether it was a smaller issue that could be handled without an incident call. This discussion would usually take up quite a bit of time, as everyone wanted to weigh in. The problem was that while you're having this discussion, the incident is still going on behind the scenes, and by the time you've finished it has become a SEV-1 and you've just wasted 10 minutes discussing severities. We now have a rule: We do not discuss incident severity during an incident call , we always assume it's the higher severity and treat it as such. So if we're not sure if it's a SEV-2 or SEV-3, we treat it like a SEV-2 and move on. We've already spun up the gears of incident response and paged responders, so even if it turns out to be a SEV-3, we may as well continue with the process and treat it as practice if nothing else. Hesitating to escalate to other responders. # If it's 3am and you're responding to an incident, we have had cases where a Subject Matter Expert (SME) would be stuck attempting to debug an issue, and they would be reluctant to involve another member of their team due to the time of day. This would end up causing our incident to last longer than it needed to. \"Never hesitate to escalate\" is now one of our mantras. If you're stuck on a problem and it's 3am, don't hesitate to page someone more knowledgable to help resolve the situation. Don't go too far with this and page everyone, otherwise you fall into the earlier anti-pattern. But you should never feel like you can't page someone if you need help. Discussing process and policy decisions during the incident call. # There are occasions where responders don't agree with the incident response policies and processes we might use. Sometimes this will cause a discussion during the incident call, which ends up derailing the process for everyone and causing the underlying incident to last longer, hindering the response. It is absolutely OK to have disagreements with the process and to want to make changes (in fact, this is something we encourage, as it allows us to iteratively improve our process), however during an incident is not the time to have that discussion. Policy and processes should not be discussed during an incident , just like with severities. The current process should be followed during an incident, and any concerns should be raised afterwards, either during a post-mortem or directly to the team managing the incident response process. Neglecting the post-mortem and followup activities. # It's tempting once an incident is resolved to not bother with the post-mortem. Either you feel like the cause is well known, or you don't feel that it's worth it. Don't fall into this trap! A post-mortem is always worthwhile. People were mobilized to respond to an incident, which had a cost associated with it. We want to be sure that we understand why that happened, so we can avoid that cost in future. Don't make the mistake of neglecting a post-mortem after an incident. Without a post-mortem you fail to recognize what you're doing right, where you could improve, and most importantly, how to avoid making the same exact mistakes next time around. A well-designed, blameless post-mortem allows teams to continuously learn, and serves as a way to iteratively improve your infrastructure and incident response process. If you mobilize responders and determine it's not a \"real\" incident, you should still conduct a post-mortem. Because the next time you're going to mobilize responders again and waste time. Find out why incident response was triggered when it may not have been needed, and fix that problem. Being too focussed on the problem in front of you. # As a responder to an incident, you would typically be focussed on the specific task in front of you. The Incident Commander generally being the person who has the bigger picture of what is going on. There can be a tendency for SME's to become too focussed on the problem they see in front of them, rather than taking the bigger picture into account. This usually presents itself on an incident call with an SME constantly bringing up the same issue without listening to instructions from the incident commander, and having tunnel vision for the specific issue on their system. Instructions from the Incident Commander should always be followed, as they will typically have more overall context on what is going on. Try not to fall into the trap of being hyper-focused on the problem in front of you, so much so that you derail the process. We want to treat the cause, not a symptom of the incident. Being averse to policy and process changes. # Once a stable process is in place, and incidents are getting resolved, there can be lots of hesitation and resistance to changing that process. \"If it ain't broke don't fix it\", etc. As your company grows, your response will need to change. Holding on to your old processes and practices for too long can hinder your incident response going forward. Don't be reckless, of course, but try to introduce sensible changes and don't be afraid to make changes which might slow things down in the short-term, but will make things faster in the long-run . These are the hardest changes to make, but usually the most worthwhile. Trying to take on multiple roles. # In past PagerDuty incidents, we've had instances where the Incident Commander has started to assume the Subject Matter Expert role and attempted to solve the problem themselves. This usually happens when the IC is an engineer in their day-to-day role. They are in an incident where the cause appears to be a system they know very well and have the requisite knowledge to fix. Wanting to solve the incident quickly, the IC will start to try and solve the problem. Sometimes you might get lucky and it will resolve the incident, but most of the time the immediately visible issue isn't necessarily the underlying cause of the incident. By the time that becomes apparent, you have an Incident Commander who is not paying attention to the other systems and is just focussed on the one problem in front of them. This effectively means there's no incident commander, as they would be busy trying to fix the problem. Inevitably, the problem turns out to be much bigger than anticipated and the response has become completely derailed. You cannot take on another role at the same time as being an Incident Commander . It can be a difficult to be an IC when you want to jump in as an SME, but you must resist the temptation to abandon the role of IC. If you really are the only person able to solve the problem, you should handover to another Incident Commander and then assume the role of SME. This ensures that the response process remains on track with a dedicated Incident Commander. Remember that the job of an IC also includes preparing backup plans in case the current action doesn't resolve the incident. If you're acting as an SME fixing on a particular issue, you're not considering the backup plan. Trying to be a hero. # It can be tempting to try and solve every issue yourself if you're acting as a Subject Matter Expert. Every request that comes up, you want to jump on it and say you'll take care of it. You'll be the indispensable one who solves all the problems. As noble as the intent is, it rarely leads to an efficient outcome. You want to avoid as much multi-tasking as possible during an incident, and focus on one problem at a time. Don't try to solve everything yourself . If multiple requests are coming up for your area of expertise, delegate them to other experts, even paging backup responders if required. Likewise, if another SME has been assigned a task, don't do the task on their behalf without consulting with them first. While you are trying to help, it will end up hindering the response as you'll have two people working on the same issue, and they may be interfering with each other in unexpected ways. Not disseminating policy changes to responders. # We've fallen into the trap in the past of making policy and process changes by simply updating our internal documentation (i.e. this), assuming everyone would read the documentation before an incident. Which of course, never happens. Any policy changes need to be appropriately disseminated to your responders ahead of time so that there are no surprises during an incident. This can be in the form of an email, or an update into a chat room, but big policy changes should never be a surprise to responders. Requiring Incident Commanders to have deep technical knowledge. # This is a trap we fell into in our early days of incident response. We had several strong technical requirements for any new Incident Commanders, aiming to only have IC's with deep technical expertise, the intention being that they could diagnose issues very quickly. When it became apparent that we would need a large selection of IC's in order to maintain an effective on-call rotation, we soon realised that we had artificially restricted our pool of potential IC candidates. Incident Commanders can come from all across your organization, and don't need to be technical experts . Since Incident Commanders only coordinate the response, they don't need deep technical knowledge of the system in order to perform their role. The Subject Matter Experts are the ones who need the deep technical knowledge. The Incident Commanders only require a high-level knowledge of how the system works. Where data flows in, how systems use it, and where data flows out. The technical details can be left to the SMEs, with the IC asking relevant questions. By dropping our strong technical requirements for Incident Commanders, we've been able to dramatically increase our pool of IC's, maintain a high level of quality and efficiency in our response, and help spread empathy for on-call workload to a larger portion of the company.","title":"Anti-Patterns"},{"location":"resources/anti_patterns/#getting-everyone-on-the-call","text":"Believe it or not, we used to page every single engineer at PagerDuty when we had a SEV-2 incident. If a SEV-2 happened at 3am, then we'd be paging the entire engineering department at 3am. There a few reasons we went down this path, When the company was smaller, there were only a few engineers. So this process worked well, since you really did need every engineer on the call. Rather than triaging an incident and then having to page in additional people, if everyone is on the call to begin with then the thinking was that we'd get a faster response. As we grew our engineer department, this did not scale well at all, and problems quickly became apparent, Most of the people on the call had nothing to do. They'd been woken up for no reason. Paging people has a cost impact. Both in employee health, and in finance. Waking up your entire engineering department at 3am means nothing productive is going to be done the next day, across the entire department. People who weren't on-call would still get paged. It's important to maintain an effective span of control on any incident response. If you have more than 7 or 8 people directly reporting to the Incident Commander things can quickly get overwhelming. We now will only page the engineers who are on-call for a specific service, rather than the entire team. If more responders are required, then they will be mobilized by the Internal Liaison to join the response. 9 times out of 10 we don't need additional responders, so the rest of the engineering department can get some rest without interference. This results in a happier engineering department and a more streamlined response process.","title":"Getting everyone on the call."},{"location":"resources/anti_patterns/#forcing-everyone-to-stay-on-the-call","text":"Our original thinking was that if you've been mobilized onto a response call, then we would need you to stick around until the end, since if you were needed at one point, chances are you'll be needed again before the incident is resolved. Unfortunately, we've since found that that's never really the case. Typically someone will be mobilized to investigate a specific system, or perform a specific action, after which time there would be nothing further required from them. We'd have a call full of people who weren't doing anything and could've gone back to sleep. This can also encourage 'hero' mentalities where individuals feel pressured that they have to stay on a call. Now we ask people to leave the call if they're no longer needed. Once the Incident Commander has ascertained which systems are impacted, they let representatives for the other systems leave the call so they can get some rest. You can always mobilize them again if they're really needed. Most of the time they won't be needed again, so we're optimizing for the 99% case.","title":"Forcing everyone to stay on the call."},{"location":"resources/anti_patterns/#too-frequent-status-updates","text":"Executives need to know what's going on and want to be provided status updates every 5 minutes to keep them in the loop. The problem with this is that you'll spend the entire time providing status updates rather than resolving the incident. We've found that providing status updates every 20-30 minutes during a major incident is a typical cadence that works. This ensures we're not just providing updates for the sake of providing updates, but that they're more likely to have some actual useful information in them. That's not to say you can't provide updates more frequently if there really is new information to share, but it shouldn't be a requirement. We want to spend as much time fixing the incident as we can, but we also want to make sure we keep stakeholders in the loop. It's a delicate balance that's easy to get wrong.","title":"Too frequent status updates."},{"location":"resources/anti_patterns/#assuming-silence-means-no-progress","text":"It can be common to assume silence on an incident call means that nothing is being done, however this is rarely the case. When joining a call, be aware that a chatterless session can be acceptable and reasonable. Silence usually means everyone is working on fixing the problem rather than talking and providing updates. We're not playing a game of \"Keep talking and nobody gets fired\". The Incident Commander is the one who should be doing most of the talking on a call. They will typically fill silence with a status update if appropriate, but others within the organization need to be trained to know that silence on a call isn't a bad thing, and doesn't mean that progress has stalled. Making sure staff are aware of this ahead of time will prevent awkward conversations during an incident call, which would be ultimately distracting from resolving the incident.","title":"Assuming silence means no progress."},{"location":"resources/anti_patterns/#litigating-severities-during-the-incident-call","text":"The start of a lot of incident calls in PagerDuty's past consisted of a discussion around whether we were really in a SEV-2 situation, or whether it was a smaller issue that could be handled without an incident call. This discussion would usually take up quite a bit of time, as everyone wanted to weigh in. The problem was that while you're having this discussion, the incident is still going on behind the scenes, and by the time you've finished it has become a SEV-1 and you've just wasted 10 minutes discussing severities. We now have a rule: We do not discuss incident severity during an incident call , we always assume it's the higher severity and treat it as such. So if we're not sure if it's a SEV-2 or SEV-3, we treat it like a SEV-2 and move on. We've already spun up the gears of incident response and paged responders, so even if it turns out to be a SEV-3, we may as well continue with the process and treat it as practice if nothing else.","title":"Litigating severities during the incident call."},{"location":"resources/anti_patterns/#hesitating-to-escalate-to-other-responders","text":"If it's 3am and you're responding to an incident, we have had cases where a Subject Matter Expert (SME) would be stuck attempting to debug an issue, and they would be reluctant to involve another member of their team due to the time of day. This would end up causing our incident to last longer than it needed to. \"Never hesitate to escalate\" is now one of our mantras. If you're stuck on a problem and it's 3am, don't hesitate to page someone more knowledgable to help resolve the situation. Don't go too far with this and page everyone, otherwise you fall into the earlier anti-pattern. But you should never feel like you can't page someone if you need help.","title":"Hesitating to escalate to other responders."},{"location":"resources/anti_patterns/#discussing-process-and-policy-decisions-during-the-incident-call","text":"There are occasions where responders don't agree with the incident response policies and processes we might use. Sometimes this will cause a discussion during the incident call, which ends up derailing the process for everyone and causing the underlying incident to last longer, hindering the response. It is absolutely OK to have disagreements with the process and to want to make changes (in fact, this is something we encourage, as it allows us to iteratively improve our process), however during an incident is not the time to have that discussion. Policy and processes should not be discussed during an incident , just like with severities. The current process should be followed during an incident, and any concerns should be raised afterwards, either during a post-mortem or directly to the team managing the incident response process.","title":"Discussing process and policy decisions during the incident call."},{"location":"resources/anti_patterns/#neglecting-the-post-mortem-and-followup-activities","text":"It's tempting once an incident is resolved to not bother with the post-mortem. Either you feel like the cause is well known, or you don't feel that it's worth it. Don't fall into this trap! A post-mortem is always worthwhile. People were mobilized to respond to an incident, which had a cost associated with it. We want to be sure that we understand why that happened, so we can avoid that cost in future. Don't make the mistake of neglecting a post-mortem after an incident. Without a post-mortem you fail to recognize what you're doing right, where you could improve, and most importantly, how to avoid making the same exact mistakes next time around. A well-designed, blameless post-mortem allows teams to continuously learn, and serves as a way to iteratively improve your infrastructure and incident response process. If you mobilize responders and determine it's not a \"real\" incident, you should still conduct a post-mortem. Because the next time you're going to mobilize responders again and waste time. Find out why incident response was triggered when it may not have been needed, and fix that problem.","title":"Neglecting the post-mortem and followup activities."},{"location":"resources/anti_patterns/#being-too-focussed-on-the-problem-in-front-of-you","text":"As a responder to an incident, you would typically be focussed on the specific task in front of you. The Incident Commander generally being the person who has the bigger picture of what is going on. There can be a tendency for SME's to become too focussed on the problem they see in front of them, rather than taking the bigger picture into account. This usually presents itself on an incident call with an SME constantly bringing up the same issue without listening to instructions from the incident commander, and having tunnel vision for the specific issue on their system. Instructions from the Incident Commander should always be followed, as they will typically have more overall context on what is going on. Try not to fall into the trap of being hyper-focused on the problem in front of you, so much so that you derail the process. We want to treat the cause, not a symptom of the incident.","title":"Being too focussed on the problem in front of you."},{"location":"resources/anti_patterns/#being-averse-to-policy-and-process-changes","text":"Once a stable process is in place, and incidents are getting resolved, there can be lots of hesitation and resistance to changing that process. \"If it ain't broke don't fix it\", etc. As your company grows, your response will need to change. Holding on to your old processes and practices for too long can hinder your incident response going forward. Don't be reckless, of course, but try to introduce sensible changes and don't be afraid to make changes which might slow things down in the short-term, but will make things faster in the long-run . These are the hardest changes to make, but usually the most worthwhile.","title":"Being averse to policy and process changes."},{"location":"resources/anti_patterns/#trying-to-take-on-multiple-roles","text":"In past PagerDuty incidents, we've had instances where the Incident Commander has started to assume the Subject Matter Expert role and attempted to solve the problem themselves. This usually happens when the IC is an engineer in their day-to-day role. They are in an incident where the cause appears to be a system they know very well and have the requisite knowledge to fix. Wanting to solve the incident quickly, the IC will start to try and solve the problem. Sometimes you might get lucky and it will resolve the incident, but most of the time the immediately visible issue isn't necessarily the underlying cause of the incident. By the time that becomes apparent, you have an Incident Commander who is not paying attention to the other systems and is just focussed on the one problem in front of them. This effectively means there's no incident commander, as they would be busy trying to fix the problem. Inevitably, the problem turns out to be much bigger than anticipated and the response has become completely derailed. You cannot take on another role at the same time as being an Incident Commander . It can be a difficult to be an IC when you want to jump in as an SME, but you must resist the temptation to abandon the role of IC. If you really are the only person able to solve the problem, you should handover to another Incident Commander and then assume the role of SME. This ensures that the response process remains on track with a dedicated Incident Commander. Remember that the job of an IC also includes preparing backup plans in case the current action doesn't resolve the incident. If you're acting as an SME fixing on a particular issue, you're not considering the backup plan.","title":"Trying to take on multiple roles."},{"location":"resources/anti_patterns/#trying-to-be-a-hero","text":"It can be tempting to try and solve every issue yourself if you're acting as a Subject Matter Expert. Every request that comes up, you want to jump on it and say you'll take care of it. You'll be the indispensable one who solves all the problems. As noble as the intent is, it rarely leads to an efficient outcome. You want to avoid as much multi-tasking as possible during an incident, and focus on one problem at a time. Don't try to solve everything yourself . If multiple requests are coming up for your area of expertise, delegate them to other experts, even paging backup responders if required. Likewise, if another SME has been assigned a task, don't do the task on their behalf without consulting with them first. While you are trying to help, it will end up hindering the response as you'll have two people working on the same issue, and they may be interfering with each other in unexpected ways.","title":"Trying to be a hero."},{"location":"resources/anti_patterns/#not-disseminating-policy-changes-to-responders","text":"We've fallen into the trap in the past of making policy and process changes by simply updating our internal documentation (i.e. this), assuming everyone would read the documentation before an incident. Which of course, never happens. Any policy changes need to be appropriately disseminated to your responders ahead of time so that there are no surprises during an incident. This can be in the form of an email, or an update into a chat room, but big policy changes should never be a surprise to responders.","title":"Not disseminating policy changes to responders."},{"location":"resources/anti_patterns/#requiring-incident-commanders-to-have-deep-technical-knowledge","text":"This is a trap we fell into in our early days of incident response. We had several strong technical requirements for any new Incident Commanders, aiming to only have IC's with deep technical expertise, the intention being that they could diagnose issues very quickly. When it became apparent that we would need a large selection of IC's in order to maintain an effective on-call rotation, we soon realised that we had artificially restricted our pool of potential IC candidates. Incident Commanders can come from all across your organization, and don't need to be technical experts . Since Incident Commanders only coordinate the response, they don't need deep technical knowledge of the system in order to perform their role. The Subject Matter Experts are the ones who need the deep technical knowledge. The Incident Commanders only require a high-level knowledge of how the system works. Where data flows in, how systems use it, and where data flows out. The technical details can be left to the SMEs, with the IC asking relevant questions. By dropping our strong technical requirements for Incident Commanders, we've been able to dramatically increase our pool of IC's, maintain a high level of quality and efficiency in our response, and help spread empathy for on-call workload to a larger portion of the company.","title":"Requiring Incident Commanders to have deep technical knowledge."},{"location":"resources/chatops/","text":"Throughout this documentation, references are made to various chat commands, all starting with an exclamation (e.g. !ir page ). We have bots running in our Slack rooms which watch for these commands and execute various actions for us when they're detected. This page gives an overview of the commands we've referenced in this documentation, and what they do behind the scenes. Incident Response # Our !ir commands poll the OpsGenie API behind the scenes for various on-call schedules we specify. It caches the names and contact details for the current on-call users, so that if there's any issue in making API requests, the funtionality isn't impacted. !ir # This command lists out the current Incident Commander(s) on-call, their phone numbers, and a message telling users how to page them. !ir page # This is the command we use to manually trigger our incident response process. It will page the current Incident Commander(s) on-call (the primary, the backup, and any trainees who are shadowing). It will also create a new incident in Jira that will be used for reporting purposes and a new Slack channel for discussions about the incident (aka an incident war room). Links to the Jira incident and new Slack channel will be displayed in the channel where the command was entered. If for any reason we are unable to page the Incident Commanders automatically, the bot will let us know that it has failed, and give us the phone numbers for the relevant people so we can manually call them. Here is some example output from our test bot, where we have simulated being unable to page via OpsGenie due to an unresponsive API call. !ir responders # This works similarly to the !ir command, but it uses all of the team schedules instead of just the Incident Commander schedules. It will list out all the current people who are on-call for each team. This is useful to also know who will likely be joining the incident call momentarily. !ir page responders # This works similarly to !ir page , but it pages all of the team responders instead of the Incident Commander(s). This is rarely used, since generally only the relevant team will get paged. However, sometimes we require an \"all hands on deck\" response, and need the ability to quickly page all the current on-calls. !ir who <user> # Sometimes we may need to identify a specific individual to bring them onto a call. This command lists out the contact info for a specific user, and a message telling users how to page them. !ir page <user> # This will page a specific person by username. Status # Our !status commands look at our internal monitoring systems to determine the current system state, as reported by the systems themselves. This is the status our alerting tooling uses to automatically notify us of issues. !status # This will tell us the current overview of our system state. It will also alert us if it is unable to check for the status, since that could also be an indication of an issue. Typically though, it will hopefully show a status of NORMAL . !status stalk # This does the same as the above, only it polls every 30s until we stop it (with !status unstalk ). It will only report the status into the chat room if it has changed since the last time it checked. We have this running during an incident so we can easily see if our system is getting worse or recovering without having to manually check our monitoring.","title":"ChatOps"},{"location":"resources/chatops/#incident-response","text":"Our !ir commands poll the OpsGenie API behind the scenes for various on-call schedules we specify. It caches the names and contact details for the current on-call users, so that if there's any issue in making API requests, the funtionality isn't impacted.","title":"Incident Response"},{"location":"resources/chatops/#ir","text":"This command lists out the current Incident Commander(s) on-call, their phone numbers, and a message telling users how to page them.","title":"!ir"},{"location":"resources/chatops/#ir-page","text":"This is the command we use to manually trigger our incident response process. It will page the current Incident Commander(s) on-call (the primary, the backup, and any trainees who are shadowing). It will also create a new incident in Jira that will be used for reporting purposes and a new Slack channel for discussions about the incident (aka an incident war room). Links to the Jira incident and new Slack channel will be displayed in the channel where the command was entered. If for any reason we are unable to page the Incident Commanders automatically, the bot will let us know that it has failed, and give us the phone numbers for the relevant people so we can manually call them. Here is some example output from our test bot, where we have simulated being unable to page via OpsGenie due to an unresponsive API call.","title":"!ir page"},{"location":"resources/chatops/#ir-responders","text":"This works similarly to the !ir command, but it uses all of the team schedules instead of just the Incident Commander schedules. It will list out all the current people who are on-call for each team. This is useful to also know who will likely be joining the incident call momentarily.","title":"!ir responders"},{"location":"resources/chatops/#ir-page-responders","text":"This works similarly to !ir page , but it pages all of the team responders instead of the Incident Commander(s). This is rarely used, since generally only the relevant team will get paged. However, sometimes we require an \"all hands on deck\" response, and need the ability to quickly page all the current on-calls.","title":"!ir page responders"},{"location":"resources/chatops/#ir-who-ltusergt","text":"Sometimes we may need to identify a specific individual to bring them onto a call. This command lists out the contact info for a specific user, and a message telling users how to page them.","title":"!ir who &amp;lt;user&amp;gt;"},{"location":"resources/chatops/#ir-page-ltusergt","text":"This will page a specific person by username.","title":"!ir page &amp;lt;user&amp;gt;"},{"location":"resources/chatops/#status","text":"Our !status commands look at our internal monitoring systems to determine the current system state, as reported by the systems themselves. This is the status our alerting tooling uses to automatically notify us of issues.","title":"Status"},{"location":"resources/chatops/#status_1","text":"This will tell us the current overview of our system state. It will also alert us if it is unable to check for the status, since that could also be an indication of an issue. Typically though, it will hopefully show a status of NORMAL .","title":"!status"},{"location":"resources/chatops/#status-stalk","text":"This does the same as the above, only it polls every 30s until we stop it (with !status unstalk ). It will only report the status into the chat room if it has changed since the last time it checked. We have this running during an incident so we can easily see if our system is getting worse or recovering without having to manually check our monitoring.","title":"!status stalk"},{"location":"resources/reading/","text":"Credit: Axelle B This is a collection of additional reading on the topic of incident response that we've found useful. Books # Incident Management for Operations (Rob Schnepp, Ron Vidal, Chris Hawley) Incident Response (Kenneth R. van Wyk, Richard Forno) The Checklist Manifesto (Atul Gawande) The Field Guide to Understanding Human Error (Sidney Dekker) Normal Accidents: Living with High-Risk Technologies (Charles Perrow) Site Reliability Engineering (Google) IT Disaster Response: Lessons Learned in the Field (Greg D. Moore) Comparative Emergency Management (David A. McEntire, Ph.D.) Documents # Debriefing Facilitation Guide (Etsy) Talks # Every Minute Counts: Leading Heroku's Incident Response (Blake Gentry) Three Analytical Traps in Accident Investigation (Dr. Johan Bergstr\u00f6m) Official Resources # US National Incident Management System (NIMS) (FEMA) UK Government Fire and Rescue Manual - Incident Command (UK.GOV) New Zealand Coordinated Incident Management System (CIMS) (NZCDEM) The Australasian Inter-Service Incident Management System (AIIMS) (AFAC) Academic Emergency Management and Related Courses (FEMA) Other Useful Resources # Informed's NIMS Incident Command System Field Guide (Michael J. Ward)","title":"Reading"},{"location":"resources/reading/#books","text":"Incident Management for Operations (Rob Schnepp, Ron Vidal, Chris Hawley) Incident Response (Kenneth R. van Wyk, Richard Forno) The Checklist Manifesto (Atul Gawande) The Field Guide to Understanding Human Error (Sidney Dekker) Normal Accidents: Living with High-Risk Technologies (Charles Perrow) Site Reliability Engineering (Google) IT Disaster Response: Lessons Learned in the Field (Greg D. Moore) Comparative Emergency Management (David A. McEntire, Ph.D.)","title":"Books"},{"location":"resources/reading/#documents","text":"Debriefing Facilitation Guide (Etsy)","title":"Documents"},{"location":"resources/reading/#talks","text":"Every Minute Counts: Leading Heroku's Incident Response (Blake Gentry) Three Analytical Traps in Accident Investigation (Dr. Johan Bergstr\u00f6m)","title":"Talks"},{"location":"resources/reading/#official-resources","text":"US National Incident Management System (NIMS) (FEMA) UK Government Fire and Rescue Manual - Incident Command (UK.GOV) New Zealand Coordinated Incident Management System (CIMS) (NZCDEM) The Australasian Inter-Service Incident Management System (AIIMS) (AFAC) Academic Emergency Management and Related Courses (FEMA)","title":"Official Resources"},{"location":"resources/reading/#other-useful-resources","text":"Informed's NIMS Incident Command System Field Guide (Michael J. Ward)","title":"Other Useful Resources"},{"location":"training/customer_liaison/","text":"So you want to be a customer liaison? You've come to the right place! Purpose # The purpose of the Customer Liaison is to be the primary individual in charge of notifying our customers of the current conditions, and informing the Incident Commander of any relevant feedback from customers as the incident progresses. It's important for the rest of the command staff to be able to focus on the problem at hand, rather than worrying about crafting messages to customers. Your job as Customer Liaison is to listen to the call, watch the incident Slack room, and track incoming customer support requests, keeping track of what's going on and how far the incident is progressing (still investigating vs close to resolution). The Incident Commander will instruct you to notify customers of the incident and keep them updated at various points throughout the call. You will be required to craft the message, gain approval from the IC, and then disseminate that message to customers. Prerequisites # Before you can be a Customer Liaison, it is expected that you meet the following criteria. Don't worry if you don't meet them all yet, you can still continue with training! Excellent verbal and written communication skills . Be a member of the Customer Support team , or have had customer support training. Responsibilities # Read up on our Different Roles for Incidents to see what is expected from a Customer Liaison, as well as what we expect from the other roles you'll be interacting with. Training Process # There is no formal training process for this role, you should feel free to contact our Customer Support team to learn more. Customer Liaison # The objective of a customer liaison is to keep our customers informed during an incident as to what is happening, and to act as a voice for our customers to the Incident Commander. It is important for customers to have visibility into how they are impacted by an incident we are having, and to have insight into the fact that the problem is actively being worked on. Crafting a public message for customers is tricky, especially on platforms such as Twitter where the number of characters you can use are limited. But here are some general tips for crafting a public message, Prepare a default message in advance. One that can be used for the initial update if the scope of the issue is unknown. Be honest. Never lie, and never guess. Work with the incident commander if you are unsure as to what is actually happening. Provide transparent information to customers. If we are dropping the ball, be upfront about it. Describe our progress in resolving the incident. \"We are aware of an incident...\" \"We are investigating delayed notifications...\" \"A fix has been applied and is currently being deployed...\" \"The issue has been resolved...\" Be clear about how the incident is affecting customers. This is the primary piece of information customers will care about. Are notifications delayed? Is the website loading slowly but still working? Provide any workarounds customers can use until the incident is resolved. \u2718 Don't estimate resolution times. Never say something like \"We expect this incident to be resolved in 10 minutes\" . Something else could happen, and customers get angry when you set an expectation you can't keep. \u2718 Don't provide too much detail. Customers don't care if application-server-123 is having issues, they care that they are not getting notifications. Make sure the information you provide is relevant and not just noise. Incident Call Procedures and Lingo # The Steps for Customer Liaison provide a detailed description of what you should be doing during an incident. Here are some examples of phrases and patterns you should use during incident calls. Gaining Message Approval # After you have crafted the public message, you should gain approval from the IC before posting it publicly. Simply copy the message into Slack and wait for verbal/written confirmation from the IC before proceeding. (You) Message for customers: \"We are currently experiencing delayed notifications and are actively investigating the issue.\" (IC): Looks good, go ahead and post. It's important to get sign-off from the IC before posting as the nature of the incident may have changed while you were crafting the message, and new information might now be known and need to be included. Notification of Customer Response # You may be receiving calls and emails from customers during the incident. This provides useful context for the incident commander, as it gives an indication of the scope of the incident. You should keep the IC apprised of any relevant information from customers. We've had 6 customers call so far and say they haven't received notifications for the last several minutes. This can provide the IC with information which affects which areas we investigate first, or an indication of how the incident is progressing.","title":"Customer Liaison"},{"location":"training/customer_liaison/#purpose","text":"The purpose of the Customer Liaison is to be the primary individual in charge of notifying our customers of the current conditions, and informing the Incident Commander of any relevant feedback from customers as the incident progresses. It's important for the rest of the command staff to be able to focus on the problem at hand, rather than worrying about crafting messages to customers. Your job as Customer Liaison is to listen to the call, watch the incident Slack room, and track incoming customer support requests, keeping track of what's going on and how far the incident is progressing (still investigating vs close to resolution). The Incident Commander will instruct you to notify customers of the incident and keep them updated at various points throughout the call. You will be required to craft the message, gain approval from the IC, and then disseminate that message to customers.","title":"Purpose"},{"location":"training/customer_liaison/#prerequisites","text":"Before you can be a Customer Liaison, it is expected that you meet the following criteria. Don't worry if you don't meet them all yet, you can still continue with training! Excellent verbal and written communication skills . Be a member of the Customer Support team , or have had customer support training.","title":"Prerequisites"},{"location":"training/customer_liaison/#responsibilities","text":"Read up on our Different Roles for Incidents to see what is expected from a Customer Liaison, as well as what we expect from the other roles you'll be interacting with.","title":"Responsibilities"},{"location":"training/customer_liaison/#training-process","text":"There is no formal training process for this role, you should feel free to contact our Customer Support team to learn more.","title":"Training Process"},{"location":"training/customer_liaison/#customer-liaison","text":"The objective of a customer liaison is to keep our customers informed during an incident as to what is happening, and to act as a voice for our customers to the Incident Commander. It is important for customers to have visibility into how they are impacted by an incident we are having, and to have insight into the fact that the problem is actively being worked on. Crafting a public message for customers is tricky, especially on platforms such as Twitter where the number of characters you can use are limited. But here are some general tips for crafting a public message, Prepare a default message in advance. One that can be used for the initial update if the scope of the issue is unknown. Be honest. Never lie, and never guess. Work with the incident commander if you are unsure as to what is actually happening. Provide transparent information to customers. If we are dropping the ball, be upfront about it. Describe our progress in resolving the incident. \"We are aware of an incident...\" \"We are investigating delayed notifications...\" \"A fix has been applied and is currently being deployed...\" \"The issue has been resolved...\" Be clear about how the incident is affecting customers. This is the primary piece of information customers will care about. Are notifications delayed? Is the website loading slowly but still working? Provide any workarounds customers can use until the incident is resolved. \u2718 Don't estimate resolution times. Never say something like \"We expect this incident to be resolved in 10 minutes\" . Something else could happen, and customers get angry when you set an expectation you can't keep. \u2718 Don't provide too much detail. Customers don't care if application-server-123 is having issues, they care that they are not getting notifications. Make sure the information you provide is relevant and not just noise.","title":"Customer Liaison"},{"location":"training/customer_liaison/#incident-call-procedures-and-lingo","text":"The Steps for Customer Liaison provide a detailed description of what you should be doing during an incident. Here are some examples of phrases and patterns you should use during incident calls.","title":"Incident Call Procedures and Lingo"},{"location":"training/customer_liaison/#gaining-message-approval","text":"After you have crafted the public message, you should gain approval from the IC before posting it publicly. Simply copy the message into Slack and wait for verbal/written confirmation from the IC before proceeding. (You) Message for customers: \"We are currently experiencing delayed notifications and are actively investigating the issue.\" (IC): Looks good, go ahead and post. It's important to get sign-off from the IC before posting as the nature of the incident may have changed while you were crafting the message, and new information might now be known and need to be included.","title":"Gaining Message Approval"},{"location":"training/customer_liaison/#notification-of-customer-response","text":"You may be receiving calls and emails from customers during the incident. This provides useful context for the incident commander, as it gives an indication of the scope of the incident. You should keep the IC apprised of any relevant information from customers. We've had 6 customers call so far and say they haven't received notifications for the last several minutes. This can provide the IC with information which affects which areas we investigate first, or an indication of how the incident is progressing.","title":"Notification of Customer Response"},{"location":"training/deputy/","text":"Credit: oregondot @ Flickr So you want to be a deputy? You've come to the right place! Purpose # The purpose of the Deputy is to support the IC by keeping track of timers, notifying the IC of important information, and paging other people as directed by the IC. It's important for the IC to focus on the problem at hand, rather than worrying about monitoring timers. The deputy is there to help support the IC and keep them focused on the incident. As a Deputy, you will be expected to take over command from the IC if they request it. You should not be performing any remediations, checking graphs, or investigating logs . Those tasks will be delegated to the resolvers by the IC. Prerequisites # Before you can be a Deputy, it is expected that you meet the following criteria. Don't worry if you don't meet them all yet, you can still continue with training! Be trained as an Incident Commander . Responsibilities # Read up on our Different Roles for Incidents to see what is expected from a Deputy, as well as what we expect from the other roles you'll be interacting with. Training Process # The training process for a Deputy is quite simple. Follow our Incident Commander Training . Read this page. Incident Call Procedures and Lingo # The Steps for Deputy provide a detailed description of what you should be doing during an incident. Here are some examples of phrases and patterns you should use during incident calls. Alert IC to Timers # You are expected to keep track of how long the incident has been running for, and provide callouts to the IC every 10 minutes so they can take actions such as increasing the severity, or asking Support to Tweet out. This is as simple as telling the IC on the call, IC, be advised the incident is now at the 10 minute mark. Similarly, when the IC asks for someone to get back to them in X minutes, you are expected to keep track of that. You should remind the IC when that time has been reached. IC, be advised the timer for [TEAM]'s investigation is up.","title":"Deputy"},{"location":"training/deputy/#purpose","text":"The purpose of the Deputy is to support the IC by keeping track of timers, notifying the IC of important information, and paging other people as directed by the IC. It's important for the IC to focus on the problem at hand, rather than worrying about monitoring timers. The deputy is there to help support the IC and keep them focused on the incident. As a Deputy, you will be expected to take over command from the IC if they request it. You should not be performing any remediations, checking graphs, or investigating logs . Those tasks will be delegated to the resolvers by the IC.","title":"Purpose"},{"location":"training/deputy/#prerequisites","text":"Before you can be a Deputy, it is expected that you meet the following criteria. Don't worry if you don't meet them all yet, you can still continue with training! Be trained as an Incident Commander .","title":"Prerequisites"},{"location":"training/deputy/#responsibilities","text":"Read up on our Different Roles for Incidents to see what is expected from a Deputy, as well as what we expect from the other roles you'll be interacting with.","title":"Responsibilities"},{"location":"training/deputy/#training-process","text":"The training process for a Deputy is quite simple. Follow our Incident Commander Training . Read this page.","title":"Training Process"},{"location":"training/deputy/#incident-call-procedures-and-lingo","text":"The Steps for Deputy provide a detailed description of what you should be doing during an incident. Here are some examples of phrases and patterns you should use during incident calls.","title":"Incident Call Procedures and Lingo"},{"location":"training/deputy/#alert-ic-to-timers","text":"You are expected to keep track of how long the incident has been running for, and provide callouts to the IC every 10 minutes so they can take actions such as increasing the severity, or asking Support to Tweet out. This is as simple as telling the IC on the call, IC, be advised the incident is now at the 10 minute mark. Similarly, when the IC asks for someone to get back to them in X minutes, you are expected to keep track of that. You should remind the IC when that time has been reached. IC, be advised the timer for [TEAM]'s investigation is up.","title":"Alert IC to Timers"},{"location":"training/glossary/","text":"Ever wonder what all of those strange words you sometimes see in our documentation mean? This page is here to help. Incident Commander / IC # The incident commander is the person responsible for bringing any major incident to resolution. They are the highest ranking individual on any major incident call, regardless of their day-to-day rank. Their decisions made as commander are final. More info . Deputy # Typically the backup IC. The deputy's job is to support the IC during the call, providing them with any help they need. More info . Scribe # The scribe's job is to keep a log of all activities performed during the call in a written chat log on Slack. More info . Resolver # A person on the incident call who is able to help resolve issues within a particular system. Also referred to as an SME (see below). More info . SME # \"Subject Matter Expert\", someone who is an expert in a particular service or subject who can provide information to the IC, and perform resolution actions for a particular system. More info . Command Staff # The Command Staff consists of the Incident Commander, Deputy, and Scribe. CAN Report # CAN stands for \"Conditions\" \"Actions\" \"Needs\", if an IC asks you for a CAN report, you should provide the current state of your service (condition), what actions need to be taken to return it to a healthy state (actions), and what support you need in order to perform the actions (needs). Severity / Sev # How severe the incident is. The \"sev\" of an incident determines the type of response we give. The higher the severity, the higher the likelihood of making risky actions to resolve the situation. More info . Span of Control # Refers to the number of direct reports you have. For example, if the IC has 10 people as direct reports on a call, they have a large span of control. We aim to make the span of control as minimal as we can while still being productive. Grenade Thrower # Someone who joins the call at a late time in the game, and provides information that completely derails the current thinking. They then leave almost immediately. Executive Swoop # When an executive comes on the call and drops some sort of bombshell. A version of grenade throwing.","title":"Glossary"},{"location":"training/glossary/#incident-commander-ic","text":"The incident commander is the person responsible for bringing any major incident to resolution. They are the highest ranking individual on any major incident call, regardless of their day-to-day rank. Their decisions made as commander are final. More info .","title":"Incident Commander / IC"},{"location":"training/glossary/#deputy","text":"Typically the backup IC. The deputy's job is to support the IC during the call, providing them with any help they need. More info .","title":"Deputy"},{"location":"training/glossary/#scribe","text":"The scribe's job is to keep a log of all activities performed during the call in a written chat log on Slack. More info .","title":"Scribe"},{"location":"training/glossary/#resolver","text":"A person on the incident call who is able to help resolve issues within a particular system. Also referred to as an SME (see below). More info .","title":"Resolver"},{"location":"training/glossary/#sme","text":"\"Subject Matter Expert\", someone who is an expert in a particular service or subject who can provide information to the IC, and perform resolution actions for a particular system. More info .","title":"SME"},{"location":"training/glossary/#command-staff","text":"The Command Staff consists of the Incident Commander, Deputy, and Scribe.","title":"Command Staff"},{"location":"training/glossary/#can-report","text":"CAN stands for \"Conditions\" \"Actions\" \"Needs\", if an IC asks you for a CAN report, you should provide the current state of your service (condition), what actions need to be taken to return it to a healthy state (actions), and what support you need in order to perform the actions (needs).","title":"CAN Report"},{"location":"training/glossary/#severity-sev","text":"How severe the incident is. The \"sev\" of an incident determines the type of response we give. The higher the severity, the higher the likelihood of making risky actions to resolve the situation. More info .","title":"Severity / Sev"},{"location":"training/glossary/#span-of-control","text":"Refers to the number of direct reports you have. For example, if the IC has 10 people as direct reports on a call, they have a large span of control. We aim to make the span of control as minimal as we can while still being productive.","title":"Span of Control"},{"location":"training/glossary/#grenade-thrower","text":"Someone who joins the call at a late time in the game, and provides information that completely derails the current thinking. They then leave almost immediately.","title":"Grenade Thrower"},{"location":"training/glossary/#executive-swoop","text":"When an executive comes on the call and drops some sort of bombshell. A version of grenade throwing.","title":"Executive Swoop"},{"location":"training/incident_commander/","text":"Credit: NASA So you want to be an Incident Commander (IC)? You've come to the right place! You don't need to be a senior team member to become an IC, anyone can do it providing you have the requisite knowledge (yes, even an intern)! Purpose # If you could boil down the purpose of an Incident Commander to one sentence, it would be, Keep the incident moving towards resolution. The Incident Commander is the decision maker during an major incident; Delegating tasks and listening to input from subject matter experts in order to bring the incident to resolution. They become the highest ranking individual on any major incident call, regardless of their day-to-day rank. Their decisions made as commander are final. Your job as an Incident Commander is to listen to the call and to watch the incident Slack room in order to provide clear coordination, recruiting others to gather context/details. You should not be performing any actions or remediations, checking graphs, or investigating logs. Those tasks should be delegated. An IC should also be considering next steps and backup plans at every opportunity, in an effort avoid getting stuck without any clear options to proceed, and to keep things moving towards resolution. Prerequisites # Before you can be an Incident Commander, it is expected that you meet the following criteria. Don't worry if you don't meet them all yet, you can still continue with training! Excellent verbal and written communication skills . Has high-level knowledge of how the different PagerDuty services interact with each other. Ability to size-up a situation, assess the effectiveness of various tactics/strategies, and make rapid decisions on appropriate courses of action. Has flexibility, and is able to listen to expert feedback , modifying plans on-the-fly as necessary. Has been involved in at least two major incident responses , either as a spectator, or as an active participant. Has gravitas, takes command , and is willing to kick people off a call to remove distractions, even if it's the CEO. Deep technical knowledge not required! Incident Commanders do not require deep technical knowledge of our systems . Your job as Incident Commander is to coordinate the response, not make technical changes. Don\u2019t think you can\u2019t be an Incident Commander just because you\u2019re not in the engineering department! Responsibilities # Read up on our Different Roles for Incidents to see what is expected from an Incident Commander, as well as what we expect from the other roles you'll be interacting with. Training Process # The process is fairly loose for now. Here's a list of things you can do to train though, Read the rest of this page, particularly the sections below. Participate in Failure Friday (FF). Shadow a FF to see how it's run. Be the scribe for multiple FF's. Be the incident commander for multiple FF's. Play a game of \" Keep Talking and Nobody Explodes \" with other people in the office. For a more realistic experience, play it with someone in a different office over Hangouts. Shadow a current incident commander for at least a full week shift. Get alerted when they do, join in on the same calls. Sit in on an active incident call, follow along with the chat, and follow along with what the Incident Commander is doing. Do not actively participate in the call, keep your questions until the end. Reverse shadow a current incident commander for at least a full week shift. You should be the one to respond to incidents, and you will take point on calls, however the current IC will be there to take over should you not know how to proceed. Graduation # What's the difference between an IC in training, and an IC? (This isn't the set up to a joke). Simple, an IC puts themselves on the schedules. Also, don't forget to announce yourself in the IC Slack channel, and get yourself added to our IC mailing list. Handling Incidents # Every incident is different (we're hopefully not repeating the same issue multiple times!), but there's a common process you can apply to each one. The language used in each step is discussed in more detail in the \"Procedures and Lingo\" section below. Size-Up # Sizing-up involves getting an idea of what's going on, and how much impact it's having. This is an information gathering step that will allow you to make good decisions later. Identify the symptoms. - Ask \"What's wrong?\" Identify what the symptoms are, ask your experts to provide this information. Gather as much information as you can, as quickly as you can (remember the incident is still happening while you're doing this). Identify scope of incident. - Ask \"Is this affecting multiple services?\" Identify how big the issue is, and whether it's escalating/flapping/static. Get the facts, the possibilities of what can happen, and the probability of those things happening. Stabilize # Next step is to stabilize the incident. We need to determine what we can do to fix it, and then execute those actions. Identify possible actions. - Ask \"What actions can we take? How risky are they?\" Identify any actions you can take to alleviate the issue. Ask your experts what they want to do. Identify the risks associated with each of those actions. Make a decision. - Say \"We're proceeding with...\" Decide which action to take based on the information you have available. Making the \"wrong\" decision is better than making no decision. If you have nothing but bad options, pick one and proceed. Gain consensus. - Ask \"Are there any strong objections?\" Gather support for the plan (See \"Polling During a Decision\" below). Listen for objections. Be prepared to adjust your plan if new information is presented. Assign task. - Say \"A, please perform B, I'll come back to you in X minutes. Understood?\" Delegate remediation actions to your SME's. Tasks should be assigned to an individual and be time-boxed. Get acknowledgement that the task was understood and is being executed. Update # While remediation steps are being carried out, it's important to provide status updates, not just to responders, but other stakeholders within the organization. Provide regular updates. - Say \"Here's a status update: ...\" Maintain a cadence, and provide regular updates to everyone on the call. What's happening, what are we doing about it, etc. Keep updates short and factual. Verify # Once remediation actions have been performed, we need to verify that they have been successful or not, and proceed with a backup plan if not. Follow-up on task completion. - Ask \"Have you finished?\" Ask for the status of task completion from the tasks you assigned out. If responders need more time, give them more time. If problems persist, begin again from the size-up step. Deputy # The deputy for an incident is generally the backup Incident Commander. However, as an Incident Commander, you may appoint one or more Deputies. Note that Deputy Incident Commanders must be as qualified as the Incident Commander, and that if a Deputy is assigned, he or she must be fully qualified to assume the Incident Commander\u2019s position if required. Communication Responsibilities # Sharing information during an incident is a critical process. As an Incident Commander (or Deputy), you should be prepared to brief others as necessary. You will also be required to communicate your intentions and decisions clearly so that there is no ambiguity in your commands. When given information from a responder, you should clearly acknowledge that you have received and understood their message, so that the responder can be confident in moving on to other tasks. After an incident, you should communicate with other training Incident Commanders on any debrief actions you feel are necessary. Clear is better than concise. Remember that clear communication is better than concise communication. It can be tempting to try and abbreviate or rush speech in order to speed up the response. This can lead to confusion and misunderstandings which will ultimate increase the response time. Always favour clear communication, even if takes a little bit longer. Incident Call Procedures and Lingo # The Steps for Incident Commander provide a detailed description of what you should be doing during an incident. Additionally, aside from following the usual incident call etiquette , there a few extra etiquette guidelines you should follow as IC: Always announce when you join the call if you are the on-call IC. Don't let discussions get out of hand. Keep conversations short. Note objections from others, but your call is final. If anyone is being actively disruptive to your call, kick them off. Announce the end of the call. Here are some examples of phrases and patterns you should use during incident calls. Start of Call Announcement # At the start of any major incident call, the incident commander should announce the following, This is [NAME], I am the Incident Commander for this call. This establishes to everyone on the call what your name is, and that you are now the commander. Identify yourself by name and state that you are the \"Incident Commander\" and not \"IC\", as newcomers may not be familiar with the terminology yet. The word \"commander\" makes it very clear that you're in charge. Start of Incident, IC Not Present # If you are trained to be an IC and have joined a call, even if you aren't the IC on-call, you should do the following, Is there an IC on the call? (pause) Hearing no response, this is [NAME], and I am now the Incident Commander for this call. If the on-call IC joins later, you may hand over to them at your discretion (see below for the hand-off procedure) Checking if SME's are Present # During a call, you will want to know who is available from the various teams in order to resolve the incident. Etiquette dictates that people should announce themselves, but sometimes you may be joining late to the call. If you need a representative from a team, just ask on the call. Your deputy can page one if no one answers. Do we have a representative from [X] on the call? (pause) Deputy, can you go ahead and page the [X] on-call please. Assigning Tasks # When you need to give out an assignment or task, you should follow these three steps, Assign the task to a specific person directly. Time-box the task with a specific number of minutes. Confirm that the responder has acknowledged and understood the instructions. Can someone... Never say \"Can someone...\" as this leads to the bystander effect . Tasks should always be assigned directly to an individual, and never just thrown out with the hope that someone will pick it up. IC: Bob, please investigate the high latency on web app boxes. I'll come back to you for an answer in 3 minutes. Bob: Understood Keep track of how many minutes you assigned, and check in with that person after that time. You can get help from your deputy to help track the timings. Gaining Consensus (Polling During a Decision) # If a decision needs to be made, it comes down to the IC. Once the IC makes a decision, it is final. But it's important that no one can come later and object to the plan, saying things like \"I knew that would happen\". An IC will use very specific language to be sure that doesn't happen, and to gain implicit consensus of everyone on the response. The proposal is to [EXPLAIN PROPOSAL] Are there any strong objections to this plan? (pause) Hearing no objects, we are proceeding with this proposal. If you were to ask \"Does everyone agree?\", you'd get people speaking over each other, you'd have quiet people not speaking up, etc. Asking for any STRONG objections gives people the chance to object, but only if they feel strongly on the matter. It also means that the information you care about the most (objections to proceeding) are heard loud and clearly. Status Updates # It's important to maintain a cadence during a major incident call. Whenever there is a lull in the proceedings, usually because you're waiting for someone to get back to you, you can fill the gap by explaining the current situation and the actions that are outstanding. This makes sure everyone is on the same page. While we wait for [X], here's an update of our current situation. We are currently in a SEV-1 situation, we believe to be caused by [X]. There's an open question to [Y] who will be getting back to us in 2 minutes. In the meantime, we have Tweeted out that we are experiencing issues. Our next Tweet will be in 10 minutes if the incident is still ongoing at that time. Are there any additional actions or proposals from anyone else at this time? Reducing Scope # Once you've identified the cause of an incident, you can take some time to reduce the scope of your call. For example, if you've identified that a bad deploy is the cause, there's no need to keep your network engineering responder on the call. Responders will usually appreciate not having to stick around for something that doesn't involve them, especially when it's 3am. Generally, it's best to list out the people you want to remain on the call (rather than listing those that can leave), as this not only re-affirms who is required, but makes sure you won't forget about anyone who can leave. Now that we've identified the primary cause and are on the way to recovery, I need the deputy, scribe, support, and site-reliability experts to stay on the call. Everyone else, thanks very much for your response, feel free to drop-off at your discretion. There's no need to forcibly remove anyone from the call, leave the choice open. Sometimes responders prefer to remain to see how the incident eventually resolves, since they're already awake anyway. Spinning Off Sub-Teams # When handling complex incidents , it will sometimes be necessary to spin off a sub-team (or multiple sub-teams) to investigate specific issues in more detail before reporting back. This is to ensure that you can maintain an effective span of control. To do this, you should assign a team leader, give them a specific task (time-boxed in the usual way), and re-affirm that they are your primary contact and that all communication from their team should come via the leader. Use our pre-defined team names of Alpha, Bravo, and Charlie to avoid confusion when creating the teams. IC: Anne, I'd like you to lead a sub-team to investigate the ongoing latency in the web-tier. Please gather your desired team and investigate, get back to me in 20 minutes. All communication from your team should be filtered through you. Use the Alpha team rooms and phone bridge. Anne: Understood, I'll get back with an update in 20 minutes. You do not need to prescribe who their team consists of. Either they will have a pre-existing team structure they can utilize, or they should take the initiative to assemble a team on their own. You should pick your team leaders accordingly. Transfer of Command # Transfer of command, involves (as the name suggests) transferring command to another Incident Commander. There are multiple reasons why a transfer of command might take place, Commander has become fatigued and is unable to continue. Incident complexity changes. Change of command is necessary for effectiveness or efficiency. Personal emergencies arise (e.g., Incident Commander has a family emergency). Never feel like you are not doing your job properly by handing over. Handovers are encouraged. In order to handover, out of band from the main call (via Slack for example), notify the other IC that you wish to transfer command. Update them with anything you feel appropriate. Then announce on the call, Everyone on the call, be advised, at this time I am handing over command to [X]. The new IC should then announce on the call as if they were joining a new call (see above), so that everyone is aware of the new commander. Note that the arrival of a more qualified person does NOT necessarily mean a change in incident command. End of Call Sign-Off # At the end of an incident, you should announce to everyone on the call that you are ending the call at this time, and provide information on where followup discussion can take place. It's also customary to thank everyone. Ok everyone, we're ending the call at this time. Please continue any followup discussion on Slack. Thanks everyone. Handling Problems # Things don't always go smoothly on incident response calls, so as an Incident Commander you need to be prepared for instances where the conversation gets derailed, either intentionally or unintentionally. Here are some procedures and lingo you can follow when things get disruptive, in order to get things back on track. Maintaining Order # Often times on a call people will be talking over one another, or an argument on the correct way to proceed may break out. As Incident Commander it's important that order is maintained on a call. The Incident Commander has the power to remove someone from the call if necessary (even if it's the CEO). But often times you just need to remind people to speak one at a time. Sometimes the discussion can be healthy even if it starts as an argument, but you shouldn't let it go on for too long. (noise) Ok everyone, can we all speak one at a time please. So far I'm hearing two options to proceed: 1) [X], 2) [Y]. Are there any other proposals someone would like to make at this time? ...etc Getting Straight Answers # You may ask a question as IC and receive an answer that doesn't actually answer your question. This is generally when you ask for a yes/no answer but get a more detailed explanation. This can often times be because the person doesn't understand the call etiquette. But if it continues, you need to take action in order to proceed. IC: Is this going to disable the service for everyone? SME: Well... for some people it.... IC: Stop. I need a yes/no answer. Is this going to disable the service for everyone? SME: Well... it might not do... IC: Stop. I'm going to ask again, and the only two words I want to hear from you are \"yes\" or \"no. Is this going to disable the service for everyone? SME: Well.. like I was saying.. IC: Stop. Leave the call. Backup IC can you please page the backup on-call for [service] so that we can get an answer. Executive Swoop - Overriding the Incident Commander # Executive: Ignore the incident commander, do what I say! This is an extreme example, but illustrates the concept of \"Executive Swoop\", whereby someone who would be senior to you during peacetime comes on the call and starts overriding your decisions as IC. This is unacceptable behaviour during wartime, as the IC is in command. This is rare, but can cripple the response process if it happens. There is a simple question you can ask as an Incident Commander to get things back on track, \"Do you wish to take command?\", Executive: No, I don't want us doing that. Everyone stop. We need to rollback instead. IC: Hold please. [EXECUTIVE], do you wish to take over command? Executive: Yes/No (If yes) IC: Understood. Everyone on the call, be advised, at this time I am handling over command to [EXECUTIVE]. They are now the incident commander for this call. (If no) IC: In that case, please cause no further interruptions or I will remove you from the call. This makes it clear to the executive that they have the option of being in charge and making decisions, but in order to do so they must continue as an Incident Commander. If they refuse, then remind them that you are in charge and disruptive interruptions will not be tolerated. If they continue, remove them from the call. Executive Swoop - Anti-Motiviation # Executive: Let's try and resolve this in 10 minutes please! It's rare for an executive to maliciously derail an incident response call, usually it is done with the best of intentions. However, these good intentions can still derail your response process and demotivate responders. As an Incident Commander you will need to recognize and respond to these situations. In the case above, it seems motivational, however it assumes responders aren't already working as hard as possible to solve the problem, and adds no value to the response process. You can respond to this by reminding the commenter that these things should be kept until after the incident is over. IC: We're in the middle of an incident, please keep your comments until the end. Executive Swoop - Wants Information # Executive: Can I get a spreadsheet of all affected customers? The most common case of executive swoop is a request for more information. Unfortunately, when in the middle of an incident, you typically cannot spare the resources to gather such information. As an incident commander you should remind the executive of this, and that the incident takes priority. We can either get you that list, or fix the incident. Not both. The incident takes priority. Note that this isn't phrased as a question, you've already made the decision as Incident Commander, you're just informing the executive of that decision. Executive Swoop - Questioning Severity # Executive: Is this really a SEV-1? Our severity levels determine the scale of response we give to an incident. Conversations on what severity an incident is can very quickly consume the entire call and doesn't change the fact that there is an incident on-going. We do not discuss incident severity during an incident call, as we treat an incident as the highest severity we think it could be. We can downgrade the severity during the post-mortem, however we cannot waste time litigating severities on an incident call. So simply remind folks of this in order to get things back on track, IC: We do not discuss incident severity during the call. We're treating this as a SEV-1. The Belligerent Responder # Sometimes you will have a responder who does not follow instructions and/or is being actively disruptive to your response call. Perhaps this is being done intentionally, or it could even be unintentional (an un-muted microphone while in a loud environment, etc). In either case, you need to resolve the situation and get back to the incident at hand. State the fact that the individual is being disruptive, provide them a way to save face, but also state what will happen if they don't stop. No second chances, if they don't follow through, remove them from the call. You're being disruptive. Please stop, or I will have to remove you from the call. Examples From Pop Culture # PagerDuty employees have access to all previous incident calls, and can listen to them at their discretion. We can't release these calls, so for everyone else, here are some short examples from popular culture to show the techniques at work. Here's a clip from the movie Apollo 13, where Gene Kranz (Flight Director / Incident Commander) shows some great examples of Incident Command. Here are some things to note: Walks into the room, and immediately obvious that he's the IC. Calms the noise, and makes sure everyone is paying attention. Provides a status update so people are aware of the situation. Projector breaks, doesn't get sidetracked on fixing it, just moves on to something else. Provides a proposal for how to proceed and elicits feedback. Listens to the feedback calmly. When counter-proposal is raised, states that he agrees and why. Allows a discussion to happen, listens to all points. When discussion gets out of hand, re-asserts command of the situation. Explains his decision, and why. Explains his full plan and decision, so everyone is on the same page. Another clip from Apollo 13. Things to note: Summarizes the situation, and states the facts. Listens to the feedback from various people. When a trusted SME provides information counter to what everyone else is saying, asks for additional clarification (\"What do you mean, everything?\") Wise cracking remarks are not acknowledged by the IC (\"You can't run a vacuum cleaner on 12 amps!\") \"That's the deal?\".. \"That's the deal\". Once decision is made, moves on to the next discussion. Delegates tasks.","title":"Incident Commander"},{"location":"training/incident_commander/#purpose","text":"If you could boil down the purpose of an Incident Commander to one sentence, it would be, Keep the incident moving towards resolution. The Incident Commander is the decision maker during an major incident; Delegating tasks and listening to input from subject matter experts in order to bring the incident to resolution. They become the highest ranking individual on any major incident call, regardless of their day-to-day rank. Their decisions made as commander are final. Your job as an Incident Commander is to listen to the call and to watch the incident Slack room in order to provide clear coordination, recruiting others to gather context/details. You should not be performing any actions or remediations, checking graphs, or investigating logs. Those tasks should be delegated. An IC should also be considering next steps and backup plans at every opportunity, in an effort avoid getting stuck without any clear options to proceed, and to keep things moving towards resolution.","title":"Purpose"},{"location":"training/incident_commander/#prerequisites","text":"Before you can be an Incident Commander, it is expected that you meet the following criteria. Don't worry if you don't meet them all yet, you can still continue with training! Excellent verbal and written communication skills . Has high-level knowledge of how the different PagerDuty services interact with each other. Ability to size-up a situation, assess the effectiveness of various tactics/strategies, and make rapid decisions on appropriate courses of action. Has flexibility, and is able to listen to expert feedback , modifying plans on-the-fly as necessary. Has been involved in at least two major incident responses , either as a spectator, or as an active participant. Has gravitas, takes command , and is willing to kick people off a call to remove distractions, even if it's the CEO. Deep technical knowledge not required! Incident Commanders do not require deep technical knowledge of our systems . Your job as Incident Commander is to coordinate the response, not make technical changes. Don\u2019t think you can\u2019t be an Incident Commander just because you\u2019re not in the engineering department!","title":"Prerequisites"},{"location":"training/incident_commander/#responsibilities","text":"Read up on our Different Roles for Incidents to see what is expected from an Incident Commander, as well as what we expect from the other roles you'll be interacting with.","title":"Responsibilities"},{"location":"training/incident_commander/#training-process","text":"The process is fairly loose for now. Here's a list of things you can do to train though, Read the rest of this page, particularly the sections below. Participate in Failure Friday (FF). Shadow a FF to see how it's run. Be the scribe for multiple FF's. Be the incident commander for multiple FF's. Play a game of \" Keep Talking and Nobody Explodes \" with other people in the office. For a more realistic experience, play it with someone in a different office over Hangouts. Shadow a current incident commander for at least a full week shift. Get alerted when they do, join in on the same calls. Sit in on an active incident call, follow along with the chat, and follow along with what the Incident Commander is doing. Do not actively participate in the call, keep your questions until the end. Reverse shadow a current incident commander for at least a full week shift. You should be the one to respond to incidents, and you will take point on calls, however the current IC will be there to take over should you not know how to proceed.","title":"Training Process"},{"location":"training/incident_commander/#graduation","text":"What's the difference between an IC in training, and an IC? (This isn't the set up to a joke). Simple, an IC puts themselves on the schedules. Also, don't forget to announce yourself in the IC Slack channel, and get yourself added to our IC mailing list.","title":"Graduation"},{"location":"training/incident_commander/#handling-incidents","text":"Every incident is different (we're hopefully not repeating the same issue multiple times!), but there's a common process you can apply to each one. The language used in each step is discussed in more detail in the \"Procedures and Lingo\" section below.","title":"Handling Incidents"},{"location":"training/incident_commander/#size-up","text":"Sizing-up involves getting an idea of what's going on, and how much impact it's having. This is an information gathering step that will allow you to make good decisions later. Identify the symptoms. - Ask \"What's wrong?\" Identify what the symptoms are, ask your experts to provide this information. Gather as much information as you can, as quickly as you can (remember the incident is still happening while you're doing this). Identify scope of incident. - Ask \"Is this affecting multiple services?\" Identify how big the issue is, and whether it's escalating/flapping/static. Get the facts, the possibilities of what can happen, and the probability of those things happening.","title":"Size-Up"},{"location":"training/incident_commander/#stabilize","text":"Next step is to stabilize the incident. We need to determine what we can do to fix it, and then execute those actions. Identify possible actions. - Ask \"What actions can we take? How risky are they?\" Identify any actions you can take to alleviate the issue. Ask your experts what they want to do. Identify the risks associated with each of those actions. Make a decision. - Say \"We're proceeding with...\" Decide which action to take based on the information you have available. Making the \"wrong\" decision is better than making no decision. If you have nothing but bad options, pick one and proceed. Gain consensus. - Ask \"Are there any strong objections?\" Gather support for the plan (See \"Polling During a Decision\" below). Listen for objections. Be prepared to adjust your plan if new information is presented. Assign task. - Say \"A, please perform B, I'll come back to you in X minutes. Understood?\" Delegate remediation actions to your SME's. Tasks should be assigned to an individual and be time-boxed. Get acknowledgement that the task was understood and is being executed.","title":"Stabilize"},{"location":"training/incident_commander/#update","text":"While remediation steps are being carried out, it's important to provide status updates, not just to responders, but other stakeholders within the organization. Provide regular updates. - Say \"Here's a status update: ...\" Maintain a cadence, and provide regular updates to everyone on the call. What's happening, what are we doing about it, etc. Keep updates short and factual.","title":"Update"},{"location":"training/incident_commander/#verify","text":"Once remediation actions have been performed, we need to verify that they have been successful or not, and proceed with a backup plan if not. Follow-up on task completion. - Ask \"Have you finished?\" Ask for the status of task completion from the tasks you assigned out. If responders need more time, give them more time. If problems persist, begin again from the size-up step.","title":"Verify"},{"location":"training/incident_commander/#deputy","text":"The deputy for an incident is generally the backup Incident Commander. However, as an Incident Commander, you may appoint one or more Deputies. Note that Deputy Incident Commanders must be as qualified as the Incident Commander, and that if a Deputy is assigned, he or she must be fully qualified to assume the Incident Commander\u2019s position if required.","title":"Deputy"},{"location":"training/incident_commander/#communication-responsibilities","text":"Sharing information during an incident is a critical process. As an Incident Commander (or Deputy), you should be prepared to brief others as necessary. You will also be required to communicate your intentions and decisions clearly so that there is no ambiguity in your commands. When given information from a responder, you should clearly acknowledge that you have received and understood their message, so that the responder can be confident in moving on to other tasks. After an incident, you should communicate with other training Incident Commanders on any debrief actions you feel are necessary. Clear is better than concise. Remember that clear communication is better than concise communication. It can be tempting to try and abbreviate or rush speech in order to speed up the response. This can lead to confusion and misunderstandings which will ultimate increase the response time. Always favour clear communication, even if takes a little bit longer.","title":"Communication Responsibilities"},{"location":"training/incident_commander/#incident-call-procedures-and-lingo","text":"The Steps for Incident Commander provide a detailed description of what you should be doing during an incident. Additionally, aside from following the usual incident call etiquette , there a few extra etiquette guidelines you should follow as IC: Always announce when you join the call if you are the on-call IC. Don't let discussions get out of hand. Keep conversations short. Note objections from others, but your call is final. If anyone is being actively disruptive to your call, kick them off. Announce the end of the call. Here are some examples of phrases and patterns you should use during incident calls.","title":"Incident Call Procedures and Lingo"},{"location":"training/incident_commander/#start-of-call-announcement","text":"At the start of any major incident call, the incident commander should announce the following, This is [NAME], I am the Incident Commander for this call. This establishes to everyone on the call what your name is, and that you are now the commander. Identify yourself by name and state that you are the \"Incident Commander\" and not \"IC\", as newcomers may not be familiar with the terminology yet. The word \"commander\" makes it very clear that you're in charge.","title":"Start of Call Announcement"},{"location":"training/incident_commander/#start-of-incident-ic-not-present","text":"If you are trained to be an IC and have joined a call, even if you aren't the IC on-call, you should do the following, Is there an IC on the call? (pause) Hearing no response, this is [NAME], and I am now the Incident Commander for this call. If the on-call IC joins later, you may hand over to them at your discretion (see below for the hand-off procedure)","title":"Start of Incident, IC Not Present"},{"location":"training/incident_commander/#checking-if-smes-are-present","text":"During a call, you will want to know who is available from the various teams in order to resolve the incident. Etiquette dictates that people should announce themselves, but sometimes you may be joining late to the call. If you need a representative from a team, just ask on the call. Your deputy can page one if no one answers. Do we have a representative from [X] on the call? (pause) Deputy, can you go ahead and page the [X] on-call please.","title":"Checking if SME's are Present"},{"location":"training/incident_commander/#assigning-tasks","text":"When you need to give out an assignment or task, you should follow these three steps, Assign the task to a specific person directly. Time-box the task with a specific number of minutes. Confirm that the responder has acknowledged and understood the instructions. Can someone... Never say \"Can someone...\" as this leads to the bystander effect . Tasks should always be assigned directly to an individual, and never just thrown out with the hope that someone will pick it up. IC: Bob, please investigate the high latency on web app boxes. I'll come back to you for an answer in 3 minutes. Bob: Understood Keep track of how many minutes you assigned, and check in with that person after that time. You can get help from your deputy to help track the timings.","title":"Assigning Tasks"},{"location":"training/incident_commander/#gaining-consensus-polling-during-a-decision","text":"If a decision needs to be made, it comes down to the IC. Once the IC makes a decision, it is final. But it's important that no one can come later and object to the plan, saying things like \"I knew that would happen\". An IC will use very specific language to be sure that doesn't happen, and to gain implicit consensus of everyone on the response. The proposal is to [EXPLAIN PROPOSAL] Are there any strong objections to this plan? (pause) Hearing no objects, we are proceeding with this proposal. If you were to ask \"Does everyone agree?\", you'd get people speaking over each other, you'd have quiet people not speaking up, etc. Asking for any STRONG objections gives people the chance to object, but only if they feel strongly on the matter. It also means that the information you care about the most (objections to proceeding) are heard loud and clearly.","title":"Gaining Consensus (Polling During a Decision)"},{"location":"training/incident_commander/#status-updates","text":"It's important to maintain a cadence during a major incident call. Whenever there is a lull in the proceedings, usually because you're waiting for someone to get back to you, you can fill the gap by explaining the current situation and the actions that are outstanding. This makes sure everyone is on the same page. While we wait for [X], here's an update of our current situation. We are currently in a SEV-1 situation, we believe to be caused by [X]. There's an open question to [Y] who will be getting back to us in 2 minutes. In the meantime, we have Tweeted out that we are experiencing issues. Our next Tweet will be in 10 minutes if the incident is still ongoing at that time. Are there any additional actions or proposals from anyone else at this time?","title":"Status Updates"},{"location":"training/incident_commander/#reducing-scope","text":"Once you've identified the cause of an incident, you can take some time to reduce the scope of your call. For example, if you've identified that a bad deploy is the cause, there's no need to keep your network engineering responder on the call. Responders will usually appreciate not having to stick around for something that doesn't involve them, especially when it's 3am. Generally, it's best to list out the people you want to remain on the call (rather than listing those that can leave), as this not only re-affirms who is required, but makes sure you won't forget about anyone who can leave. Now that we've identified the primary cause and are on the way to recovery, I need the deputy, scribe, support, and site-reliability experts to stay on the call. Everyone else, thanks very much for your response, feel free to drop-off at your discretion. There's no need to forcibly remove anyone from the call, leave the choice open. Sometimes responders prefer to remain to see how the incident eventually resolves, since they're already awake anyway.","title":"Reducing Scope"},{"location":"training/incident_commander/#spinning-off-sub-teams","text":"When handling complex incidents , it will sometimes be necessary to spin off a sub-team (or multiple sub-teams) to investigate specific issues in more detail before reporting back. This is to ensure that you can maintain an effective span of control. To do this, you should assign a team leader, give them a specific task (time-boxed in the usual way), and re-affirm that they are your primary contact and that all communication from their team should come via the leader. Use our pre-defined team names of Alpha, Bravo, and Charlie to avoid confusion when creating the teams. IC: Anne, I'd like you to lead a sub-team to investigate the ongoing latency in the web-tier. Please gather your desired team and investigate, get back to me in 20 minutes. All communication from your team should be filtered through you. Use the Alpha team rooms and phone bridge. Anne: Understood, I'll get back with an update in 20 minutes. You do not need to prescribe who their team consists of. Either they will have a pre-existing team structure they can utilize, or they should take the initiative to assemble a team on their own. You should pick your team leaders accordingly.","title":"Spinning Off Sub-Teams"},{"location":"training/incident_commander/#transfer-of-command","text":"Transfer of command, involves (as the name suggests) transferring command to another Incident Commander. There are multiple reasons why a transfer of command might take place, Commander has become fatigued and is unable to continue. Incident complexity changes. Change of command is necessary for effectiveness or efficiency. Personal emergencies arise (e.g., Incident Commander has a family emergency). Never feel like you are not doing your job properly by handing over. Handovers are encouraged. In order to handover, out of band from the main call (via Slack for example), notify the other IC that you wish to transfer command. Update them with anything you feel appropriate. Then announce on the call, Everyone on the call, be advised, at this time I am handing over command to [X]. The new IC should then announce on the call as if they were joining a new call (see above), so that everyone is aware of the new commander. Note that the arrival of a more qualified person does NOT necessarily mean a change in incident command.","title":"Transfer of Command"},{"location":"training/incident_commander/#end-of-call-sign-off","text":"At the end of an incident, you should announce to everyone on the call that you are ending the call at this time, and provide information on where followup discussion can take place. It's also customary to thank everyone. Ok everyone, we're ending the call at this time. Please continue any followup discussion on Slack. Thanks everyone.","title":"End of Call Sign-Off"},{"location":"training/incident_commander/#handling-problems","text":"Things don't always go smoothly on incident response calls, so as an Incident Commander you need to be prepared for instances where the conversation gets derailed, either intentionally or unintentionally. Here are some procedures and lingo you can follow when things get disruptive, in order to get things back on track.","title":"Handling Problems"},{"location":"training/incident_commander/#maintaining-order","text":"Often times on a call people will be talking over one another, or an argument on the correct way to proceed may break out. As Incident Commander it's important that order is maintained on a call. The Incident Commander has the power to remove someone from the call if necessary (even if it's the CEO). But often times you just need to remind people to speak one at a time. Sometimes the discussion can be healthy even if it starts as an argument, but you shouldn't let it go on for too long. (noise) Ok everyone, can we all speak one at a time please. So far I'm hearing two options to proceed: 1) [X], 2) [Y]. Are there any other proposals someone would like to make at this time? ...etc","title":"Maintaining Order"},{"location":"training/incident_commander/#getting-straight-answers","text":"You may ask a question as IC and receive an answer that doesn't actually answer your question. This is generally when you ask for a yes/no answer but get a more detailed explanation. This can often times be because the person doesn't understand the call etiquette. But if it continues, you need to take action in order to proceed. IC: Is this going to disable the service for everyone? SME: Well... for some people it.... IC: Stop. I need a yes/no answer. Is this going to disable the service for everyone? SME: Well... it might not do... IC: Stop. I'm going to ask again, and the only two words I want to hear from you are \"yes\" or \"no. Is this going to disable the service for everyone? SME: Well.. like I was saying.. IC: Stop. Leave the call. Backup IC can you please page the backup on-call for [service] so that we can get an answer.","title":"Getting Straight Answers"},{"location":"training/incident_commander/#executive-swoop-overriding-the-incident-commander","text":"Executive: Ignore the incident commander, do what I say! This is an extreme example, but illustrates the concept of \"Executive Swoop\", whereby someone who would be senior to you during peacetime comes on the call and starts overriding your decisions as IC. This is unacceptable behaviour during wartime, as the IC is in command. This is rare, but can cripple the response process if it happens. There is a simple question you can ask as an Incident Commander to get things back on track, \"Do you wish to take command?\", Executive: No, I don't want us doing that. Everyone stop. We need to rollback instead. IC: Hold please. [EXECUTIVE], do you wish to take over command? Executive: Yes/No (If yes) IC: Understood. Everyone on the call, be advised, at this time I am handling over command to [EXECUTIVE]. They are now the incident commander for this call. (If no) IC: In that case, please cause no further interruptions or I will remove you from the call. This makes it clear to the executive that they have the option of being in charge and making decisions, but in order to do so they must continue as an Incident Commander. If they refuse, then remind them that you are in charge and disruptive interruptions will not be tolerated. If they continue, remove them from the call.","title":"Executive Swoop - Overriding the Incident Commander"},{"location":"training/incident_commander/#executive-swoop-anti-motiviation","text":"Executive: Let's try and resolve this in 10 minutes please! It's rare for an executive to maliciously derail an incident response call, usually it is done with the best of intentions. However, these good intentions can still derail your response process and demotivate responders. As an Incident Commander you will need to recognize and respond to these situations. In the case above, it seems motivational, however it assumes responders aren't already working as hard as possible to solve the problem, and adds no value to the response process. You can respond to this by reminding the commenter that these things should be kept until after the incident is over. IC: We're in the middle of an incident, please keep your comments until the end.","title":"Executive Swoop - Anti-Motiviation"},{"location":"training/incident_commander/#executive-swoop-wants-information","text":"Executive: Can I get a spreadsheet of all affected customers? The most common case of executive swoop is a request for more information. Unfortunately, when in the middle of an incident, you typically cannot spare the resources to gather such information. As an incident commander you should remind the executive of this, and that the incident takes priority. We can either get you that list, or fix the incident. Not both. The incident takes priority. Note that this isn't phrased as a question, you've already made the decision as Incident Commander, you're just informing the executive of that decision.","title":"Executive Swoop - Wants Information"},{"location":"training/incident_commander/#executive-swoop-questioning-severity","text":"Executive: Is this really a SEV-1? Our severity levels determine the scale of response we give to an incident. Conversations on what severity an incident is can very quickly consume the entire call and doesn't change the fact that there is an incident on-going. We do not discuss incident severity during an incident call, as we treat an incident as the highest severity we think it could be. We can downgrade the severity during the post-mortem, however we cannot waste time litigating severities on an incident call. So simply remind folks of this in order to get things back on track, IC: We do not discuss incident severity during the call. We're treating this as a SEV-1.","title":"Executive Swoop - Questioning Severity"},{"location":"training/incident_commander/#the-belligerent-responder","text":"Sometimes you will have a responder who does not follow instructions and/or is being actively disruptive to your response call. Perhaps this is being done intentionally, or it could even be unintentional (an un-muted microphone while in a loud environment, etc). In either case, you need to resolve the situation and get back to the incident at hand. State the fact that the individual is being disruptive, provide them a way to save face, but also state what will happen if they don't stop. No second chances, if they don't follow through, remove them from the call. You're being disruptive. Please stop, or I will have to remove you from the call.","title":"The Belligerent Responder"},{"location":"training/incident_commander/#examples-from-pop-culture","text":"PagerDuty employees have access to all previous incident calls, and can listen to them at their discretion. We can't release these calls, so for everyone else, here are some short examples from popular culture to show the techniques at work. Here's a clip from the movie Apollo 13, where Gene Kranz (Flight Director / Incident Commander) shows some great examples of Incident Command. Here are some things to note: Walks into the room, and immediately obvious that he's the IC. Calms the noise, and makes sure everyone is paying attention. Provides a status update so people are aware of the situation. Projector breaks, doesn't get sidetracked on fixing it, just moves on to something else. Provides a proposal for how to proceed and elicits feedback. Listens to the feedback calmly. When counter-proposal is raised, states that he agrees and why. Allows a discussion to happen, listens to all points. When discussion gets out of hand, re-asserts command of the situation. Explains his decision, and why. Explains his full plan and decision, so everyone is on the same page. Another clip from Apollo 13. Things to note: Summarizes the situation, and states the facts. Listens to the feedback from various people. When a trusted SME provides information counter to what everyone else is saying, asks for additional clarification (\"What do you mean, everything?\") Wise cracking remarks are not acknowledged by the IC (\"You can't run a vacuum cleaner on 12 amps!\") \"That's the deal?\".. \"That's the deal\". Once decision is made, moves on to the next discussion. Delegates tasks.","title":"Examples From Pop Culture"},{"location":"training/internal_liaison/","text":"So you want to be an internal liaison? You've come to the right place! Purpose # The purpose of the Internal Liaison is to be the primary individual in charge of notifying internal stakeholders of the current conditions, and informing the Incident Commander of any relevant feedback from stakeholders as the incident progresses. They are also responsible for mobilizing additional responders during an incident if requested by the incident commander. It's important for the rest of the command staff to be able to focus on the problem at hand, rather than worrying about crafting messages to internal teams, or having to find a user's contact information in order to page them. Your job as Internal Liaison is to listen to the call, watch the incident Slack room, and follow instructions from the IC. The Incident Commander will instruct you to notify stakeholders of the incident and keep them updated at various points throughout the call. They will also instruct you to page additional responders should it be necessary. Prerequisites # Before you can be an Internal Liaison, it is expected that you meet the following criteria. Don't worry if you don't meet them all yet, you can still continue with training! Excellent verbal and written communication skills . Responsibilities # Read up on our Different Roles for Incidents to see what is expected from an Internal Liaison, as well as what we expect from the other roles you'll be interacting with. Training Process # There is no formal training process for this role, reading this page should be sufficient for most tasks. Incident Call Procedures and Lingo # The Steps for Internal Liaison provide a detailed description of what you should be doing during an incident. Here are some examples of phrases and patterns you should use during incident calls. Keep Track of Responders # As you listen to the call, you should keep track of the responders to the call as you hear them speak. Make a note on a piece of paper, or use the !ic responders to see who they are. The IC may ask you who is on-call for a particular system, and you should know the answer, and be able to page them. Do we have a representative from [X] on the call? (pause) Hearing no response, Internal Liaison, please page the [X] on-call. You can page them however you see fit, phone call, etc. Mobilizing Additional Responders # As you listen to the call, the IC may ask you to reach out to another internal team and include them in the response. Teams such as Legal, Marketing, Finance, etc may be called upon. You should be ready to page such teams as necessary. IC: Internal Liaison, please page the Marketing team and ask them to join the call. IL: Understood, paging Marketing team. Provide Executive Status Updates # Provide regular status updates on Slack (roughly every 30mins), giving an executive summary of the current status during SEV-1 incidents. Keep it short and to the point, and use @here . Mention the current state, the actions in progress, customer impact, and expected time remaining. It's OK to miss out some of those if the information isn't known. @here: We are in SEV-1 due to X. Current actions in progress are to do Y. Expecting 3 mins to complete that action. Once action is complete, system should recover on its own within 5 minutes.","title":"Internal Liaison"},{"location":"training/internal_liaison/#purpose","text":"The purpose of the Internal Liaison is to be the primary individual in charge of notifying internal stakeholders of the current conditions, and informing the Incident Commander of any relevant feedback from stakeholders as the incident progresses. They are also responsible for mobilizing additional responders during an incident if requested by the incident commander. It's important for the rest of the command staff to be able to focus on the problem at hand, rather than worrying about crafting messages to internal teams, or having to find a user's contact information in order to page them. Your job as Internal Liaison is to listen to the call, watch the incident Slack room, and follow instructions from the IC. The Incident Commander will instruct you to notify stakeholders of the incident and keep them updated at various points throughout the call. They will also instruct you to page additional responders should it be necessary.","title":"Purpose"},{"location":"training/internal_liaison/#prerequisites","text":"Before you can be an Internal Liaison, it is expected that you meet the following criteria. Don't worry if you don't meet them all yet, you can still continue with training! Excellent verbal and written communication skills .","title":"Prerequisites"},{"location":"training/internal_liaison/#responsibilities","text":"Read up on our Different Roles for Incidents to see what is expected from an Internal Liaison, as well as what we expect from the other roles you'll be interacting with.","title":"Responsibilities"},{"location":"training/internal_liaison/#training-process","text":"There is no formal training process for this role, reading this page should be sufficient for most tasks.","title":"Training Process"},{"location":"training/internal_liaison/#incident-call-procedures-and-lingo","text":"The Steps for Internal Liaison provide a detailed description of what you should be doing during an incident. Here are some examples of phrases and patterns you should use during incident calls.","title":"Incident Call Procedures and Lingo"},{"location":"training/internal_liaison/#keep-track-of-responders","text":"As you listen to the call, you should keep track of the responders to the call as you hear them speak. Make a note on a piece of paper, or use the !ic responders to see who they are. The IC may ask you who is on-call for a particular system, and you should know the answer, and be able to page them. Do we have a representative from [X] on the call? (pause) Hearing no response, Internal Liaison, please page the [X] on-call. You can page them however you see fit, phone call, etc.","title":"Keep Track of Responders"},{"location":"training/internal_liaison/#mobilizing-additional-responders","text":"As you listen to the call, the IC may ask you to reach out to another internal team and include them in the response. Teams such as Legal, Marketing, Finance, etc may be called upon. You should be ready to page such teams as necessary. IC: Internal Liaison, please page the Marketing team and ask them to join the call. IL: Understood, paging Marketing team.","title":"Mobilizing Additional Responders"},{"location":"training/internal_liaison/#provide-executive-status-updates","text":"Provide regular status updates on Slack (roughly every 30mins), giving an executive summary of the current status during SEV-1 incidents. Keep it short and to the point, and use @here . Mention the current state, the actions in progress, customer impact, and expected time remaining. It's OK to miss out some of those if the information isn't known. @here: We are in SEV-1 due to X. Current actions in progress are to do Y. Expecting 3 mins to complete that action. Once action is complete, system should recover on its own within 5 minutes.","title":"Provide Executive Status Updates"},{"location":"training/overview/","text":"Learning about the PagerDuty major incident response process is an important part of being an effective on-call engineer at PagerDuty. This section goes over our training material for the various roles that are involved in our incident response, along with some additional information and training material from government agencies. Training Guides # Our training guides are split up by role, however you are encouraged to read through the training guides even for roles you don't belong to, as it can give you some good insight into how those people will be behaving during major incidents. Incident Commander Training - The \"IC\" is the person who drives a major incident to resolution. They're the person who will be directing everyone else. Deputy Training - The Deputy is someone who supports the Incident Commander and can take over for them if necessary. Scribe Training - This is intended for individuals who will be acting as a scribe during an incident. SME / Resolver Training - This is relevant to everyone at PagerDuty who are on-call for any team. Customer Liaison Training - This is for individuals who will be publicly representing us and interacting with customers. Internal Liaison Training - This is relevant for anyone who might be called upon to work with teams internal during an incident. Training Courses # We've also published slides and videos of some of our training courses. Originally used internally at PagerDuty to train our staff, we've since adapted them for a wider audience so you can make use of them in your own organizations. Incident Response Training Course - Introductory course on incident response and the role of the Incident Commander. Example Incident # This recorded call is a reenactment of an actual major incident that occurred at PagerDuty in January 2017 . Some details have been changed in the interest of brevity and privacy, but the incident remains otherwise largely intact. For more details about the recording, you can read the PagerDuty blog post . National Incident Management System (NIMS) # Our incident response process is loosely based on the US National Incident Management System (NIMS) , which is described as, A systematic, proactive approach to guide departments and agencies at all levels of government, nongovernmental organizations, and the private sector to work together seamlessly and manage incidents involving all threats and hazards\u2014regardless of cause, size, location, or complexity\u2014in order to reduce loss of life, property and harm to the environment. While it might not initially seem that this would be applicable to an IT operations environment, we've found that many of the lessons learned from major incidents in these situations can be directly applied to our industry too. The principles are the same and span many different environments. If you want to learn more about NIMS, we recommend the ICS-100 and ICS-700 online training courses, which go over NIMS and the Incident Command System (You can also take an online examination after training in order to get a certificate from FEMA). There is also a wealth of additional training material and courses from FEMA on NIMS, which I would encourage you to look at. If you're based in the US and interested in taking a more active incident response role in your community, we recommend investigating your local CERT programs (Community Emergency Response Teams). Many cities offer CERT training, after which you can volunteer as a CERT contributor within your community. Not only is it an opportunity to get real world experience with disaster response, but the skills you learn can be applied to everyday life too. Also take a look at the Additional Reading page. Incident Response Around the World # While NIMS is the US incident response framework, many countries have their own similar frameworks. Some are based on the US system, but many were developed on their own. There's a wealth of additional information to be learned by investigating the methods and frameworks used in countries all over the world. A book called \" Comparative Emergency Management: Understanding Disaster Policies, Organizations, and Initiatives from Around the World \" (available from the FEMA website ) compares the systems used by 30 or so different countries, and is an amazing collection of information on emergency management frameworks used around the world. Here are a few of the systems we looked at in more detail in order to adapt and improve our own process at PagerDuty. United Kingdom # The United Kingdom emergency services use a command hierarchy called Gold-Silver-Bronze Command Structure for their major operations. The framework involves three levels responsible for strategic (gold), tactical (silver), and operational (bronze) command decisions. Here are some useful reading materials if you're interested in learning more: UK.GOV - Emergency Response and Recovery . UK.GOV - Incident Command - 3rd Edition (2008) . UK Home Office - Critical Incident Management (PDF). New Zealand # New Zealand's system is called the Coordinated Incident Management System (CIMS) and is based upon the Incident Command System used in the US. One area we particularly liked from CIMS is its focus on common terminology, which helps prevents confusion during an incident and allows for a faster and more effective response. Some terminology has been changed from ICS (e.g. \"Control\" instead of \"Command\" to describe the management functions), but should still be familiar. Here are some useful reading materials if you're interested in learning more: Ministry of Civil Defence & Emergency Management - New Zealand Coordinated Incident Management System (CIMS) ( PDF ). Devereux-Blum Training & Development - Emergency Management Training Australia # Australia uses a system called the Australasian Inter-Service Incident Management System (AIIMS) which is a derivative of the NIMS framework used in the US. While based on ICS, AIIMS puts a bigger focus on span of control than other frameworks. As with New Zealand's system, there are some differences to the terminology being used (e.g. \"Incident Controller\" instead of \"Incident Commander\"), but it should still be familiar to those who know ICS. Here are some useful reading materials if you're interested in learning more: The Australasian Inter-Service Incident Management System, 3rd Edition (PDF). Incident Management in Australia Handbook Canada # Canada uses their own Incident Command System (PDF). The standard for which is maintained by a network of organizations called ICS Canada . Their website has a collection of information on how you can find local training courses in Canada, depending on your Province. Here are some useful reading materials if you're interested in learning more: ICSCanada - I-100 Introduction to Incident Command System (PDF). Canada ICS Forms - Standard ICS forms that you can download and use in your own incidents ( FEMA has the US equivalents ).","title":"Overview"},{"location":"training/overview/#training-guides","text":"Our training guides are split up by role, however you are encouraged to read through the training guides even for roles you don't belong to, as it can give you some good insight into how those people will be behaving during major incidents. Incident Commander Training - The \"IC\" is the person who drives a major incident to resolution. They're the person who will be directing everyone else. Deputy Training - The Deputy is someone who supports the Incident Commander and can take over for them if necessary. Scribe Training - This is intended for individuals who will be acting as a scribe during an incident. SME / Resolver Training - This is relevant to everyone at PagerDuty who are on-call for any team. Customer Liaison Training - This is for individuals who will be publicly representing us and interacting with customers. Internal Liaison Training - This is relevant for anyone who might be called upon to work with teams internal during an incident.","title":"Training Guides"},{"location":"training/overview/#training-courses","text":"We've also published slides and videos of some of our training courses. Originally used internally at PagerDuty to train our staff, we've since adapted them for a wider audience so you can make use of them in your own organizations. Incident Response Training Course - Introductory course on incident response and the role of the Incident Commander.","title":"Training Courses"},{"location":"training/overview/#example-incident","text":"This recorded call is a reenactment of an actual major incident that occurred at PagerDuty in January 2017 . Some details have been changed in the interest of brevity and privacy, but the incident remains otherwise largely intact. For more details about the recording, you can read the PagerDuty blog post .","title":"Example Incident"},{"location":"training/overview/#national-incident-management-system-nims","text":"Our incident response process is loosely based on the US National Incident Management System (NIMS) , which is described as, A systematic, proactive approach to guide departments and agencies at all levels of government, nongovernmental organizations, and the private sector to work together seamlessly and manage incidents involving all threats and hazards\u2014regardless of cause, size, location, or complexity\u2014in order to reduce loss of life, property and harm to the environment. While it might not initially seem that this would be applicable to an IT operations environment, we've found that many of the lessons learned from major incidents in these situations can be directly applied to our industry too. The principles are the same and span many different environments. If you want to learn more about NIMS, we recommend the ICS-100 and ICS-700 online training courses, which go over NIMS and the Incident Command System (You can also take an online examination after training in order to get a certificate from FEMA). There is also a wealth of additional training material and courses from FEMA on NIMS, which I would encourage you to look at. If you're based in the US and interested in taking a more active incident response role in your community, we recommend investigating your local CERT programs (Community Emergency Response Teams). Many cities offer CERT training, after which you can volunteer as a CERT contributor within your community. Not only is it an opportunity to get real world experience with disaster response, but the skills you learn can be applied to everyday life too. Also take a look at the Additional Reading page.","title":"National Incident Management System (NIMS)"},{"location":"training/overview/#incident-response-around-the-world","text":"While NIMS is the US incident response framework, many countries have their own similar frameworks. Some are based on the US system, but many were developed on their own. There's a wealth of additional information to be learned by investigating the methods and frameworks used in countries all over the world. A book called \" Comparative Emergency Management: Understanding Disaster Policies, Organizations, and Initiatives from Around the World \" (available from the FEMA website ) compares the systems used by 30 or so different countries, and is an amazing collection of information on emergency management frameworks used around the world. Here are a few of the systems we looked at in more detail in order to adapt and improve our own process at PagerDuty.","title":"Incident Response Around the World"},{"location":"training/overview/#united-kingdom","text":"The United Kingdom emergency services use a command hierarchy called Gold-Silver-Bronze Command Structure for their major operations. The framework involves three levels responsible for strategic (gold), tactical (silver), and operational (bronze) command decisions. Here are some useful reading materials if you're interested in learning more: UK.GOV - Emergency Response and Recovery . UK.GOV - Incident Command - 3rd Edition (2008) . UK Home Office - Critical Incident Management (PDF).","title":"United Kingdom"},{"location":"training/overview/#new-zealand","text":"New Zealand's system is called the Coordinated Incident Management System (CIMS) and is based upon the Incident Command System used in the US. One area we particularly liked from CIMS is its focus on common terminology, which helps prevents confusion during an incident and allows for a faster and more effective response. Some terminology has been changed from ICS (e.g. \"Control\" instead of \"Command\" to describe the management functions), but should still be familiar. Here are some useful reading materials if you're interested in learning more: Ministry of Civil Defence & Emergency Management - New Zealand Coordinated Incident Management System (CIMS) ( PDF ). Devereux-Blum Training & Development - Emergency Management Training","title":"New Zealand"},{"location":"training/overview/#australia","text":"Australia uses a system called the Australasian Inter-Service Incident Management System (AIIMS) which is a derivative of the NIMS framework used in the US. While based on ICS, AIIMS puts a bigger focus on span of control than other frameworks. As with New Zealand's system, there are some differences to the terminology being used (e.g. \"Incident Controller\" instead of \"Incident Commander\"), but it should still be familiar to those who know ICS. Here are some useful reading materials if you're interested in learning more: The Australasian Inter-Service Incident Management System, 3rd Edition (PDF). Incident Management in Australia Handbook","title":"Australia"},{"location":"training/overview/#canada","text":"Canada uses their own Incident Command System (PDF). The standard for which is maintained by a network of organizations called ICS Canada . Their website has a collection of information on how you can find local training courses in Canada, depending on your Province. Here are some useful reading materials if you're interested in learning more: ICSCanada - I-100 Introduction to Incident Command System (PDF). Canada ICS Forms - Standard ICS forms that you can download and use in your own incidents ( FEMA has the US equivalents ).","title":"Canada"},{"location":"training/scribe/","text":"Credit: John-Mark Smith So you want to be a scribe? You've come to the right place! You don't need to be a senior team member to become a deputy or scribe, anyone can do it providing you have the requisite knowledge! Purpose # The purpose of the Scribe is to maintain a timeline of key events during an incident. Documenting actions, and keeping track of any followup items that will need to be addressed. It's important for the rest of the command staff to be able to focus on the problem at hand, rather than worrying about documenting the steps. Your job as Scribe is to listen to the call and to watch the incident Slack room, keeping track of context and actions that need to be performed, documenting these in Slack as you go. You should not be performing any remediations, checking graphs, or investigating logs. Those tasks will be delegated to the subject matter experts (SME's) by the Incident Commander. Prerequisites # Before you can be a Scribe, it is expected that you meet the following criteria. Don't worry if you don't meet them all yet, you can still continue with training! Excellent verbal and written communication skills . Responsibilities # Read up on our Different Roles for Incidents to see what is expected from a Scribe, as well as what we expect from the other roles you'll be interacting with. Training Process # There is no formal training process for this role, reading this page should be sufficient for most tasks. Here's a list of things you can do to train though, Read the rest of this page, particularly the sections below. Participate in Failure Friday (FF). Shadow a FF to see how it's run. Be the scribe for multiple FF's. Scribing # Scribing is more art than science. The objective is to keep an accurate record of important events that occurred on the call, so that we can look back at the timeline to see what happened. But what exactly is important? There's no overwhelming answer, and it really comes down the judgement and experience. But here are some general things you most definitely want to capture as scribe. The result of any polling decisions. \u2718 This is not \"9 people voted yay, 3 voted nay\". \u2713 It is \"Polled for if we should do rolling restart. is proceeding with restart.\" Any followup items that are called out as \"We should do this..\", \"Why didn't this?..\", etc. \u2718 This is not \"Why isn't the Support representative on the call?\" \u2713 This is \"TODO: Why didn't we get paged for this earlier?\" Incident Call Procedures and Lingo # The Steps for Scribe provide a detailed description of what you should be doing during an incident. Here are some examples of phrases and patterns you should use during incident calls. Status Stalking # At the start of any major incident call, you should start our status stalking bot, so that it will post to the room an update automatically. !status stalk This will provide the update and allow the IC to see the status without having to keep asking. Note Important Actions # During a call, you will hear lots of discussion happening, you should not be documenting all of this in the chat room. You only want to document things which will be important for the final timeline. It's not always obvious what this might be, and it's usually a matter of judgement. You generally want to note any actions the IC has asked someone to perform, along with the result of any polling decisions. Polled for decision on whether to perform rolling restart. We are proceeding with restart. [USER_A] to execute. Some actions might seem important at the time, but end up not being. That's OK. It's better to have more info than not enough, but don't go overboard. Note Followup Actions # Sometimes during the call, someone will either mention something we \"should fix\", or the IC will specifically ask you to note a followup item. You can do this in Slack by simply prefixing with \"TODO\", this will make it easier to search for later. TODO: Why did we not get paged for the fall in traffic on [X] cluster? The post-mortem owner will find these after and raise tasks for them. End of Call Notification # When the IC ends the call, you should post a message into Slack to let everyone know the call is over, and that they should continue discussion elsewhere. Call is over, thanks everyone. Follow up in Slack. Don't forget to also stop the status stalking. !status unstalk","title":"Scribe"},{"location":"training/scribe/#purpose","text":"The purpose of the Scribe is to maintain a timeline of key events during an incident. Documenting actions, and keeping track of any followup items that will need to be addressed. It's important for the rest of the command staff to be able to focus on the problem at hand, rather than worrying about documenting the steps. Your job as Scribe is to listen to the call and to watch the incident Slack room, keeping track of context and actions that need to be performed, documenting these in Slack as you go. You should not be performing any remediations, checking graphs, or investigating logs. Those tasks will be delegated to the subject matter experts (SME's) by the Incident Commander.","title":"Purpose"},{"location":"training/scribe/#prerequisites","text":"Before you can be a Scribe, it is expected that you meet the following criteria. Don't worry if you don't meet them all yet, you can still continue with training! Excellent verbal and written communication skills .","title":"Prerequisites"},{"location":"training/scribe/#responsibilities","text":"Read up on our Different Roles for Incidents to see what is expected from a Scribe, as well as what we expect from the other roles you'll be interacting with.","title":"Responsibilities"},{"location":"training/scribe/#training-process","text":"There is no formal training process for this role, reading this page should be sufficient for most tasks. Here's a list of things you can do to train though, Read the rest of this page, particularly the sections below. Participate in Failure Friday (FF). Shadow a FF to see how it's run. Be the scribe for multiple FF's.","title":"Training Process"},{"location":"training/scribe/#scribing","text":"Scribing is more art than science. The objective is to keep an accurate record of important events that occurred on the call, so that we can look back at the timeline to see what happened. But what exactly is important? There's no overwhelming answer, and it really comes down the judgement and experience. But here are some general things you most definitely want to capture as scribe. The result of any polling decisions. \u2718 This is not \"9 people voted yay, 3 voted nay\". \u2713 It is \"Polled for if we should do rolling restart. is proceeding with restart.\" Any followup items that are called out as \"We should do this..\", \"Why didn't this?..\", etc. \u2718 This is not \"Why isn't the Support representative on the call?\" \u2713 This is \"TODO: Why didn't we get paged for this earlier?\"","title":"Scribing"},{"location":"training/scribe/#incident-call-procedures-and-lingo","text":"The Steps for Scribe provide a detailed description of what you should be doing during an incident. Here are some examples of phrases and patterns you should use during incident calls.","title":"Incident Call Procedures and Lingo"},{"location":"training/scribe/#status-stalking","text":"At the start of any major incident call, you should start our status stalking bot, so that it will post to the room an update automatically. !status stalk This will provide the update and allow the IC to see the status without having to keep asking.","title":"Status Stalking"},{"location":"training/scribe/#note-important-actions","text":"During a call, you will hear lots of discussion happening, you should not be documenting all of this in the chat room. You only want to document things which will be important for the final timeline. It's not always obvious what this might be, and it's usually a matter of judgement. You generally want to note any actions the IC has asked someone to perform, along with the result of any polling decisions. Polled for decision on whether to perform rolling restart. We are proceeding with restart. [USER_A] to execute. Some actions might seem important at the time, but end up not being. That's OK. It's better to have more info than not enough, but don't go overboard.","title":"Note Important Actions"},{"location":"training/scribe/#note-followup-actions","text":"Sometimes during the call, someone will either mention something we \"should fix\", or the IC will specifically ask you to note a followup item. You can do this in Slack by simply prefixing with \"TODO\", this will make it easier to search for later. TODO: Why did we not get paged for the fall in traffic on [X] cluster? The post-mortem owner will find these after and raise tasks for them.","title":"Note Followup Actions"},{"location":"training/scribe/#end-of-call-notification","text":"When the IC ends the call, you should post a message into Slack to let everyone know the call is over, and that they should continue discussion elsewhere. Call is over, thanks everyone. Follow up in Slack. Don't forget to also stop the status stalking. !status unstalk","title":"End of Call Notification"},{"location":"training/subject_matter_expert/","text":"Credit: oregondot @ Flickr If you are on-call for any team at PagerDuty, you may be paged for a major incident and will be expected to respond as a subject matter expert (SME) for your service. This page details everything you need to know in order to be prepared for that responsibility. If you are interested in becoming an Incident Commander, take a look at the Incident Commander Training page . On-Call Expectations # If you are on-call for your team, there are certain expectations of you as that on-call. This applies to both the primary and secondary on-calls. Getting paged about a SEV-3 or SEV-4 in your system comes with different expectations than getting paged with a major SEV-2. Before Going On-Call # Be prepared, by having already familiarized yourself with our incident response policies and procedures. In particular, Different Roles for Incidents - You will be acting as a \"Resolver\" or \"SME\". But you should familiarize yourself with the other roles and what they will be doing. Incident Call Etiquette - How to behave during an incident call. During an Incident - What to do during an incident. You are specifically interested in the \"Resolver\" steps, but you should familiarize yourself with the entire document. Glossary - Familiarize yourself with the terminology that may be used during the call. Make sure you have set up your alerting methods, and that PagerDuty can bypass your \"Do Not Disturb\" settings . Check you can join the incident call. You may need to install a browser plugin. You don't want to be doing that the first time you get paged. Be aware of your upcoming on-call time and arrange swaps around travel, vacations, appointments, etc. If you are an Incident Commander, make sure you are not on-call for your team at the same time as being on-call as Incident Commander. During On-Call Period # Have your laptop and Internet with you at all times during your on-call period (office, home, a MiFi, a phone with a tethering plan, etc). If you have important appointments, you need to get someone else on your team to cover that time slot in advance. When you receive an alert for a major incident, you are expected to join the incident call and Slack as quickly as possible (within minutes). You will be asked questions or given actions by the Incident Commander. Answer questions concisely, and follow all actions given (even if you disagree with them). Response Mobilization # When an incident occurs, you must be mobilized or assigned to become part of the incident response. In other words, until you are mobilized to the incident via a page or being directly asked by someone else on the incident, you remain in your everyday role. After being mobilized, your first task is to check in and receive an assignment. While it's tempting to see an incident happening and want to jump in and help, when resources show up that have not been requested, the management of the incident can be compromised. \"Never Hesitate to Escalate\" # If you're not sure about something, it is perfectly acceptable to bring in other SMEs from your team that you believe know a given system better than you. Don't let your ego keep you from bringing in additional help. Our motto is \"Never hesitate to escalate\", you will never be looked down upon for escalating something because you didn't know how to handle it. Blameless # There will be incidents. Some will be caused by you, some will be caused by others... some will just happen. Our entire incident response process is completely blameless. Blaming people is counter productive and just distracts from the problem at hand. No matter how an incident started, they all need to get solved as quickly as possible. Wartime vs Peacetime # Behavior during a major incident is very different to any other alert you may have received in the past. We call a major incident \"wartime\", and make a distinction between that and normal everyday operations (\"peacetime\"). Peacetime # The organizational structure is generally based on seniority. The more senior members of a team will lead discussions, and managers or team leads will have the final say. Decisions are made after careful consideration of all options, and to minimize potential risk to customers. Wartime # Wartime is different, and you will notice on our major incident calls that there's a different organizational structure. The Incident Commander is in charge. No matter their rank during peacetime, they are now the highest ranked individual on the call, higher than the CEO. Primary responders (folks acting as primary on-call for a team/service) are the highest ranked individuals for that service. Decisions will be made by the IC after consideration of the information presented. Once that decision is made, it is final. Riskier decisions can be made by the IC than would normally be considered during peacetime. For example, the IC may decide to drop events for a particular customer in order to maintain the integrity of the system for everyone else. The IC may go against a consensus decision. If a poll is done, and 9/10 people agree but 1 disagrees. The IC may choose the disagreement option despite a majority vote. Even if you disagree, the IC's decision is final. During the call is not the time to argue with them. The IC may use language or behave in a way you find rude. This is wartime, and they need to do whatever it takes to resolve the situation, so sometimes rudeness occurs. This is never anything personal, and something you should be prepared to experience if you've never been in a wartime situation before. You may be asked to leave the call by the IC, or you may even be forceable kicked off a call. It is at the IC's discretion to do this if they feel you are not providing useful input. Again, this is nothing personal and you should remember that wartime is different than peacetime.","title":"Subject Matter Expert"},{"location":"training/subject_matter_expert/#on-call-expectations","text":"If you are on-call for your team, there are certain expectations of you as that on-call. This applies to both the primary and secondary on-calls. Getting paged about a SEV-3 or SEV-4 in your system comes with different expectations than getting paged with a major SEV-2.","title":"On-Call Expectations"},{"location":"training/subject_matter_expert/#before-going-on-call","text":"Be prepared, by having already familiarized yourself with our incident response policies and procedures. In particular, Different Roles for Incidents - You will be acting as a \"Resolver\" or \"SME\". But you should familiarize yourself with the other roles and what they will be doing. Incident Call Etiquette - How to behave during an incident call. During an Incident - What to do during an incident. You are specifically interested in the \"Resolver\" steps, but you should familiarize yourself with the entire document. Glossary - Familiarize yourself with the terminology that may be used during the call. Make sure you have set up your alerting methods, and that PagerDuty can bypass your \"Do Not Disturb\" settings . Check you can join the incident call. You may need to install a browser plugin. You don't want to be doing that the first time you get paged. Be aware of your upcoming on-call time and arrange swaps around travel, vacations, appointments, etc. If you are an Incident Commander, make sure you are not on-call for your team at the same time as being on-call as Incident Commander.","title":"Before Going On-Call"},{"location":"training/subject_matter_expert/#during-on-call-period","text":"Have your laptop and Internet with you at all times during your on-call period (office, home, a MiFi, a phone with a tethering plan, etc). If you have important appointments, you need to get someone else on your team to cover that time slot in advance. When you receive an alert for a major incident, you are expected to join the incident call and Slack as quickly as possible (within minutes). You will be asked questions or given actions by the Incident Commander. Answer questions concisely, and follow all actions given (even if you disagree with them).","title":"During On-Call Period"},{"location":"training/subject_matter_expert/#response-mobilization","text":"When an incident occurs, you must be mobilized or assigned to become part of the incident response. In other words, until you are mobilized to the incident via a page or being directly asked by someone else on the incident, you remain in your everyday role. After being mobilized, your first task is to check in and receive an assignment. While it's tempting to see an incident happening and want to jump in and help, when resources show up that have not been requested, the management of the incident can be compromised.","title":"Response Mobilization"},{"location":"training/subject_matter_expert/#never-hesitate-to-escalate","text":"If you're not sure about something, it is perfectly acceptable to bring in other SMEs from your team that you believe know a given system better than you. Don't let your ego keep you from bringing in additional help. Our motto is \"Never hesitate to escalate\", you will never be looked down upon for escalating something because you didn't know how to handle it.","title":"\"Never Hesitate to Escalate\""},{"location":"training/subject_matter_expert/#blameless","text":"There will be incidents. Some will be caused by you, some will be caused by others... some will just happen. Our entire incident response process is completely blameless. Blaming people is counter productive and just distracts from the problem at hand. No matter how an incident started, they all need to get solved as quickly as possible.","title":"Blameless"},{"location":"training/subject_matter_expert/#wartime-vs-peacetime","text":"Behavior during a major incident is very different to any other alert you may have received in the past. We call a major incident \"wartime\", and make a distinction between that and normal everyday operations (\"peacetime\").","title":"Wartime vs Peacetime"},{"location":"training/subject_matter_expert/#peacetime","text":"The organizational structure is generally based on seniority. The more senior members of a team will lead discussions, and managers or team leads will have the final say. Decisions are made after careful consideration of all options, and to minimize potential risk to customers.","title":"Peacetime"},{"location":"training/subject_matter_expert/#wartime","text":"Wartime is different, and you will notice on our major incident calls that there's a different organizational structure. The Incident Commander is in charge. No matter their rank during peacetime, they are now the highest ranked individual on the call, higher than the CEO. Primary responders (folks acting as primary on-call for a team/service) are the highest ranked individuals for that service. Decisions will be made by the IC after consideration of the information presented. Once that decision is made, it is final. Riskier decisions can be made by the IC than would normally be considered during peacetime. For example, the IC may decide to drop events for a particular customer in order to maintain the integrity of the system for everyone else. The IC may go against a consensus decision. If a poll is done, and 9/10 people agree but 1 disagrees. The IC may choose the disagreement option despite a majority vote. Even if you disagree, the IC's decision is final. During the call is not the time to argue with them. The IC may use language or behave in a way you find rude. This is wartime, and they need to do whatever it takes to resolve the situation, so sometimes rudeness occurs. This is never anything personal, and something you should be prepared to experience if you've never been in a wartime situation before. You may be asked to leave the call by the IC, or you may even be forceable kicked off a call. It is at the IC's discretion to do this if they feel you are not providing useful input. Again, this is nothing personal and you should remember that wartime is different than peacetime.","title":"Wartime"},{"location":"training/courses/incident_response/","text":"Incident Response Training Course This is an open-source version of \"Incident Response Training\", our PagerDuty training course for incident response and incident command. It started as an internal course to train new Incident Commanders and has since developed into one that we now deliver publicly. It includes lots of introductory information on our process, and details on the Incident Commander role specifically. All the information is already available as part of our public documentation , this is just a different way of presenting it that's hopefully more engaging. Feel free to use this as a base for training in your own organization. The text presented here is a semi-accurate transcription of how the training is usually delivered. You can also watch a video of an older version of this course if you prefer. Introduction # 001. \"Incident Response Training\". Hi, I'm Rich, and welcome to \"Incident Response Training\". This is a shorter version of our internal training at PagerDuty, which we use to train up our new Incident Commanders. It's been slightly adapted for a wider audience, but the majority is exactly what we run ourselves. We're not going to be able to cover everything, otherwise we'd be here for a few days, but I'll cover some of the most important parts of our process. I'll try to keep this as short as I can. Actually, how long do we have? Can someone keep track of time for me? That would be great, thanks! Learn How to Effectively Manage Incidents # 002. Learn how to effectively manage incidents. The goal of this session is to give you an understanding of how to effectively manage incidents within your organization. I'll describe the process we use at PagerDuty for managing critical incidents, and talk in more detail about a specific role called the \"Incident Commander\". This isn't a sales pitch. I'm not on our sales team, and this isn't a talk about how to use PagerDuty to manage your incidents (although obviously it would be awesome if you did). The intent today is to introduce you to how we manage incidents internally at PagerDuty, and provide you with lots of practical information you can take away to your own organizations to either start or improve your own incident response processes. Replace Chaos with Calm # 003. Replace chaos with calm. Let's start with a quick question. How does incident response usually go in your organization today? Is it a smooth and streamlined process, or is it a lot of people talking over one another? For most of you it's probably going to be somewhere in the middle. We want less of the latter, and more of the former. We want to replace chaos with calm. Panic and chaos are not good during an incident, they only exacerbate things and causes more confusion. We want things to be calm and organized. What is Incident Response? # 004. What is incident response? Docs Reference So when we talk about incident response, what we're really talking about is an organized approach to addressing and managing an incident. This is how we define incident response at PagerDuty. They key here is on the word organized . We don't want to be running around in a panic anytime an alert goes off. We want our response to be almost routine, and for everyone to work together like a well-oiled machine. There's a quote I really like from an excellent book called Incident Management for Operations that's appropriate here, Fire is not an emergency to the fire department . . . [Y]ou expect a rapid response from a group of professionals, skilled in the art of solving whatever issues you are having. Goal of Incident Response # 005. Goal of incident response. Docs Reference It may surprise you to learn the goal of incident response isn\u2019t just about solving the problem. Give a thousand monkeys a keyboard and enough time and they can probably solve your problem. That's not good incident response. We want to solve the problem in a way which limits the damage caused, and reduces the recovery time and costs. I don\u2019t just mean financial cost either, there\u2019s a cost associated with engineer health too. Constantly waking people up at 3am can have a dramatic negative effect on their health and happiness. If financial impact is all you care about though, let's not forget that people are expensive . In a large organization, a phone bridge with 100 people sitting there mostly idle for several hours is not unheard of. If each of those people cost ~$100/hour, that\u2019s $10K every hour! That\u2019s really expensive to the business. What is an Incident? # 006. What is an incident? Docs Reference Before we can respond to an incident though, we need to define what an incident actually is . It sounds silly, but if you\u2019re not sure whether something\u2019s an incident, you don\u2019t know whether to respond to it. Here is PagerDuty\u2019s definition of an incident, An unplanned disruption or degradation of service that is actively affecting customers' ability to use the product. \"Customers\" here doesn't just refer to external customers, but can refer to internal customers too. Your definition might be different, and that\u2019s OK. I just wanted to give you an idea of the kind of definition that can get you started. You want your definition to be simple, no more than a sentence, and easily understood by anyone. You may notice that this is quite a broad definition though. A typo technically fits this description. As does a full outage. Obviously these are very different scenarios. So we do have something else too. What is a Major Incident? # 007. What is a major incident? Docs Reference We also have something we call a major incident . This is any incident where we require a coordinated response between teams. Again, this is just our definition at PagerDuty, feel free to use your own. The intention behind this definition is that sometimes incidents can be handled by a single team, maybe the owners of a service that's having trouble. That rarely requires a large response in and of itself. But as soon as they need to involve another team, whether it's customer support, or database administrators, then we declare it to be a major incident and kick off a much larger response. The coordination is key here, and we\u2019ll talk more about this later. But this still covers quite a range of potential incidents. We can get more granular. Severity Levels # 008. Severity levels. Docs Reference We also use severity levels to determine how severe an incident is, and what type of response it gets. We use SEV-5 through SEV-1 for our levels, but you may use a different scheme, P0 through P5 , or maybe even emoji, \ud83d\udd25 through \ud83d\udca9, etc. Let's imagine you're looking at a graph of traffic to your website. You can typically determine severity based on how drastically your metrics are affected. So as your website traffic drops, the severity increases. You will usually reach a point where you've set some predefined target or watermark, where as soon as the metric passes, you automatically consider something a major incident. At PagerDuty it's the difference between a SEV-3 and SEV-2 , but it may be different for your organization. Then as things get even more dire, we get into our SEV-2 's and our SEV-1 's when things completely flatline. Having pre-defined thresholds and metrics can allow you to have automatic triggers for your response process. We recommend using metrics tied to business impact. Metrics can be very useful, and often work best when they're tied to business impact . For example, a metric we monitor at PagerDuty is \"number of outbound notifications per second\", at Amazon it could be \"number of orders per second\", at Netflix it might be \"stream starts per second\", etc. Monitoring these important business metrics will then let you use automation to determine the severity of an incident and the type of response you use. If you use metrics that aren't tied to business impact (e.g. \"CPU usage is high on a host\"), then it's difficult, and sometimes impossible, to determine the severity of an incident associated with that metric. You want to use a metric that lets you know how your business is doing, not how a particular piece of equipment is doing. Anyone Can Trigger Incident Response # 009. Anyone can trigger incident response at any time. But sometimes you won't know the impact straight away. Or maybe your metric hasn't reached the predefined threshold yet. We still need a way for a human to jump in and call something a major incident. So even though we have automation at PagerDuty, we also have a mechanism whereby anyone can trigger our major incident response process at any time. This is very important for us. We've found that lowering the barrier to triggering incident response has lead to a dramatic increase in the speed with which incidents are resolved . We don't want people to sit on something because the official alarm hasn't gone off yet. If customer support gets lots of requests very quickly, it's a good sign there's something wrong, and we need them to be able to raise the alarm. We've even had interns trigger our incident response process in their first week. If the janitor walks past a graph and thinks it looks wrong, I want them to be able to trigger incident response. We were initially hesitant to introduce this, as we feared it would lead to lots of false positives. People triggering the alarm in an abundance of caution and it not really being an incident. But we've seen quite the opposite, people are pretty good at policing this themselves. Only twice have we had it be a false alarm, and both times it was warranted based on the information available at the time. Even if you do get a false positive now and then, you can use it as free practice. Triggering Incidents via Chat # 010. Triggering incidents via chat. Docs Reference So how do we let humans trigger the process? We do it with a chat command , but don\u2019t feel like that\u2019s the only right way. I just wanted to demonstrate how we do it to give you an idea. You can do it however your want. Air horn, flashing light in the office, hire a mariachi band, etc. The point is you want a way to trigger your response that's fast, easy, and available to everyone. Peacetime vs Wartime # 011. Peacetime vs Wartime. Docs Reference Once an incident is triggered, we need to switch our mode of thinking. We need a mentality shift . We want a distinction between \u201cnormal operations\u201d and \u201cthere\u2019s an incident in progress\u201d. We need to switch decision making from peacetime to wartime. From day-to-day operations, to defending the business. Something that would be considered completely unacceptable during normal operations, such as deploying code without running any tests, might be perfectly acceptable during a major incident when you need to restore service quickly. The way you operate, your role hierarchy, and the level of risk you\u2019re willing to take will all change as we make this shift. Normal vs Emergency # 012. Normal vs Emergency. Docs Reference Some people don\u2019t like the peacetime/wartime analogy, so you can call it what you want. Normal/Emergency. OK vs Not OK # 013. OK vs Not OK. Docs Reference Or OK/NOT OK. What you call it isn't as important as being able to make the mental shift. Incident Command System # 014. Incident Command System (ICS). Docs Reference So let\u2019s talk about our process a bit more. The way we do incident response at PagerDuty isn\u2019t something we invented ourselves, it is heavily based on the Incident Command System , usually abbreviated to ICS. ICS was developed after some devastating wildfires in Southern California in 1970. Thousands of firefighters responded, but found it difficult to work together. They knew how to fight fires individually, but lacked a common framework to work effectively as a larger group. After the fires, an interagency group called FIRESCOPE (Which believe it or not is an acronym for \"FIrefighting REsources of Southern California Organized for Potential Emergencies\") was formed and set out to develop two systems for managing wildland fire. One of those systems became known as ICS, and eventually became a national model for command structures at any major incident. In 2004, the National Incident Management System (NIMS) was established by FEMA, and is now used as the standard for emergency management by all public agencies in the United States. NIMS defines several operational systems as part of it, of which ICS is one. It\u2019s used by everyone from the local fire department responding to a house fire, to the US government responding to a natural disaster. It provides a standardized response framework that everyone is familiar with. NIMS and ICS are the basis of the process we use at PagerDuty, however we have heavily modified it for our needs. Turns out that things can be streamlined a bit when human life isn't on the line. Incident Response Around The World # 015. Incident response around the world. Docs Reference It's worth noting that even though our process is based on the US systems, NIMS and ICS, there are many similar systems in use all over the world. While many are also based on ICS, some were developed separately, yet offer many of the same features. I particularly like the UK system , simply because it has a role called the \"Gold Commander\", which just sounds like a Bond villain. When developing our process at PagerDuty, we looked at a few of the other systems in use around the world, and chose the bits we liked the most to add to our own. Emergency Management Around the World If you're interested in learning more about the systems in use by other countries, we have links to some official resources . There's also a book available from the US FEMA website, called \" Comparative Emergency Management: Understanding Disaster Policies, Organizations, and Initiatives from Around the World \" where it compares the systems used by about 30 different countries. Roles # 016. Roles of incident response. So let's talk about the roles involved in our process . I'm going to introduce the roles one by one, but I don't want you to get scared by the number that will be on the slide by the end. We didn't start with this many roles, and we don't have people filling all the roles in every incident. This is just showing the available roles, and defining what they are. The process and roles will grow and shrink to fit the size of the incident at hand. While we don\u2019t use exactly the same roles as ICS, we picked out the ones that matter for us in order to get our role structure. First up is the Incident Commander , usually abbreviated to IC. They're the person in charge and the most important role in the process. They make all the decisions, and all information flows up to them. What they say goes. I'm going to be talking more about this role in a moment, so we'll continue on with the others for now. Next up we have the Deputy . This is basically a backup Incident Commander. They're training as an IC and will be listening to all the same information. They help to make sure nothing gets missed by the IC, and acts as a hot-swap standby should the IC want to handover command. This is one of those roles you won't need if you're just starting out. For a long time our Deputy and Scribe would be the same person. Then we have the Scribe . The scribe's job is to keep an accurate timeline of events. What has happened, when it happened, and the key decisions that have been made. We use Slack for this, and the Scribe will be writing down notes into a Slack room, which gets us nicely timestamped data. It's worth noting that they're not doing a direct dictation of a voice call. It's not \"John said this, Mary said this\". It's more like \"We're deciding between A and B, we've decided on A\". Together, these roles are called the Command Staff. Next we have the Customer Liaison . This is a member of our support team, and their job is to handle the two-way interaction with our customers. So they'll update customers as to what is going on, whether that's via email, tweet, or updating our status page. But they'll also let us know what customers are saying too. If we're getting 100's of support requests, or no one has raised a ticket at all. Since this can be useful information in tracking down a cause, and determining the level of risk we can take during our recovery. The Internal Liaison is a relatively new role in our process. Their job is to handle all the interaction with internal teams, such as our executives, or our marketing teams, and so on. We have a separate Slack channel for incident updates, to which the Internal Liaison will post regular status updates, and answer any questions from the rest of the organization. This keeps those questions out of our main response, but allows people to still get answers. The internal liaison will also page other teams as necessary if they're needed on the response. Again, this isn't a role you'll need for most companies, for a while this was also handled by our Deputy/Scribe role. Together, these are the liaisons. Finally, we have the Subject Matter Experts , or SME's. These are the people who will actually be fixing the problem. They'll be the ones logging into servers, changing code, and running commands. Today, I\u2019m going to focus on one role in particular, that of the Incident Commander. Incident Commander # 017. Incident commander. Docs Reference The incident commander is one of the most important roles you can have. Even if you don\u2019t have deputy, scribe, customer liaison, etc. The Incident Commander is one you should get first (well, after the SME\u2019s of course, you probably need someone to solve your problem before you need someone to coordinate the response). Single Source of Truth # 018. Single source of truth. They\u2019re the single source of truth during an incident, and are the ones in charge. The big cheese. They make all decisions, and no action should be performed unless the IC has said so. Highest Ranking Person # 019. Highest ranking person. No matter their day-to-day role, and IC is always becomes the highest ranking person on the response. They're running the show. Remember what we talked about earlier with the mentality shift? Well, this is one of the things that will change during an incident. Is the Incident Commander a dictator? Only in the sense that they are in charge and their decisions cannot be overruled. They do not rule by force, or go on power trips and order people around for the sake of it. A good Incident Commander will listen to their experts and make the best decision they can based on the information available. Incident Commanders should show empathy towards all responders. Higher than the CEO # 020. Higher than the CEO. Even if the CEO joins the response, the IC still outranks them in an incident response situation. This is absolutely critical for successful incident response, but it does require buy-in from your executives. Please don't surprise your CEO with this, it will not go well for you. Whether this works for you will depend on your organization. This is how we do it at PagerDuty and it works well for us, but I can imagine it not being easy to get this sort of buy-in in other organizations. I used to work in the airline industry, and I don't think this rule would fly there (Get it? Airline industry. Fly. Hello? This thing on?). Not a Resolver # 021. Not a resolver. Importantly, the IC doesn't resolve the incident, they coordinate and delegate all tasks. They're the conductor of the orchestra, they're not playing an instrument. They're not acting as a resolver, and shouldn't be looking at graphs, or logging into servers. This can be especially hard sometimes if the IC is an engineer in their day-to-day role, as they may naturally want to jump in to try and help, but that urge must be resisted if they're acting as an IC. If they absolutely are the only person who can solve the problem, then they should handover to another Incident Commander and assume the role of a Subject Matter Expert instead. We'll talk a bit more about this scenario later. BSOD # 022. Uh oh! Oh dear, that\u2019s not good. Did something go wrong? Seems we have ourselves an incident! So what do you do when things break in the middle of the night? What's the first step to responding to an incident? It turns out that the first step in any incident response is always the same, whether you're a small startup or a large enterprise. Whether there's 25 cents on the line, or 25 billion dollars. Don't Panic # 023. Don't panic. Don\u2019t panic. It elevates stress, and causes others to panic. It\u2019ll end up hurting your incident response a lot more. It's OK to panic on the inside. We're only human after all. It's a natural reaction to panic in these sorts of situations a little bit. Everything about getting paged is designed to get adrenaline flowing. Loud pager sounds and so on. Just try not to outwardly show panic, because it will cause others to do the same. Act calm, and others will follow suit. Those with experience will stay calm, and that can make the difference between a chaotic incident, and one that resolves smoothly. So don\u2019t panic! IC Introduction # 024. \"Hi, I'm Rich, I'm the Incident Commander\". Docs Reference But today we're lucky to have an IC here. Hi, I'm Rich, I'm the Incident Commander. Every call should start like this. Well, use your own name and not mine, but you get the idea. There are a few important things here with the way I phrased this. What if you join a call and there's no IC? It doesn't always work out that the first person to join the call is the incident commander. So what do you do in that case? Some guides will recommend the first person who joins acts as the Incident Commander, regardless of training. ICS usually works this way, where the first person on-site acts as the IC until someone more qualified arrives. We tried that and found it just doesn't work in practice. For example, let's say that the first person to join your response call becomes the acting IC. After an incident occurs, how long do you think it will be before one of your responders joins the call? Yeah, turns out it's about 10 minutes. Because people are afraid of being the IC. You can't force someone to be an IC, and we prefer to make sure the acting IC at least has the training. It may add a few minutes to the start of an incident, but it makes it go smoother and quicker overall. If a trained IC joins the call and there's isn't an existing IC, they will take on the role. They ask \"Is there an IC on the call?.... hearing nothing, this is X, I'm the Incident Commander\". It's also worth noting that the on-call IC doesn't automatically take over when they join. Whoever is the active IC on the call is in charge until they perform a handover. Introduce Yourself # 025. Introduce yourself. Firstly, I introduced myself by name. I didn't just say \"Hi, I'm the IC\". I'm not an emotionless robot. Introductions are important so that people know who you are. They\u2019ll usually be referring to you as \"IC\", rather than by name, but it still humanizes you more. If you introduce yourself by name people will treat you differently and it'll help to make things go a little bit smoother. You\u2019ll find soon that a lot of the job of an IC involves psychology and phrasing more than technical expertise. Say Incident Commander # 026. Say \"Incident Commander\". Then I said that I'm the \"Incident Commander\". I didn't abbreviate to IC, since new people might not understand the lingo yet. Stating it that way made it very clear. We want to make sure we always use clear language . Additionally, saying the word \"Commander\" here will subconsciously instill in people that you're in charge. Good Communication # 027. Good communication is essential. Good communication is essential. A breakdown in communication can hamper the entire response process. One of your jobs as an IC is to keep the lines of communication clear and maintain discipline. Don\u2019t throw weird or unfamiliar acronyms into the discussion. Acronym Overload # 028. Acronym overload. Too many acronyms and internal lingo will upset newcomers and adds cognitive overhead. As shown in this completely realistic example. It takes much longer to say \"Let's get the Incident Commander on the response call, then get a bacon, lettuce, and tomato sandwich for all the Subject Matter Experts\". But it's much clearer what is being asked for. Clear is Better Than Concise # 029. Clear is better than concise. Docs Reference Clear instructions are more important than concise instructions. You want to favor explicit and clear communication over all else . If you have the choice between taking 5 seconds and abbreviating, or taking 30 seconds and making it clear, take 30 seconds. Don\u2019t give a long essay, but make sure the instructions are unambiguous. There's a tendency to want to rush through an incident, that every second counts, and that you need to use abbreviations to get the fastest possible response. Unfortunately you'll find that using unfamiliar and unclear language will almost always prolong an incident. What's Wrong? # 030. What's wrong? Docs Reference OK, so how do we actually start solving our problem? The first step is to collect information from your Subject Matter Experts (SME) for their services/area of ownership. Ask what's wrong, and gather the symptoms of the incident. Is it only affecting one system? Is it affecting everything? Was there a specific metric that triggered an alarm? We call this \u201c sizing up \u201d. We're trying to get an idea of the scope of the incident. What Actions Can We Take? # 031. What actions can we take? Docs Reference Next we want to ask our experts what they want to do to fix their systems. Remember, the IC isn't coming up with solutions, we want to ask the people who are the experts for their services what they want to do to. They will have a much better idea of the actions that can be taken. We want to collect proposed repair actions. What Risks Are Involved? # 032. What are the risks involved? Importantly, we also want to make sure we ask what risks are involved with the proposed actions. \"What impact will that have?\", \"What are the risks involved?\", \"How confident are you it will work?\". Without this information, we can't make an informed decision. For example, if we need to restart our servers to fix a problem, we could either reboot them all at once and be done in 30 seconds, or we could do a rolling restart and take 10 minutes. On the surface of it, the first option sounds like the best. But, if we ask for the risks involved, we'll learn that the first option will cause downtime for everyone in those 30s, whereas the second option, while slower, will result in no downtime for end users. This can change the decision you make, so it's important to get the information. Make a Decision # 033. Make a decision. Once we have a collection of actions and their associated risks, it's time to make a decision. Sometimes there's an obvious path forward, with one option being clearly better. But sometimes you're presented with two equally bad options. There\u2019s no golden rule here I can give you, it\u2019ll be up to context and your company culture. But my advice if you can't decide between two options is to literally flip a coin. Making the wrong decision is better than making no decision. Making no decision doesn't help to make forward progress, you learn nothing new and the incident is still going on. Making a decision, even if it's the \"wrong\" one will give you more information. If it turns out to be wrong, you can then put all your resources into the other option. A wrong decision gives you more useful information, making no decision gives you nothing. You want to avoid decision paralysis at all costs, as it can prolong your incident further. 'Do nothing' is an acceptable decision. I should note that the above advice is intended for the situation when you can't decide between two options. \"Do nothing\" is a perfectly acceptable decision if that's the course of action you want to take. It is sometimes appropriate to get more information by waiting and seeing what changes. Gain Consensus # 034. Gain consensus. Once we've made a decision, we need to gain consensus for our plan. But wait, why? Didn't I say earlier that the IC is basically a dictator and everyone should follow their instructions? While technically true, we want to be sure we give a chance to listen to any potential problems our experts may have with the plan. We don't want people to come back later and say things like \"I knew that wouldn't work\". We want to make sure we stop the hindsight 20/20 problem . It demotivates responders, and wastes time. But gaining consensus amongst a large group of people can be a bit difficult. Blue Background # 035. This background is blue. Let's look at a quick example to show what I mean, I propose that this background is blue. Does everyone agree? Audience participation! This is where I would usually get few people from the audience nodding or quietly saying \"Yes\". I'll point to about 5 or 6 people who did nothing and ask them one by one if they agree. Until it gets uncomfortable. See how long it\u2019s taking us to reach consensus? Distributed consensus is hard , you\u2019ll be there forever trying to agree on the proposed actions. Let's try it a different way, I propose that this background is blue. Are there any strong objections? ... Hearing none, the background is blue, let's proceed. Any Strong Objections? # 036. Are there any strong objections? Docs Reference See how much faster that was? I implicitly got the consensus of everyone in the room , so none of you could come back later and say you didn't think the background was blue, because I gave you all a chance to object. Doing it this way optimizes for the 99% case. Most of the time there won\u2019t be any objections and you can just continue. But likewise, we don't really care if people agree with us. We care if people disagree , that's the information we need the most. If you ask everyone to agree, you may get one person saying \"No\" with a really important point, but you can't hear them because everyone else is saying \"Yes\". Strong # 037. STRONG objections. This is one of the most useful phrases in your toolkit as an IC as it allows you to get consensus on a decision very quickly, and prevents the hindsight problem from popping up later. The way we phrased it though is important, the word \" strong \" subconsciously instills in people that we're still in an incident situation, and normal concerns might not apply. Make sure to leave enough of a pause for people to raise any objections they may have. It's no good asking for strong objections then moving on. All responders need to have the opportunity to raise a concern. So now that we have consensus, we need to execute the plan, that means assigning the task to someone. Before we do that though, I have a quick question. How Long Have I Been Talking? # 038. How long have I been talking? At the start, I asked if someone could keep track of the time for me. Did anyone actually do that? Can anyone tell me exactly how long I've been talking? Probably not. But why not? I clearly asked. It's likely because of how I phrased the question. Bystander Effect # 039. Bystander effect. I said \"Can someone...\". This is called the bystander effect . Everyone assumed someone else was doing it, so no one ended up doing it. If by some chance, someone actually did do it, you won't know who it is anyway, or if they've even started. A good example of this is if there's a medical emergency, and you shout \"Somebody call 911!\", you'll find that no one does, because everyone assumes someone else is doing it. If you're ever in that situation, you want to point to someone and say \"You, call 911\". Then it'll get done. Localization If you're outside the US, substitute 911 for whatever your local emergency number is. You probably don't want to be shouting \"You, call 911\" if you're in Europe for example. So what I should've done was point to someone in the room and say, You, please keep track of the time and give me a little wave when we get to 30 minutes, starting now. Understood? See how different that was? Assigning Tasks # 040. Assigning tasks. Docs Reference In the context of an incident, that might look like this. It's a little more verbose than \"Can someone investigate the cause?\", but it's a lot clearer what I want to happen. Brings us back to clear is better than concise from earlier. Several important things happened in this exchange as I was assigning the task. Assign Specific People # 041. Assign tasks to a specific person. First, the task was assigned directly to a specific person . It\u2019s ok to assign it to a role to \u201cDBA on-call\u2026\u201d, etc. But it must be a single individual. Don't assign things to a group, because they won't get done. Time Limit # 042. Time-box all tasks. Second, the task was given a time-limit. This means the SME knows exactly how long until I come back to them for an answer, so they won\u2019t be surprised or caught off guard. It sets the expectations. Acknowledgement # 043. Get acknowledgement. Finally, I confirmed that they had understood the instructions and are going to carry them out. So I don\u2019t come back in 5 minutes and find they never started, or have additional questions. Followup # 044. Followup. Then after the time is up, we can simply ask for the results of their task. Of course they will always have the right answer the first time and will never need any additional time to investigate, right? Need More Time? # 045. What if they need more time? It's not always going to be the case that things get done within the timeframe. So what do you do if, after 5 minutes, they need more time? Don\u2019t just give them another arbitrary time limit, because they\u2019ll keep coming back and you\u2019re just going to be wasting time giving them 5 minutes chunks for an hour. Instead, ask your experts how long they need. More Time # 046. More time. This isn't going to be like in the movies, where you ask how long someone needs, they say two hours and you slam you fist on a table and say \"You've got one!\". You need to trust in your experts to give you accurate estimates, and give them the time they need. Putting people under unreasonable pressure is only going to lead to mistakes being made. You can do this the first time too. You can also just ask your experts how long they need the first time you hand out the task instead of picking a time yourself. Sometimes we've found it easier to give a time-limit ourselves if it's an action that's been done before and we have a rough idea of how long it should take. But there's nothing wrong with asking how long someone needs as you assign the task either. Solving Incidents (1) # 047. Flow chart for solving incidents. And we just keep following this pattern until the incident is resolved. Ask for a status from your experts. Decide on an action based on what you\u2019re told, gaining consensus for the plan. Assign the task out. Follow up with the tasks once done, and repeat if there are still problems. Solving Incidents (2) # 048. Another flow chart for solving incidents.. More generally, we\u2019re following this cycle for each incident. We size-up the situation, stabilize things (that's the loop we just showed), keep everyone updated as to what's going on, then verify the situation is fixed before ending the response. If it's not fixed, we start again. Ignore the IC # 049. \"Ignore the IC, do what I say!\" Uh oh, an executive has joined the response and is trying to override the IC\u2019s decisions. That\u2019s not great. Remember that the IC is the ultimate authority in incident response, their decisions are the ones that matter. So how do you handle the awkward situation when someone tries to override those decisions? We have a top tip here, a great question to keep in your tool belt as an incident commander. A simple question that can immediately diffuse this situation. Do You Wish To Take Command? # 050. Do you wish to take command? Docs Reference Do you wish to take command? Watch how quickly they don\u2019t answer with \u201cyes\u201d. If they do, great! You\u2019re off the hook and can respond with, Understood. Everyone on the call be advised, I'm handing over command to executive A. They are now the Incident Commander. But most of the time they\u2019ll say either say \"No\"\", or not answer at all, in which case you can continue on as normal, perhaps saying, In that case, please cause no further interruptions or I will have to remove you from the call. But \"Do you wish to take command?\" is the most useful phrase for dealing with that kind of executive hostile takeover. Executive Swoop # 051. Executive swoop. This is a class of problem we call Executive Swoop. Well, actually it's \"Executive Swoop and Poop\", but I was asked not to put that on the slide. The previous example isn't typical though. It's rare for someone to come in and purposefully start causing a problem like that. We're going to look at some more common examples of executive swoop next, but it's worth noting that none of these happen maliciously . No executive joins with the intent of hindering the process, they\u2019re trying to motivate people and find out what\u2019s going on. It\u2019s their business too! Making sure your executives understand why these things are a problem is important, so be sure to followup after an incident if these things happen. Let's Resolve This in 10 Minutes # 052. Let's try and resolve this in 10 minutes. Let's try and resolve this in 10 minutes please! This one definitely wasn't said by anyone at PagerDuty ever. Nope. Definitely not. On the surface this seems pretty benign. The executive is merely trying to motivate staff and encourage them to solve the problem quickly, right? Unfortunately, that's not how others on the call are going to interpret it. Instead they're more likely to be sarcastically thinking \"Well, I was going to take an hour, but since you've said that, OK I'll do it in 10 minutes\". It assumes people aren\u2019t already working as hard as possible to solve the problem. It demotivates responders, and adds additional stress. Your job as IC is to nip this in the bud and keep things on track. Keep Comments Until The End # 053. Keep your comments until the end. Docs Reference To keep things moving, you need to remind the executive of what's going on, and direct questions to be handled at a later time. It may come across slightly abrupt, but it gets the point across quickly and allows you to keep moving. Most people will pick up on the subtext here. We're in the middle of an incident, please keep your comments until the end. Remember, don't be mean , just state the facts and keep things flowing. We're not trying to shame people or make them feel bad, we're trying to keep our incident moving towards a resolution. Spreadsheet of Affected Customers # 054. Can I get a spreadsheet of affected customers? Can I get a spreadsheet of all affected customers? Another one that's definitely never been mentioned on any PagerDuty incident response call ever. An exec joins the call and wants to get a list of impacted customers. The problem is that in order to find that out, we'll need to take someone away from the effort of responding to the incident, at a time when we need them most. If you can spare the resources, then feel free to dedicate some to finding the information. But more often than not you wont have the resources to spare. So we can just tell the executive that. The Incident Takes Priority # 055. The incident takes priority. Docs Reference We can either get you that list, or fix the incident. Not both. The incident takes priority. Note how this isn't phrased as a question, it wasn't \"We can either get you that list or fix the incident, which do you want?\". The incident commander has already made the decision , they're simply letting the executive know what it is. Remember that the IC is still in charge, you don't want to cede decision making to someone else during an incident. Really a SEV-1? # 056. Is this really a SEV-1? Is this really a SEV-1? Oh look, it's another one that's never ever happened at PagerDuty. We used to have a really big problem with this one. We'd start incident response calls, then spend the first 10 minutes arguing over whether it was a SEV-3 or a SEV-2 . By the time we were finished, we would be 10 minutes into a SEV-1 and have made no progress. So now we have a rule. We don't discuss incident severity during the call. If we can't decide between two, we always assume it's the higher severity and move on. Don't litigate severities during an incident. It's a waste of time. Treating as a SEV-1 # 057. We're treating this as a SEV-1. Docs Reference So the IC will need to make it clear we don\u2019t discuss, and that we\u2019re treating it as a SEV-1 . It may turn out to be a SEV-4 , who knows, it doesn\u2019t matter. That's a discussion for the post-mortem. We do not discuss incident severity during the call. We're treating this as a SEV-1. Once you\u2019ve spun up the gears of incident response, you may as well finish the process, if anything it just gives you all more practice. Notify Stakeholders # 058. Notify stakeholders. Pretty much all of these examples of executive swoop can be pre-empted by involving stakeholders in the process , giving them a way to stay up to date. At PagerDuty we have a separate Slack room just for incident updates. It's less noisy than our main response room, and gives succinct updates for folks who want it. This allows execs to stay in the loop, and also ask questions without affecting the main response. In our process, the Internal Liaison is responsible for monitoring and updating that channel. Stakeholders are not allowed to talk on our response call, or in our main incident response chat room. They must take all discussion to the updates room or via the Internal Liaison. The Incident Commander is responsible for keeping our primary communication channels free of those types of discussions or questions, and direct people towards the Internal Liaison. Belligerent Responder # 059. The belligerent responder. There are other things than can hinder your response though that don't fall under the category of executive swoop. This one we call the belligerent responder. It was originally called The Drunk Engineer , but again, I was asked not to put that in the slide. There are plenty of awkward situations that can present themselves on response calls. There can be big ego\u2019s and strong opinions. What do you do when someone on the call is being belligerent and hampering the response process? You need to be firm, and let them know what will happen if they continue. Disruptive # 060. You're being disruptive. Docs Reference You're being disruptive. Please stop, or I will have to remove you from the call. State the facts, give them a way out to save face , but state what will happen if they don\u2019t. No second chances, follow through on the action if they don\u2019t respond. It can be harsh, but that\u2019s what needs to be done. Again, we've phrased this in a particular way. We said \"I will have to remove you\", rather than \"I will remove you\". It makes it seem like the decision is out of your hands and that you'll be forced to do it. It can make it less personal and less likely to cause awkward problems after the call is over. Do Responders Get Tired? # 061. Do responders get tired? Another problem that can pop up during a response is when we have long running incidents. Do we use the same IC for a 12 hour incident? Do IC's even get tired? Well, of course they do! They're people too. This is another human cost associated with incident response that you want to try and minimize. All of the roles in the response process can be mentally fatiguing. When you get tired you start to forget things and make mistakes, so it's important to try and keep a fresh perspective as often as you can. Handovers Are Encouraged # 062. Handovers are encouraged. For this reason, we actively encourage handovers in our process. Usually every hour or so is what we recommend, but it's at the discretion of the people involved. 3 hours would be the absolute upper limit where we would start requiring a handover. Make sure to rest after handing over. It can be very tempting after handing over command to want to stay on the call and listen in, to try and stay on top of things and see how things are going. Avoid this at all costs. Once handing over your role to someone else, you should leave the response call and all associated chat rooms. Take a break away from anything related to the incident. You may be required to take the role again later if the incident is very long running. If you've stayed listening in the entire time, then while you would be up to speed, you would still be fatigued and would not be able to respond as efficient as if you'd taken a real break. Handing over command is important, and really easy. Get the new IC up to speed out-of-band from the main discussion. We privately message on Slack for example. If you have a deputy, then it's even better, because they would already be on the call and up to speed. Then you handover. Handover # 063. Handover. Docs Reference State that you are handing over command, and then the new IC begins as if it were a new call. Simple! Since we want to be able to handover, it's important to have as many trained IC's as you can. The more you have, the better. Ideally you want enough for at least a daily rotation. How do you get lots of trained Incident Commands? This is a question we get a lot, and not something we have a golden solution for. Many people are hesitant to take on the responsibilities of being an IC in addition to their current work and on-call responsibilities, which is a perfectly valid concern. One of the best ways we found to increase the pool of Incident Commanders is to encourage folks outside of normal engineering teams to take on the role. Those who wouldn't normally be part of an on-call rotation. Not only can they provide an outside perspective during incidents that is sometimes missing, but it can also help to build further empathy with others in the organization who regularly go on-call. Anti-Patterns # 064. Anti-Patterns. Docs Reference Let\u2019s talk about some anti-patterns . Things which seem like they would help incident response, but really don\u2019t. Knowing these now will save you the headaches and growing pains we went through. Getting Everyone on the Call # 065. Getting everyone on the call. Docs Reference Believe it or not, we used to page every single engineer at PagerDuty whenever we had a SEV-2 . I'm not joking. It was horrible. It worked great when we only had 5 engineers, less so when we had 50. It's important to maintain an effective span of control during an incident. No one person should have more than ~7 people reporting to them. Any more than that and you have too many cooks in the kitchen. Waking up 30 engineers at 3am causes untold damage. Please don't do it. Not Letting Responders Leave # 066. Not letting responders leave. Docs Reference Remember I mentioned at the beginning that one of our goals is to reduce the cost associated with an incident? That includes the human cost. Waking people up at 3am is costly. But keeping those people on a call in which they can\u2019t do anything is even worse. So if some responders are no longer needed, let them leave the call. Your co-workers time is more costly than servers, don\u2019t burn them out! I would recommend you not list everyone you want to leave the call, since you might miss people. Better to say who you want to stay, that way it also solidifies who you want to stay too. For example, Operations, Support, and Rich, please stay on the call. Everyone else, feel free to drop off at your discretion. Too Frequent Status Updates # 067. Too frequent status updates. Docs Reference Executives especially love frequent status updates. But providing them too frequently can cause things to get out of hand. If it takes 5 minutes to write an update, and they want an update every 5 minutes, you can start to see how long it's going to take to solve the incident. At PagerDuty, we keep our internal updates to about once every 20-30 minutes. Writing the update takes away time from solving the incident, so that needs to be balanced. This cadence has worked well for us. Being Overly Focussed On An Issue # 068. Being overly focussed on an issue. Docs Reference The IC is generally the person who has the bigger picture of what's going on. But there can be a tendency for responders to become too focussed on the problem they see in front of them, rather than taking the bigger picture into account. This usually presents itself on an incident call with an SME constantly bringing up the same issue without listening to instructions from the incident commander, and having tunnel vision for the specific issue on their system. Try not to get tunnel vision or chase red herrings. Always keep the bigger picture in mind. Requiring Deeply Technical ICs # 069. Requiring deeply technical incident commanders. Docs Reference We used to require that all of our Incident Commanders be experienced engineers with deep technical knowledge of all PagerDuty systems. This was one of our bigger mistakes. Remember that IC's aren't responders, they aren't the ones actually fixing the problem, so they don't need deep technical knowledge. IC's are experts at coordinating the response, not at solving technical issues. You should be relying on your SMEs for that. Removing the restriction on technical knowledge led to a dramatic increase in our pool of available incident commanders , and didn't have any effect on our ability to respond to incidents. We now have ICs from all across the organization, with even more currently in training. It's already hard enough to get people to want to be an IC, so don't add further unnecessary restrictions to your pool. Taking on Multiple Roles # 070. Taking on multiple roles. Docs Reference In past PagerDuty incidents, we've had instances where the Incident Commander has started to assume the Subject Matter Expert role and attempted to solve the problem themselves. This typically happens when an engineer is the IC, and the incident is something to do with a system they helped to build. It's very tempting to say \"I know how to fix this!\" and jump in and solve the problem yourself. But you cannot do that as an IC. Inevitably, the incident will be bigger than you think, and while you're trying to fix you little fire, there's another one happening in another service and you've lost sight of the bigger picture. You cannot take on another role at the same time as being an Incident Commander. If you absolutely are the only person who can solve the problem, then handover to another IC and assume the role of an SME for the remainder of the incident. Litigating Policy # 071. Litigating policy during an incident. Docs Reference Just like with severities, policy and processes should not be discussed during an incident. The current process should be followed, and any concerns should be raised afterwards, either during a post-mortem or directly to the team managing the incident response process. Trying to change the process during an incident is only going to prolong the current incident. That's not the time to have that discussion. Averse to Process Changes # 072. Being averse to process changes. Docs Reference Finally, once a stable process is in place and incidents are getting resolved, there can be lots of hesitation and resistance to changing that process. \"If it ain't broke don't fix it\". As your company grows, your response will need to change. Holding on to your old processes and practices for too long can hinder your incident response going forward. Don't be reckless, of course, but try to introduce sensible changes and don't be afraid to make changes which might slow things down in the short-term, but will make things faster in the long-run. These are the hardest changes to make, but ultimately the most worthwhile. Resolved # 073. Resolved. OK, so if all goes well, you're incident will get resolved. That means we're all done and we can go home, right?! Well, not quite yet. There's still one more thing we need to do. Don't Neglect The Post-Mortem # 074. Don't neglect the post-mortem. Docs Reference We need to do a post-mortem . Or after-action review, learning review, retrospective, incident report, etc. Whatever you want to call it, the name doesn't matter as much as actually doing one! Don't make the mistake of neglecting a post-mortem after an incident. Without a post-mortem you fail to recognize what you're doing right, where you could improve, and most importantly, how to avoid making the same exact mistakes next time around. A well-designed, blameless post-mortem allows teams to continuously learn, and serves as a way to iteratively improve your infrastructure and incident response process. Post-mortems are an important followup action and should never be missed. Even if you triggered an incident and then decided it was a false alarm, you should still do a brief post-mortem. You just mobilized a response when you didn't need to, so you want to identify how you can make sure that doesn't happen again. Create the Post-Mortem # 075. Create the post-mortem. Docs Reference The first step is to create the post-mortem itself. This is the job of the Incident Commander. I don't mean they're going to write the entire post-mortem, they're just going to create the initial template . We want to make sure that a link exists so that when people ask \"When will we know what went wrong?\" you have something to give them. Pick an Owner # 076. Pick an owner. Then the IC needs to assign an owner. Remember how we assigned tasks to specific individuals? It\u2019s no different with a post-mortem. Make sure there\u2019s a clear owner, and that it's an individual and not a team. The most surefire way to make sure a post-mortem doesn't get completed is to assign it to a team instead of a specific person. The person you assign is responsible for completing the post-mortem, but they don't have to do it all themselves. They can delegate out sections as they see fit. But you need someone on the hook for making sure it gets finished. As with assigning other tasks, you also want to give them a deadline, and make sure they've understood that they're responsible for completing the post-mortem. Blameless # 077. Blameless. Importantly, post-mortems need to be blameless . If someone made a mistake, you just spent lots of money training them to never do it again. You can\u2019t fire your way to reliability. Let's say Bob ran a command which deleted your entire database. Your post-mortem shouldn't be \"Bob made a mistake and should be fired or have his access revoked!\". The post-mortem should be \"Why is our system configured in a way which allowed a single user to delete the entire database?\". If you name and shame people in a post-mortem, it demotivates everyone. Next time someone makes a mistake, they're not going to own up to it, because they'll be afraid of getting shamed too. You want people to bring up problems, because then you get to fix them quickly. Review the Process # 078. Review the process too! Don\u2019t forget to also review the process as part of the post-mortem. How can you change the process to make it better? What isn\u2019t working out well? Just as it\u2019s important to learn from and fix mistakes in your software, you want to do the same for your incident response process. Practice # 079. Practice makes perfect. Finally, you want to practice your incident response process as much as you can. You don't want to be doing it for the first time during a real incident. Reading about it is one thing, but going through the motions is very different. Start by running mock incidents. Then treat your smaller incidents as if they're larger ones. If you trigger incident response and find it's not a real incident, treat it like one anyway since it's free practice. At PagerDuty, we run something called Failure Friday where we purposefully inject failure into our systems to test their resilience. We treat this like a major incident, with an incident commander and everything. It allows us to practice while we're not under the stress of a normal incident. We also play a game called Keep Talking And Nobody Explodes . Yes, that's right, we play video games at work. But we've found that this game really helps to simulate a lot of the things an incident commander has to deal with, and is a great way to get some stress free practice. The bottom line is to practice as much as you can, so that when you do have the inevitable incident, your response is just routine. Open-Source Response Docs # 080. Our open-source incident response documentation. This was just a brief taste of the training we run at PagerDuty for our own Incident Commanders. We had nowhere near enough time to cover everything. Good news though! We have published our entire incident response process online. It is an exact copy of our internal documentation only with things like phone numbers removed. It's complete free to use, and is open-sourced under an Apache 2 license so you can use it in your own organizations. It's on GitHub and we do accept pull requests if you spot any mistakes or have improvement suggestions. Everything I've talked about today can be found in the documentation, and there's lots of great additional reading material if you want to learn more. Response Docs Image # 081. Response docs screenshot. It looks pretty too. Summary # 082. Summary. Incident command training is useful in so many situations outside of a server exploding in the night. It can be applicable to many different things in your life, whether it's staying calm after a fender bender on the highway, or jumping into action to help during a major natural disaster. You'd be surprised at how useful it can be in everyday life. I even wrote a slightly tongue-in-cheek blog post called \" BabyDuty or: How PagerDuty Accidentally Prepared me for Fatherhood \" where I compared childbirth to a major incident, and the role of a parent with that of an Incident Commander. Though the older my daughter gets, the more I'm starting to realize it perhaps wasn't quite as tongue-in-cheek as I first thought. Anyway, with that, I'll leave you with a quick summary of the main things we discussed today. Thanks! Questions? If you have questions about this training material, feel free to ask me on Twitter, I'm @r_adams . Image Credits # 083. Image credits. Image Credits Here are the credits for all the images used throughout this training material. PagerDuty University # 084. PagerDuty University. PagerDuty University Shameless plug: If you're interested in our longer courses on this and other topics, including how to use PagerDuty to do it, we offer a variety of different training programs as part of PagerDuty University \u2014 from private full-day courses at your own offices, to public instructor-led training. Video # PagerDuty Summit Series Chicago 2017 As a special bonus for making it to the end, here's a recording of an earlier version of this training given at a PagerDuty event in September 2017. The material will differ slightly from that shown on this website, as we have made changes and refined the content since then. But it should give you a decent idea of how the course is usually presented.","title":"Incident Response"},{"location":"training/courses/incident_response/#introduction","text":"001. \"Incident Response Training\". Hi, I'm Rich, and welcome to \"Incident Response Training\". This is a shorter version of our internal training at PagerDuty, which we use to train up our new Incident Commanders. It's been slightly adapted for a wider audience, but the majority is exactly what we run ourselves. We're not going to be able to cover everything, otherwise we'd be here for a few days, but I'll cover some of the most important parts of our process. I'll try to keep this as short as I can. Actually, how long do we have? Can someone keep track of time for me? That would be great, thanks!","title":"Introduction"},{"location":"training/courses/incident_response/#learn-how-to-effectively-manage-incidents","text":"002. Learn how to effectively manage incidents. The goal of this session is to give you an understanding of how to effectively manage incidents within your organization. I'll describe the process we use at PagerDuty for managing critical incidents, and talk in more detail about a specific role called the \"Incident Commander\". This isn't a sales pitch. I'm not on our sales team, and this isn't a talk about how to use PagerDuty to manage your incidents (although obviously it would be awesome if you did). The intent today is to introduce you to how we manage incidents internally at PagerDuty, and provide you with lots of practical information you can take away to your own organizations to either start or improve your own incident response processes.","title":"Learn How to Effectively Manage Incidents"},{"location":"training/courses/incident_response/#replace-chaos-with-calm","text":"003. Replace chaos with calm. Let's start with a quick question. How does incident response usually go in your organization today? Is it a smooth and streamlined process, or is it a lot of people talking over one another? For most of you it's probably going to be somewhere in the middle. We want less of the latter, and more of the former. We want to replace chaos with calm. Panic and chaos are not good during an incident, they only exacerbate things and causes more confusion. We want things to be calm and organized.","title":"Replace Chaos with Calm"},{"location":"training/courses/incident_response/#what-is-incident-response","text":"004. What is incident response? Docs Reference So when we talk about incident response, what we're really talking about is an organized approach to addressing and managing an incident. This is how we define incident response at PagerDuty. They key here is on the word organized . We don't want to be running around in a panic anytime an alert goes off. We want our response to be almost routine, and for everyone to work together like a well-oiled machine. There's a quote I really like from an excellent book called Incident Management for Operations that's appropriate here, Fire is not an emergency to the fire department . . . [Y]ou expect a rapid response from a group of professionals, skilled in the art of solving whatever issues you are having.","title":"What is Incident Response?"},{"location":"training/courses/incident_response/#goal-of-incident-response","text":"005. Goal of incident response. Docs Reference It may surprise you to learn the goal of incident response isn\u2019t just about solving the problem. Give a thousand monkeys a keyboard and enough time and they can probably solve your problem. That's not good incident response. We want to solve the problem in a way which limits the damage caused, and reduces the recovery time and costs. I don\u2019t just mean financial cost either, there\u2019s a cost associated with engineer health too. Constantly waking people up at 3am can have a dramatic negative effect on their health and happiness. If financial impact is all you care about though, let's not forget that people are expensive . In a large organization, a phone bridge with 100 people sitting there mostly idle for several hours is not unheard of. If each of those people cost ~$100/hour, that\u2019s $10K every hour! That\u2019s really expensive to the business.","title":"Goal of Incident Response"},{"location":"training/courses/incident_response/#what-is-an-incident","text":"006. What is an incident? Docs Reference Before we can respond to an incident though, we need to define what an incident actually is . It sounds silly, but if you\u2019re not sure whether something\u2019s an incident, you don\u2019t know whether to respond to it. Here is PagerDuty\u2019s definition of an incident, An unplanned disruption or degradation of service that is actively affecting customers' ability to use the product. \"Customers\" here doesn't just refer to external customers, but can refer to internal customers too. Your definition might be different, and that\u2019s OK. I just wanted to give you an idea of the kind of definition that can get you started. You want your definition to be simple, no more than a sentence, and easily understood by anyone. You may notice that this is quite a broad definition though. A typo technically fits this description. As does a full outage. Obviously these are very different scenarios. So we do have something else too.","title":"What is an Incident?"},{"location":"training/courses/incident_response/#what-is-a-major-incident","text":"007. What is a major incident? Docs Reference We also have something we call a major incident . This is any incident where we require a coordinated response between teams. Again, this is just our definition at PagerDuty, feel free to use your own. The intention behind this definition is that sometimes incidents can be handled by a single team, maybe the owners of a service that's having trouble. That rarely requires a large response in and of itself. But as soon as they need to involve another team, whether it's customer support, or database administrators, then we declare it to be a major incident and kick off a much larger response. The coordination is key here, and we\u2019ll talk more about this later. But this still covers quite a range of potential incidents. We can get more granular.","title":"What is a Major Incident?"},{"location":"training/courses/incident_response/#severity-levels","text":"008. Severity levels. Docs Reference We also use severity levels to determine how severe an incident is, and what type of response it gets. We use SEV-5 through SEV-1 for our levels, but you may use a different scheme, P0 through P5 , or maybe even emoji, \ud83d\udd25 through \ud83d\udca9, etc. Let's imagine you're looking at a graph of traffic to your website. You can typically determine severity based on how drastically your metrics are affected. So as your website traffic drops, the severity increases. You will usually reach a point where you've set some predefined target or watermark, where as soon as the metric passes, you automatically consider something a major incident. At PagerDuty it's the difference between a SEV-3 and SEV-2 , but it may be different for your organization. Then as things get even more dire, we get into our SEV-2 's and our SEV-1 's when things completely flatline. Having pre-defined thresholds and metrics can allow you to have automatic triggers for your response process. We recommend using metrics tied to business impact. Metrics can be very useful, and often work best when they're tied to business impact . For example, a metric we monitor at PagerDuty is \"number of outbound notifications per second\", at Amazon it could be \"number of orders per second\", at Netflix it might be \"stream starts per second\", etc. Monitoring these important business metrics will then let you use automation to determine the severity of an incident and the type of response you use. If you use metrics that aren't tied to business impact (e.g. \"CPU usage is high on a host\"), then it's difficult, and sometimes impossible, to determine the severity of an incident associated with that metric. You want to use a metric that lets you know how your business is doing, not how a particular piece of equipment is doing.","title":"Severity Levels"},{"location":"training/courses/incident_response/#anyone-can-trigger-incident-response","text":"009. Anyone can trigger incident response at any time. But sometimes you won't know the impact straight away. Or maybe your metric hasn't reached the predefined threshold yet. We still need a way for a human to jump in and call something a major incident. So even though we have automation at PagerDuty, we also have a mechanism whereby anyone can trigger our major incident response process at any time. This is very important for us. We've found that lowering the barrier to triggering incident response has lead to a dramatic increase in the speed with which incidents are resolved . We don't want people to sit on something because the official alarm hasn't gone off yet. If customer support gets lots of requests very quickly, it's a good sign there's something wrong, and we need them to be able to raise the alarm. We've even had interns trigger our incident response process in their first week. If the janitor walks past a graph and thinks it looks wrong, I want them to be able to trigger incident response. We were initially hesitant to introduce this, as we feared it would lead to lots of false positives. People triggering the alarm in an abundance of caution and it not really being an incident. But we've seen quite the opposite, people are pretty good at policing this themselves. Only twice have we had it be a false alarm, and both times it was warranted based on the information available at the time. Even if you do get a false positive now and then, you can use it as free practice.","title":"Anyone Can Trigger Incident Response"},{"location":"training/courses/incident_response/#triggering-incidents-via-chat","text":"010. Triggering incidents via chat. Docs Reference So how do we let humans trigger the process? We do it with a chat command , but don\u2019t feel like that\u2019s the only right way. I just wanted to demonstrate how we do it to give you an idea. You can do it however your want. Air horn, flashing light in the office, hire a mariachi band, etc. The point is you want a way to trigger your response that's fast, easy, and available to everyone.","title":"Triggering Incidents via Chat"},{"location":"training/courses/incident_response/#peacetime-vs-wartime","text":"011. Peacetime vs Wartime. Docs Reference Once an incident is triggered, we need to switch our mode of thinking. We need a mentality shift . We want a distinction between \u201cnormal operations\u201d and \u201cthere\u2019s an incident in progress\u201d. We need to switch decision making from peacetime to wartime. From day-to-day operations, to defending the business. Something that would be considered completely unacceptable during normal operations, such as deploying code without running any tests, might be perfectly acceptable during a major incident when you need to restore service quickly. The way you operate, your role hierarchy, and the level of risk you\u2019re willing to take will all change as we make this shift.","title":"Peacetime vs Wartime"},{"location":"training/courses/incident_response/#normal-vs-emergency","text":"012. Normal vs Emergency. Docs Reference Some people don\u2019t like the peacetime/wartime analogy, so you can call it what you want. Normal/Emergency.","title":"Normal vs Emergency"},{"location":"training/courses/incident_response/#ok-vs-not-ok","text":"013. OK vs Not OK. Docs Reference Or OK/NOT OK. What you call it isn't as important as being able to make the mental shift.","title":"OK vs Not OK"},{"location":"training/courses/incident_response/#incident-command-system","text":"014. Incident Command System (ICS). Docs Reference So let\u2019s talk about our process a bit more. The way we do incident response at PagerDuty isn\u2019t something we invented ourselves, it is heavily based on the Incident Command System , usually abbreviated to ICS. ICS was developed after some devastating wildfires in Southern California in 1970. Thousands of firefighters responded, but found it difficult to work together. They knew how to fight fires individually, but lacked a common framework to work effectively as a larger group. After the fires, an interagency group called FIRESCOPE (Which believe it or not is an acronym for \"FIrefighting REsources of Southern California Organized for Potential Emergencies\") was formed and set out to develop two systems for managing wildland fire. One of those systems became known as ICS, and eventually became a national model for command structures at any major incident. In 2004, the National Incident Management System (NIMS) was established by FEMA, and is now used as the standard for emergency management by all public agencies in the United States. NIMS defines several operational systems as part of it, of which ICS is one. It\u2019s used by everyone from the local fire department responding to a house fire, to the US government responding to a natural disaster. It provides a standardized response framework that everyone is familiar with. NIMS and ICS are the basis of the process we use at PagerDuty, however we have heavily modified it for our needs. Turns out that things can be streamlined a bit when human life isn't on the line.","title":"Incident Command System"},{"location":"training/courses/incident_response/#incident-response-around-the-world","text":"015. Incident response around the world. Docs Reference It's worth noting that even though our process is based on the US systems, NIMS and ICS, there are many similar systems in use all over the world. While many are also based on ICS, some were developed separately, yet offer many of the same features. I particularly like the UK system , simply because it has a role called the \"Gold Commander\", which just sounds like a Bond villain. When developing our process at PagerDuty, we looked at a few of the other systems in use around the world, and chose the bits we liked the most to add to our own. Emergency Management Around the World If you're interested in learning more about the systems in use by other countries, we have links to some official resources . There's also a book available from the US FEMA website, called \" Comparative Emergency Management: Understanding Disaster Policies, Organizations, and Initiatives from Around the World \" where it compares the systems used by about 30 different countries.","title":"Incident Response Around The World"},{"location":"training/courses/incident_response/#roles","text":"016. Roles of incident response. So let's talk about the roles involved in our process . I'm going to introduce the roles one by one, but I don't want you to get scared by the number that will be on the slide by the end. We didn't start with this many roles, and we don't have people filling all the roles in every incident. This is just showing the available roles, and defining what they are. The process and roles will grow and shrink to fit the size of the incident at hand. While we don\u2019t use exactly the same roles as ICS, we picked out the ones that matter for us in order to get our role structure. First up is the Incident Commander , usually abbreviated to IC. They're the person in charge and the most important role in the process. They make all the decisions, and all information flows up to them. What they say goes. I'm going to be talking more about this role in a moment, so we'll continue on with the others for now. Next up we have the Deputy . This is basically a backup Incident Commander. They're training as an IC and will be listening to all the same information. They help to make sure nothing gets missed by the IC, and acts as a hot-swap standby should the IC want to handover command. This is one of those roles you won't need if you're just starting out. For a long time our Deputy and Scribe would be the same person. Then we have the Scribe . The scribe's job is to keep an accurate timeline of events. What has happened, when it happened, and the key decisions that have been made. We use Slack for this, and the Scribe will be writing down notes into a Slack room, which gets us nicely timestamped data. It's worth noting that they're not doing a direct dictation of a voice call. It's not \"John said this, Mary said this\". It's more like \"We're deciding between A and B, we've decided on A\". Together, these roles are called the Command Staff. Next we have the Customer Liaison . This is a member of our support team, and their job is to handle the two-way interaction with our customers. So they'll update customers as to what is going on, whether that's via email, tweet, or updating our status page. But they'll also let us know what customers are saying too. If we're getting 100's of support requests, or no one has raised a ticket at all. Since this can be useful information in tracking down a cause, and determining the level of risk we can take during our recovery. The Internal Liaison is a relatively new role in our process. Their job is to handle all the interaction with internal teams, such as our executives, or our marketing teams, and so on. We have a separate Slack channel for incident updates, to which the Internal Liaison will post regular status updates, and answer any questions from the rest of the organization. This keeps those questions out of our main response, but allows people to still get answers. The internal liaison will also page other teams as necessary if they're needed on the response. Again, this isn't a role you'll need for most companies, for a while this was also handled by our Deputy/Scribe role. Together, these are the liaisons. Finally, we have the Subject Matter Experts , or SME's. These are the people who will actually be fixing the problem. They'll be the ones logging into servers, changing code, and running commands. Today, I\u2019m going to focus on one role in particular, that of the Incident Commander.","title":"Roles"},{"location":"training/courses/incident_response/#incident-commander","text":"017. Incident commander. Docs Reference The incident commander is one of the most important roles you can have. Even if you don\u2019t have deputy, scribe, customer liaison, etc. The Incident Commander is one you should get first (well, after the SME\u2019s of course, you probably need someone to solve your problem before you need someone to coordinate the response).","title":"Incident Commander"},{"location":"training/courses/incident_response/#single-source-of-truth","text":"018. Single source of truth. They\u2019re the single source of truth during an incident, and are the ones in charge. The big cheese. They make all decisions, and no action should be performed unless the IC has said so.","title":"Single Source of Truth"},{"location":"training/courses/incident_response/#highest-ranking-person","text":"019. Highest ranking person. No matter their day-to-day role, and IC is always becomes the highest ranking person on the response. They're running the show. Remember what we talked about earlier with the mentality shift? Well, this is one of the things that will change during an incident. Is the Incident Commander a dictator? Only in the sense that they are in charge and their decisions cannot be overruled. They do not rule by force, or go on power trips and order people around for the sake of it. A good Incident Commander will listen to their experts and make the best decision they can based on the information available. Incident Commanders should show empathy towards all responders.","title":"Highest Ranking Person"},{"location":"training/courses/incident_response/#higher-than-the-ceo","text":"020. Higher than the CEO. Even if the CEO joins the response, the IC still outranks them in an incident response situation. This is absolutely critical for successful incident response, but it does require buy-in from your executives. Please don't surprise your CEO with this, it will not go well for you. Whether this works for you will depend on your organization. This is how we do it at PagerDuty and it works well for us, but I can imagine it not being easy to get this sort of buy-in in other organizations. I used to work in the airline industry, and I don't think this rule would fly there (Get it? Airline industry. Fly. Hello? This thing on?).","title":"Higher than the CEO"},{"location":"training/courses/incident_response/#not-a-resolver","text":"021. Not a resolver. Importantly, the IC doesn't resolve the incident, they coordinate and delegate all tasks. They're the conductor of the orchestra, they're not playing an instrument. They're not acting as a resolver, and shouldn't be looking at graphs, or logging into servers. This can be especially hard sometimes if the IC is an engineer in their day-to-day role, as they may naturally want to jump in to try and help, but that urge must be resisted if they're acting as an IC. If they absolutely are the only person who can solve the problem, then they should handover to another Incident Commander and assume the role of a Subject Matter Expert instead. We'll talk a bit more about this scenario later.","title":"Not a Resolver"},{"location":"training/courses/incident_response/#bsod","text":"022. Uh oh! Oh dear, that\u2019s not good. Did something go wrong? Seems we have ourselves an incident! So what do you do when things break in the middle of the night? What's the first step to responding to an incident? It turns out that the first step in any incident response is always the same, whether you're a small startup or a large enterprise. Whether there's 25 cents on the line, or 25 billion dollars.","title":"BSOD"},{"location":"training/courses/incident_response/#dont-panic","text":"023. Don't panic. Don\u2019t panic. It elevates stress, and causes others to panic. It\u2019ll end up hurting your incident response a lot more. It's OK to panic on the inside. We're only human after all. It's a natural reaction to panic in these sorts of situations a little bit. Everything about getting paged is designed to get adrenaline flowing. Loud pager sounds and so on. Just try not to outwardly show panic, because it will cause others to do the same. Act calm, and others will follow suit. Those with experience will stay calm, and that can make the difference between a chaotic incident, and one that resolves smoothly. So don\u2019t panic!","title":"Don't Panic"},{"location":"training/courses/incident_response/#ic-introduction","text":"024. \"Hi, I'm Rich, I'm the Incident Commander\". Docs Reference But today we're lucky to have an IC here. Hi, I'm Rich, I'm the Incident Commander. Every call should start like this. Well, use your own name and not mine, but you get the idea. There are a few important things here with the way I phrased this. What if you join a call and there's no IC? It doesn't always work out that the first person to join the call is the incident commander. So what do you do in that case? Some guides will recommend the first person who joins acts as the Incident Commander, regardless of training. ICS usually works this way, where the first person on-site acts as the IC until someone more qualified arrives. We tried that and found it just doesn't work in practice. For example, let's say that the first person to join your response call becomes the acting IC. After an incident occurs, how long do you think it will be before one of your responders joins the call? Yeah, turns out it's about 10 minutes. Because people are afraid of being the IC. You can't force someone to be an IC, and we prefer to make sure the acting IC at least has the training. It may add a few minutes to the start of an incident, but it makes it go smoother and quicker overall. If a trained IC joins the call and there's isn't an existing IC, they will take on the role. They ask \"Is there an IC on the call?.... hearing nothing, this is X, I'm the Incident Commander\". It's also worth noting that the on-call IC doesn't automatically take over when they join. Whoever is the active IC on the call is in charge until they perform a handover.","title":"IC Introduction"},{"location":"training/courses/incident_response/#introduce-yourself","text":"025. Introduce yourself. Firstly, I introduced myself by name. I didn't just say \"Hi, I'm the IC\". I'm not an emotionless robot. Introductions are important so that people know who you are. They\u2019ll usually be referring to you as \"IC\", rather than by name, but it still humanizes you more. If you introduce yourself by name people will treat you differently and it'll help to make things go a little bit smoother. You\u2019ll find soon that a lot of the job of an IC involves psychology and phrasing more than technical expertise.","title":"Introduce Yourself"},{"location":"training/courses/incident_response/#say-incident-commander","text":"026. Say \"Incident Commander\". Then I said that I'm the \"Incident Commander\". I didn't abbreviate to IC, since new people might not understand the lingo yet. Stating it that way made it very clear. We want to make sure we always use clear language . Additionally, saying the word \"Commander\" here will subconsciously instill in people that you're in charge.","title":"Say Incident Commander"},{"location":"training/courses/incident_response/#good-communication","text":"027. Good communication is essential. Good communication is essential. A breakdown in communication can hamper the entire response process. One of your jobs as an IC is to keep the lines of communication clear and maintain discipline. Don\u2019t throw weird or unfamiliar acronyms into the discussion.","title":"Good Communication"},{"location":"training/courses/incident_response/#acronym-overload","text":"028. Acronym overload. Too many acronyms and internal lingo will upset newcomers and adds cognitive overhead. As shown in this completely realistic example. It takes much longer to say \"Let's get the Incident Commander on the response call, then get a bacon, lettuce, and tomato sandwich for all the Subject Matter Experts\". But it's much clearer what is being asked for.","title":"Acronym Overload"},{"location":"training/courses/incident_response/#clear-is-better-than-concise","text":"029. Clear is better than concise. Docs Reference Clear instructions are more important than concise instructions. You want to favor explicit and clear communication over all else . If you have the choice between taking 5 seconds and abbreviating, or taking 30 seconds and making it clear, take 30 seconds. Don\u2019t give a long essay, but make sure the instructions are unambiguous. There's a tendency to want to rush through an incident, that every second counts, and that you need to use abbreviations to get the fastest possible response. Unfortunately you'll find that using unfamiliar and unclear language will almost always prolong an incident.","title":"Clear is Better Than Concise"},{"location":"training/courses/incident_response/#whats-wrong","text":"030. What's wrong? Docs Reference OK, so how do we actually start solving our problem? The first step is to collect information from your Subject Matter Experts (SME) for their services/area of ownership. Ask what's wrong, and gather the symptoms of the incident. Is it only affecting one system? Is it affecting everything? Was there a specific metric that triggered an alarm? We call this \u201c sizing up \u201d. We're trying to get an idea of the scope of the incident.","title":"What's Wrong?"},{"location":"training/courses/incident_response/#what-actions-can-we-take","text":"031. What actions can we take? Docs Reference Next we want to ask our experts what they want to do to fix their systems. Remember, the IC isn't coming up with solutions, we want to ask the people who are the experts for their services what they want to do to. They will have a much better idea of the actions that can be taken. We want to collect proposed repair actions.","title":"What Actions Can We Take?"},{"location":"training/courses/incident_response/#what-risks-are-involved","text":"032. What are the risks involved? Importantly, we also want to make sure we ask what risks are involved with the proposed actions. \"What impact will that have?\", \"What are the risks involved?\", \"How confident are you it will work?\". Without this information, we can't make an informed decision. For example, if we need to restart our servers to fix a problem, we could either reboot them all at once and be done in 30 seconds, or we could do a rolling restart and take 10 minutes. On the surface of it, the first option sounds like the best. But, if we ask for the risks involved, we'll learn that the first option will cause downtime for everyone in those 30s, whereas the second option, while slower, will result in no downtime for end users. This can change the decision you make, so it's important to get the information.","title":"What Risks Are Involved?"},{"location":"training/courses/incident_response/#make-a-decision","text":"033. Make a decision. Once we have a collection of actions and their associated risks, it's time to make a decision. Sometimes there's an obvious path forward, with one option being clearly better. But sometimes you're presented with two equally bad options. There\u2019s no golden rule here I can give you, it\u2019ll be up to context and your company culture. But my advice if you can't decide between two options is to literally flip a coin. Making the wrong decision is better than making no decision. Making no decision doesn't help to make forward progress, you learn nothing new and the incident is still going on. Making a decision, even if it's the \"wrong\" one will give you more information. If it turns out to be wrong, you can then put all your resources into the other option. A wrong decision gives you more useful information, making no decision gives you nothing. You want to avoid decision paralysis at all costs, as it can prolong your incident further. 'Do nothing' is an acceptable decision. I should note that the above advice is intended for the situation when you can't decide between two options. \"Do nothing\" is a perfectly acceptable decision if that's the course of action you want to take. It is sometimes appropriate to get more information by waiting and seeing what changes.","title":"Make a Decision"},{"location":"training/courses/incident_response/#gain-consensus","text":"034. Gain consensus. Once we've made a decision, we need to gain consensus for our plan. But wait, why? Didn't I say earlier that the IC is basically a dictator and everyone should follow their instructions? While technically true, we want to be sure we give a chance to listen to any potential problems our experts may have with the plan. We don't want people to come back later and say things like \"I knew that wouldn't work\". We want to make sure we stop the hindsight 20/20 problem . It demotivates responders, and wastes time. But gaining consensus amongst a large group of people can be a bit difficult.","title":"Gain Consensus"},{"location":"training/courses/incident_response/#blue-background","text":"035. This background is blue. Let's look at a quick example to show what I mean, I propose that this background is blue. Does everyone agree? Audience participation! This is where I would usually get few people from the audience nodding or quietly saying \"Yes\". I'll point to about 5 or 6 people who did nothing and ask them one by one if they agree. Until it gets uncomfortable. See how long it\u2019s taking us to reach consensus? Distributed consensus is hard , you\u2019ll be there forever trying to agree on the proposed actions. Let's try it a different way, I propose that this background is blue. Are there any strong objections? ... Hearing none, the background is blue, let's proceed.","title":"Blue Background"},{"location":"training/courses/incident_response/#any-strong-objections","text":"036. Are there any strong objections? Docs Reference See how much faster that was? I implicitly got the consensus of everyone in the room , so none of you could come back later and say you didn't think the background was blue, because I gave you all a chance to object. Doing it this way optimizes for the 99% case. Most of the time there won\u2019t be any objections and you can just continue. But likewise, we don't really care if people agree with us. We care if people disagree , that's the information we need the most. If you ask everyone to agree, you may get one person saying \"No\" with a really important point, but you can't hear them because everyone else is saying \"Yes\".","title":"Any Strong Objections?"},{"location":"training/courses/incident_response/#strong","text":"037. STRONG objections. This is one of the most useful phrases in your toolkit as an IC as it allows you to get consensus on a decision very quickly, and prevents the hindsight problem from popping up later. The way we phrased it though is important, the word \" strong \" subconsciously instills in people that we're still in an incident situation, and normal concerns might not apply. Make sure to leave enough of a pause for people to raise any objections they may have. It's no good asking for strong objections then moving on. All responders need to have the opportunity to raise a concern. So now that we have consensus, we need to execute the plan, that means assigning the task to someone. Before we do that though, I have a quick question.","title":"Strong"},{"location":"training/courses/incident_response/#how-long-have-i-been-talking","text":"038. How long have I been talking? At the start, I asked if someone could keep track of the time for me. Did anyone actually do that? Can anyone tell me exactly how long I've been talking? Probably not. But why not? I clearly asked. It's likely because of how I phrased the question.","title":"How Long Have I Been Talking?"},{"location":"training/courses/incident_response/#bystander-effect","text":"039. Bystander effect. I said \"Can someone...\". This is called the bystander effect . Everyone assumed someone else was doing it, so no one ended up doing it. If by some chance, someone actually did do it, you won't know who it is anyway, or if they've even started. A good example of this is if there's a medical emergency, and you shout \"Somebody call 911!\", you'll find that no one does, because everyone assumes someone else is doing it. If you're ever in that situation, you want to point to someone and say \"You, call 911\". Then it'll get done. Localization If you're outside the US, substitute 911 for whatever your local emergency number is. You probably don't want to be shouting \"You, call 911\" if you're in Europe for example. So what I should've done was point to someone in the room and say, You, please keep track of the time and give me a little wave when we get to 30 minutes, starting now. Understood? See how different that was?","title":"Bystander Effect"},{"location":"training/courses/incident_response/#assigning-tasks","text":"040. Assigning tasks. Docs Reference In the context of an incident, that might look like this. It's a little more verbose than \"Can someone investigate the cause?\", but it's a lot clearer what I want to happen. Brings us back to clear is better than concise from earlier. Several important things happened in this exchange as I was assigning the task.","title":"Assigning Tasks"},{"location":"training/courses/incident_response/#assign-specific-people","text":"041. Assign tasks to a specific person. First, the task was assigned directly to a specific person . It\u2019s ok to assign it to a role to \u201cDBA on-call\u2026\u201d, etc. But it must be a single individual. Don't assign things to a group, because they won't get done.","title":"Assign Specific People"},{"location":"training/courses/incident_response/#time-limit","text":"042. Time-box all tasks. Second, the task was given a time-limit. This means the SME knows exactly how long until I come back to them for an answer, so they won\u2019t be surprised or caught off guard. It sets the expectations.","title":"Time Limit"},{"location":"training/courses/incident_response/#acknowledgement","text":"043. Get acknowledgement. Finally, I confirmed that they had understood the instructions and are going to carry them out. So I don\u2019t come back in 5 minutes and find they never started, or have additional questions.","title":"Acknowledgement"},{"location":"training/courses/incident_response/#followup","text":"044. Followup. Then after the time is up, we can simply ask for the results of their task. Of course they will always have the right answer the first time and will never need any additional time to investigate, right?","title":"Followup"},{"location":"training/courses/incident_response/#need-more-time","text":"045. What if they need more time? It's not always going to be the case that things get done within the timeframe. So what do you do if, after 5 minutes, they need more time? Don\u2019t just give them another arbitrary time limit, because they\u2019ll keep coming back and you\u2019re just going to be wasting time giving them 5 minutes chunks for an hour. Instead, ask your experts how long they need.","title":"Need More Time?"},{"location":"training/courses/incident_response/#more-time","text":"046. More time. This isn't going to be like in the movies, where you ask how long someone needs, they say two hours and you slam you fist on a table and say \"You've got one!\". You need to trust in your experts to give you accurate estimates, and give them the time they need. Putting people under unreasonable pressure is only going to lead to mistakes being made. You can do this the first time too. You can also just ask your experts how long they need the first time you hand out the task instead of picking a time yourself. Sometimes we've found it easier to give a time-limit ourselves if it's an action that's been done before and we have a rough idea of how long it should take. But there's nothing wrong with asking how long someone needs as you assign the task either.","title":"More Time"},{"location":"training/courses/incident_response/#solving-incidents-1","text":"047. Flow chart for solving incidents. And we just keep following this pattern until the incident is resolved. Ask for a status from your experts. Decide on an action based on what you\u2019re told, gaining consensus for the plan. Assign the task out. Follow up with the tasks once done, and repeat if there are still problems.","title":"Solving Incidents (1)"},{"location":"training/courses/incident_response/#solving-incidents-2","text":"048. Another flow chart for solving incidents.. More generally, we\u2019re following this cycle for each incident. We size-up the situation, stabilize things (that's the loop we just showed), keep everyone updated as to what's going on, then verify the situation is fixed before ending the response. If it's not fixed, we start again.","title":"Solving Incidents (2)"},{"location":"training/courses/incident_response/#ignore-the-ic","text":"049. \"Ignore the IC, do what I say!\" Uh oh, an executive has joined the response and is trying to override the IC\u2019s decisions. That\u2019s not great. Remember that the IC is the ultimate authority in incident response, their decisions are the ones that matter. So how do you handle the awkward situation when someone tries to override those decisions? We have a top tip here, a great question to keep in your tool belt as an incident commander. A simple question that can immediately diffuse this situation.","title":"Ignore the IC"},{"location":"training/courses/incident_response/#do-you-wish-to-take-command","text":"050. Do you wish to take command? Docs Reference Do you wish to take command? Watch how quickly they don\u2019t answer with \u201cyes\u201d. If they do, great! You\u2019re off the hook and can respond with, Understood. Everyone on the call be advised, I'm handing over command to executive A. They are now the Incident Commander. But most of the time they\u2019ll say either say \"No\"\", or not answer at all, in which case you can continue on as normal, perhaps saying, In that case, please cause no further interruptions or I will have to remove you from the call. But \"Do you wish to take command?\" is the most useful phrase for dealing with that kind of executive hostile takeover.","title":"Do You Wish To Take Command?"},{"location":"training/courses/incident_response/#executive-swoop","text":"051. Executive swoop. This is a class of problem we call Executive Swoop. Well, actually it's \"Executive Swoop and Poop\", but I was asked not to put that on the slide. The previous example isn't typical though. It's rare for someone to come in and purposefully start causing a problem like that. We're going to look at some more common examples of executive swoop next, but it's worth noting that none of these happen maliciously . No executive joins with the intent of hindering the process, they\u2019re trying to motivate people and find out what\u2019s going on. It\u2019s their business too! Making sure your executives understand why these things are a problem is important, so be sure to followup after an incident if these things happen.","title":"Executive Swoop"},{"location":"training/courses/incident_response/#lets-resolve-this-in-10-minutes","text":"052. Let's try and resolve this in 10 minutes. Let's try and resolve this in 10 minutes please! This one definitely wasn't said by anyone at PagerDuty ever. Nope. Definitely not. On the surface this seems pretty benign. The executive is merely trying to motivate staff and encourage them to solve the problem quickly, right? Unfortunately, that's not how others on the call are going to interpret it. Instead they're more likely to be sarcastically thinking \"Well, I was going to take an hour, but since you've said that, OK I'll do it in 10 minutes\". It assumes people aren\u2019t already working as hard as possible to solve the problem. It demotivates responders, and adds additional stress. Your job as IC is to nip this in the bud and keep things on track.","title":"Let's Resolve This in 10 Minutes"},{"location":"training/courses/incident_response/#keep-comments-until-the-end","text":"053. Keep your comments until the end. Docs Reference To keep things moving, you need to remind the executive of what's going on, and direct questions to be handled at a later time. It may come across slightly abrupt, but it gets the point across quickly and allows you to keep moving. Most people will pick up on the subtext here. We're in the middle of an incident, please keep your comments until the end. Remember, don't be mean , just state the facts and keep things flowing. We're not trying to shame people or make them feel bad, we're trying to keep our incident moving towards a resolution.","title":"Keep Comments Until The End"},{"location":"training/courses/incident_response/#spreadsheet-of-affected-customers","text":"054. Can I get a spreadsheet of affected customers? Can I get a spreadsheet of all affected customers? Another one that's definitely never been mentioned on any PagerDuty incident response call ever. An exec joins the call and wants to get a list of impacted customers. The problem is that in order to find that out, we'll need to take someone away from the effort of responding to the incident, at a time when we need them most. If you can spare the resources, then feel free to dedicate some to finding the information. But more often than not you wont have the resources to spare. So we can just tell the executive that.","title":"Spreadsheet of Affected Customers"},{"location":"training/courses/incident_response/#the-incident-takes-priority","text":"055. The incident takes priority. Docs Reference We can either get you that list, or fix the incident. Not both. The incident takes priority. Note how this isn't phrased as a question, it wasn't \"We can either get you that list or fix the incident, which do you want?\". The incident commander has already made the decision , they're simply letting the executive know what it is. Remember that the IC is still in charge, you don't want to cede decision making to someone else during an incident.","title":"The Incident Takes Priority"},{"location":"training/courses/incident_response/#really-a-sev-1","text":"056. Is this really a SEV-1? Is this really a SEV-1? Oh look, it's another one that's never ever happened at PagerDuty. We used to have a really big problem with this one. We'd start incident response calls, then spend the first 10 minutes arguing over whether it was a SEV-3 or a SEV-2 . By the time we were finished, we would be 10 minutes into a SEV-1 and have made no progress. So now we have a rule. We don't discuss incident severity during the call. If we can't decide between two, we always assume it's the higher severity and move on. Don't litigate severities during an incident. It's a waste of time.","title":"Really a SEV-1?"},{"location":"training/courses/incident_response/#treating-as-a-sev-1","text":"057. We're treating this as a SEV-1. Docs Reference So the IC will need to make it clear we don\u2019t discuss, and that we\u2019re treating it as a SEV-1 . It may turn out to be a SEV-4 , who knows, it doesn\u2019t matter. That's a discussion for the post-mortem. We do not discuss incident severity during the call. We're treating this as a SEV-1. Once you\u2019ve spun up the gears of incident response, you may as well finish the process, if anything it just gives you all more practice.","title":"Treating as a SEV-1"},{"location":"training/courses/incident_response/#notify-stakeholders","text":"058. Notify stakeholders. Pretty much all of these examples of executive swoop can be pre-empted by involving stakeholders in the process , giving them a way to stay up to date. At PagerDuty we have a separate Slack room just for incident updates. It's less noisy than our main response room, and gives succinct updates for folks who want it. This allows execs to stay in the loop, and also ask questions without affecting the main response. In our process, the Internal Liaison is responsible for monitoring and updating that channel. Stakeholders are not allowed to talk on our response call, or in our main incident response chat room. They must take all discussion to the updates room or via the Internal Liaison. The Incident Commander is responsible for keeping our primary communication channels free of those types of discussions or questions, and direct people towards the Internal Liaison.","title":"Notify Stakeholders"},{"location":"training/courses/incident_response/#belligerent-responder","text":"059. The belligerent responder. There are other things than can hinder your response though that don't fall under the category of executive swoop. This one we call the belligerent responder. It was originally called The Drunk Engineer , but again, I was asked not to put that in the slide. There are plenty of awkward situations that can present themselves on response calls. There can be big ego\u2019s and strong opinions. What do you do when someone on the call is being belligerent and hampering the response process? You need to be firm, and let them know what will happen if they continue.","title":"Belligerent Responder"},{"location":"training/courses/incident_response/#disruptive","text":"060. You're being disruptive. Docs Reference You're being disruptive. Please stop, or I will have to remove you from the call. State the facts, give them a way out to save face , but state what will happen if they don\u2019t. No second chances, follow through on the action if they don\u2019t respond. It can be harsh, but that\u2019s what needs to be done. Again, we've phrased this in a particular way. We said \"I will have to remove you\", rather than \"I will remove you\". It makes it seem like the decision is out of your hands and that you'll be forced to do it. It can make it less personal and less likely to cause awkward problems after the call is over.","title":"Disruptive"},{"location":"training/courses/incident_response/#do-responders-get-tired","text":"061. Do responders get tired? Another problem that can pop up during a response is when we have long running incidents. Do we use the same IC for a 12 hour incident? Do IC's even get tired? Well, of course they do! They're people too. This is another human cost associated with incident response that you want to try and minimize. All of the roles in the response process can be mentally fatiguing. When you get tired you start to forget things and make mistakes, so it's important to try and keep a fresh perspective as often as you can.","title":"Do Responders Get Tired?"},{"location":"training/courses/incident_response/#handovers-are-encouraged","text":"062. Handovers are encouraged. For this reason, we actively encourage handovers in our process. Usually every hour or so is what we recommend, but it's at the discretion of the people involved. 3 hours would be the absolute upper limit where we would start requiring a handover. Make sure to rest after handing over. It can be very tempting after handing over command to want to stay on the call and listen in, to try and stay on top of things and see how things are going. Avoid this at all costs. Once handing over your role to someone else, you should leave the response call and all associated chat rooms. Take a break away from anything related to the incident. You may be required to take the role again later if the incident is very long running. If you've stayed listening in the entire time, then while you would be up to speed, you would still be fatigued and would not be able to respond as efficient as if you'd taken a real break. Handing over command is important, and really easy. Get the new IC up to speed out-of-band from the main discussion. We privately message on Slack for example. If you have a deputy, then it's even better, because they would already be on the call and up to speed. Then you handover.","title":"Handovers Are Encouraged"},{"location":"training/courses/incident_response/#handover","text":"063. Handover. Docs Reference State that you are handing over command, and then the new IC begins as if it were a new call. Simple! Since we want to be able to handover, it's important to have as many trained IC's as you can. The more you have, the better. Ideally you want enough for at least a daily rotation. How do you get lots of trained Incident Commands? This is a question we get a lot, and not something we have a golden solution for. Many people are hesitant to take on the responsibilities of being an IC in addition to their current work and on-call responsibilities, which is a perfectly valid concern. One of the best ways we found to increase the pool of Incident Commanders is to encourage folks outside of normal engineering teams to take on the role. Those who wouldn't normally be part of an on-call rotation. Not only can they provide an outside perspective during incidents that is sometimes missing, but it can also help to build further empathy with others in the organization who regularly go on-call.","title":"Handover"},{"location":"training/courses/incident_response/#anti-patterns","text":"064. Anti-Patterns. Docs Reference Let\u2019s talk about some anti-patterns . Things which seem like they would help incident response, but really don\u2019t. Knowing these now will save you the headaches and growing pains we went through.","title":"Anti-Patterns"},{"location":"training/courses/incident_response/#getting-everyone-on-the-call","text":"065. Getting everyone on the call. Docs Reference Believe it or not, we used to page every single engineer at PagerDuty whenever we had a SEV-2 . I'm not joking. It was horrible. It worked great when we only had 5 engineers, less so when we had 50. It's important to maintain an effective span of control during an incident. No one person should have more than ~7 people reporting to them. Any more than that and you have too many cooks in the kitchen. Waking up 30 engineers at 3am causes untold damage. Please don't do it.","title":"Getting Everyone on the Call"},{"location":"training/courses/incident_response/#not-letting-responders-leave","text":"066. Not letting responders leave. Docs Reference Remember I mentioned at the beginning that one of our goals is to reduce the cost associated with an incident? That includes the human cost. Waking people up at 3am is costly. But keeping those people on a call in which they can\u2019t do anything is even worse. So if some responders are no longer needed, let them leave the call. Your co-workers time is more costly than servers, don\u2019t burn them out! I would recommend you not list everyone you want to leave the call, since you might miss people. Better to say who you want to stay, that way it also solidifies who you want to stay too. For example, Operations, Support, and Rich, please stay on the call. Everyone else, feel free to drop off at your discretion.","title":"Not Letting Responders Leave"},{"location":"training/courses/incident_response/#too-frequent-status-updates","text":"067. Too frequent status updates. Docs Reference Executives especially love frequent status updates. But providing them too frequently can cause things to get out of hand. If it takes 5 minutes to write an update, and they want an update every 5 minutes, you can start to see how long it's going to take to solve the incident. At PagerDuty, we keep our internal updates to about once every 20-30 minutes. Writing the update takes away time from solving the incident, so that needs to be balanced. This cadence has worked well for us.","title":"Too Frequent Status Updates"},{"location":"training/courses/incident_response/#being-overly-focussed-on-an-issue","text":"068. Being overly focussed on an issue. Docs Reference The IC is generally the person who has the bigger picture of what's going on. But there can be a tendency for responders to become too focussed on the problem they see in front of them, rather than taking the bigger picture into account. This usually presents itself on an incident call with an SME constantly bringing up the same issue without listening to instructions from the incident commander, and having tunnel vision for the specific issue on their system. Try not to get tunnel vision or chase red herrings. Always keep the bigger picture in mind.","title":"Being Overly Focussed On An Issue"},{"location":"training/courses/incident_response/#requiring-deeply-technical-ics","text":"069. Requiring deeply technical incident commanders. Docs Reference We used to require that all of our Incident Commanders be experienced engineers with deep technical knowledge of all PagerDuty systems. This was one of our bigger mistakes. Remember that IC's aren't responders, they aren't the ones actually fixing the problem, so they don't need deep technical knowledge. IC's are experts at coordinating the response, not at solving technical issues. You should be relying on your SMEs for that. Removing the restriction on technical knowledge led to a dramatic increase in our pool of available incident commanders , and didn't have any effect on our ability to respond to incidents. We now have ICs from all across the organization, with even more currently in training. It's already hard enough to get people to want to be an IC, so don't add further unnecessary restrictions to your pool.","title":"Requiring Deeply Technical ICs"},{"location":"training/courses/incident_response/#taking-on-multiple-roles","text":"070. Taking on multiple roles. Docs Reference In past PagerDuty incidents, we've had instances where the Incident Commander has started to assume the Subject Matter Expert role and attempted to solve the problem themselves. This typically happens when an engineer is the IC, and the incident is something to do with a system they helped to build. It's very tempting to say \"I know how to fix this!\" and jump in and solve the problem yourself. But you cannot do that as an IC. Inevitably, the incident will be bigger than you think, and while you're trying to fix you little fire, there's another one happening in another service and you've lost sight of the bigger picture. You cannot take on another role at the same time as being an Incident Commander. If you absolutely are the only person who can solve the problem, then handover to another IC and assume the role of an SME for the remainder of the incident.","title":"Taking on Multiple Roles"},{"location":"training/courses/incident_response/#litigating-policy","text":"071. Litigating policy during an incident. Docs Reference Just like with severities, policy and processes should not be discussed during an incident. The current process should be followed, and any concerns should be raised afterwards, either during a post-mortem or directly to the team managing the incident response process. Trying to change the process during an incident is only going to prolong the current incident. That's not the time to have that discussion.","title":"Litigating Policy"},{"location":"training/courses/incident_response/#averse-to-process-changes","text":"072. Being averse to process changes. Docs Reference Finally, once a stable process is in place and incidents are getting resolved, there can be lots of hesitation and resistance to changing that process. \"If it ain't broke don't fix it\". As your company grows, your response will need to change. Holding on to your old processes and practices for too long can hinder your incident response going forward. Don't be reckless, of course, but try to introduce sensible changes and don't be afraid to make changes which might slow things down in the short-term, but will make things faster in the long-run. These are the hardest changes to make, but ultimately the most worthwhile.","title":"Averse to Process Changes"},{"location":"training/courses/incident_response/#resolved","text":"073. Resolved. OK, so if all goes well, you're incident will get resolved. That means we're all done and we can go home, right?! Well, not quite yet. There's still one more thing we need to do.","title":"Resolved"},{"location":"training/courses/incident_response/#dont-neglect-the-post-mortem","text":"074. Don't neglect the post-mortem. Docs Reference We need to do a post-mortem . Or after-action review, learning review, retrospective, incident report, etc. Whatever you want to call it, the name doesn't matter as much as actually doing one! Don't make the mistake of neglecting a post-mortem after an incident. Without a post-mortem you fail to recognize what you're doing right, where you could improve, and most importantly, how to avoid making the same exact mistakes next time around. A well-designed, blameless post-mortem allows teams to continuously learn, and serves as a way to iteratively improve your infrastructure and incident response process. Post-mortems are an important followup action and should never be missed. Even if you triggered an incident and then decided it was a false alarm, you should still do a brief post-mortem. You just mobilized a response when you didn't need to, so you want to identify how you can make sure that doesn't happen again.","title":"Don't Neglect The Post-Mortem"},{"location":"training/courses/incident_response/#create-the-post-mortem","text":"075. Create the post-mortem. Docs Reference The first step is to create the post-mortem itself. This is the job of the Incident Commander. I don't mean they're going to write the entire post-mortem, they're just going to create the initial template . We want to make sure that a link exists so that when people ask \"When will we know what went wrong?\" you have something to give them.","title":"Create the Post-Mortem"},{"location":"training/courses/incident_response/#pick-an-owner","text":"076. Pick an owner. Then the IC needs to assign an owner. Remember how we assigned tasks to specific individuals? It\u2019s no different with a post-mortem. Make sure there\u2019s a clear owner, and that it's an individual and not a team. The most surefire way to make sure a post-mortem doesn't get completed is to assign it to a team instead of a specific person. The person you assign is responsible for completing the post-mortem, but they don't have to do it all themselves. They can delegate out sections as they see fit. But you need someone on the hook for making sure it gets finished. As with assigning other tasks, you also want to give them a deadline, and make sure they've understood that they're responsible for completing the post-mortem.","title":"Pick an Owner"},{"location":"training/courses/incident_response/#blameless","text":"077. Blameless. Importantly, post-mortems need to be blameless . If someone made a mistake, you just spent lots of money training them to never do it again. You can\u2019t fire your way to reliability. Let's say Bob ran a command which deleted your entire database. Your post-mortem shouldn't be \"Bob made a mistake and should be fired or have his access revoked!\". The post-mortem should be \"Why is our system configured in a way which allowed a single user to delete the entire database?\". If you name and shame people in a post-mortem, it demotivates everyone. Next time someone makes a mistake, they're not going to own up to it, because they'll be afraid of getting shamed too. You want people to bring up problems, because then you get to fix them quickly.","title":"Blameless"},{"location":"training/courses/incident_response/#review-the-process","text":"078. Review the process too! Don\u2019t forget to also review the process as part of the post-mortem. How can you change the process to make it better? What isn\u2019t working out well? Just as it\u2019s important to learn from and fix mistakes in your software, you want to do the same for your incident response process.","title":"Review the Process"},{"location":"training/courses/incident_response/#practice","text":"079. Practice makes perfect. Finally, you want to practice your incident response process as much as you can. You don't want to be doing it for the first time during a real incident. Reading about it is one thing, but going through the motions is very different. Start by running mock incidents. Then treat your smaller incidents as if they're larger ones. If you trigger incident response and find it's not a real incident, treat it like one anyway since it's free practice. At PagerDuty, we run something called Failure Friday where we purposefully inject failure into our systems to test their resilience. We treat this like a major incident, with an incident commander and everything. It allows us to practice while we're not under the stress of a normal incident. We also play a game called Keep Talking And Nobody Explodes . Yes, that's right, we play video games at work. But we've found that this game really helps to simulate a lot of the things an incident commander has to deal with, and is a great way to get some stress free practice. The bottom line is to practice as much as you can, so that when you do have the inevitable incident, your response is just routine.","title":"Practice"},{"location":"training/courses/incident_response/#open-source-response-docs","text":"080. Our open-source incident response documentation. This was just a brief taste of the training we run at PagerDuty for our own Incident Commanders. We had nowhere near enough time to cover everything. Good news though! We have published our entire incident response process online. It is an exact copy of our internal documentation only with things like phone numbers removed. It's complete free to use, and is open-sourced under an Apache 2 license so you can use it in your own organizations. It's on GitHub and we do accept pull requests if you spot any mistakes or have improvement suggestions. Everything I've talked about today can be found in the documentation, and there's lots of great additional reading material if you want to learn more.","title":"Open-Source Response Docs"},{"location":"training/courses/incident_response/#response-docs-image","text":"081. Response docs screenshot. It looks pretty too.","title":"Response Docs Image"},{"location":"training/courses/incident_response/#summary","text":"082. Summary. Incident command training is useful in so many situations outside of a server exploding in the night. It can be applicable to many different things in your life, whether it's staying calm after a fender bender on the highway, or jumping into action to help during a major natural disaster. You'd be surprised at how useful it can be in everyday life. I even wrote a slightly tongue-in-cheek blog post called \" BabyDuty or: How PagerDuty Accidentally Prepared me for Fatherhood \" where I compared childbirth to a major incident, and the role of a parent with that of an Incident Commander. Though the older my daughter gets, the more I'm starting to realize it perhaps wasn't quite as tongue-in-cheek as I first thought. Anyway, with that, I'll leave you with a quick summary of the main things we discussed today. Thanks! Questions? If you have questions about this training material, feel free to ask me on Twitter, I'm @r_adams .","title":"Summary"},{"location":"training/courses/incident_response/#image-credits","text":"083. Image credits. Image Credits Here are the credits for all the images used throughout this training material.","title":"Image Credits"},{"location":"training/courses/incident_response/#pagerduty-university","text":"084. PagerDuty University. PagerDuty University Shameless plug: If you're interested in our longer courses on this and other topics, including how to use PagerDuty to do it, we offer a variety of different training programs as part of PagerDuty University \u2014 from private full-day courses at your own offices, to public instructor-led training.","title":"PagerDuty University"},{"location":"training/courses/incident_response/#video","text":"PagerDuty Summit Series Chicago 2017 As a special bonus for making it to the end, here's a recording of an earlier version of this training given at a PagerDuty event in September 2017. The material will differ slightly from that shown on this website, as we have made changes and refined the content since then. But it should give you a decent idea of how the course is usually presented.","title":"Video"}]}